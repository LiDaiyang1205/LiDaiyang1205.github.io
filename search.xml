<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[搭建slam前端]]></title>
    <url>%2F2019%2F10%2F08%2F%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84slam%E5%89%8D%E7%AB%AF%2F</url>
    <content type="text"><![CDATA[0 简介这个文档参考于视觉slam14讲的第九讲。 按照高博的做法，同样是一步一步搭建，由浅入深比较容易理解，当然这也带来了一些反复的修改的工作，如果想直接看最终版本，可以到这里看最终版。 1 搭建VO框架1.1 确定程序框架挑选一个工程目录，在它下面建立这些文件夹来组织代码文件： bin 用来存放可执行的二进制； include/myslam 存放 slam 模块的头文件，主要是.h。这种做法的理由是，当你把包含目录设到 include 时，在引用自己的头文件时，需要写 include ”myslam/xxx.h”，这样不容易和别的库混淆。 src 存放源代码文件，主要是 cpp; test 存放测试用的文件，也是 cpp; lib 存放编译好的库文件; config 存放配置文件; cmake_modules 第三方库的 cmake 文件，在使用 g2o 之类的库中会用到它。 1.2 确定基本数据结构 帧：一个帧是相机采集到的图像单位。它主要包含一个图像(RGB-D 情形下是一对图像，包括RGB和深度图)。此外，还有特征点、位姿、内参等信息。关键帧(Key-frame)如何选择是一个很大的问题，而且基于工程经验，很少有理论上的指导。 路标：路标点即图像中的特征点。 配置文件：在写程序中你会经常遇到各种各样的参数，比如相机的内参、特征点的数量、匹配时选择的比例等等。最好的方式是在外部定义一个配置文件，程序运行时读取该配置文件中的参数值。这样，每次只要修改配置文件内容就行了，不必对程序本身做任何修改。 坐标变换：例如世界坐标到相机坐标、相机坐标到归一化相机坐标、归一化相机坐标到像素坐标等等。定义一个类把这些操作都放在一起将更方便些。 尽量保证一个类有单独的头文件和源文件，避免把许多个类放在同一个文件中。然后，把函数声明放在头文件，实现放在源文件里(除非函数很短,也可以写在头文件中)。命名方式参照 Google的命名规范。 一共写五个类：Frame 为帧；Camera 为相机模型，实现坐标变换；MapPoint 为特征点/路标点，Map管理特征点，Config 提供配置参数。相互关系如下图所示。 1.3 Camera 类Camera 类存储相机的内参和外参，并完成相机坐标系、像素坐标系、和世界坐标系之间的坐标变换。当然，在世界坐标系中你需要一个相机的(变动的)外参，我们以参数的形式传入。 1234567891011121314151617181920212223242526272829// camera.h#ifndef CAMERA_H#define CAMERA_H#include &lt;myslam/common_include.h&gt;namespace myslam&#123; // RGB-D camera model class Camera&#123; public: typedef std::shared_ptr&lt;Camera&gt; Ptr; float fx_, fy_, cx_, cy_, depth_scale_; // 相机内参 Camera(); Camera(float fx, float fy, float cx, float cy, float depth_scale=0): fx_ (fx), fy_ (fy), cx_(cx), cy_(cy), depth_scale_(depth_scale)&#123;&#125; // 坐标转换：世界坐标系，相机坐标系，像素 Vector3d world2camera(const Vector3d&amp; p_w, const SE3&amp; T_c_w); Vector3d camera2world(const Vector3d&amp; p_c, const SE3&amp; T_c_w); Vector2d camera2pixel(const Vector3d&amp; p_c); Vector3d pixel2camera(const Vector2d&amp; p_p, double depth=1); Vector3d pixel2world (const Vector2d&amp; p_p, const SE3&amp; T_c_w, double depth=1); Vector2d world2pixel (const Vector3d&amp; p_w, const SE3&amp; T_c_w); &#125;;&#125;#endif //CAMERA_H 说明： ifndef 宏定义：如果没有这个宏，在两处引用此头文件时将出现类的重复定义。所以，在每个程序头文件里都会定义这样一个宏。 命名空间 namespace myslam：将类定义包裹起来可以防止我们不小心定义出别的库里同名的函数，也是一种比较安全和规范的做法。 common_include.h 文件：把一些常用的头文件放在一个 common_include.h 文件中，这样就可以避免每次书写一个很长的一串 include。 shared_ptr &lt; Camera &gt; Ptr：把智能指针定义成 Camera 的指针类型，因此以后在传递参数时，只需用 Camera::Ptr 类型即可。 123456789101112131415161718192021222324252627282930313233343536373839404142434445// camera.cpp#include &lt;myslam/camera.h&gt;#include "myslam/config.h"namespace myslam&#123; Camera::Camera()&#123; fx_ = Config::get&lt;float&gt;("camera.fx"); fy_ = Config::get&lt;float&gt;("camera.fy"); cx_ = Config::get&lt;float&gt;("camera.cx"); cy_ = Config::get&lt;float&gt;("camera.cy"); depth_scale_ = Config::get&lt;float&gt;("camera.depth_scale"); &#125; Vector3d Camera::world2camera(const Vector3d&amp; p_w, const SE3&amp; T_c_w) &#123; return T_c_w*p_w; &#125; Vector3d Camera::camera2world(const Vector3d&amp; p_c, const SE3&amp; T_c_w)&#123; return T_c_w.inverse() *p_c; &#125; Vector2d Camera::camera2pixel(const Vector3d&amp; p_c)&#123; return Vector2d( fx_ * p_c(0,0)/p_c(2,0)+cx_, fy_ * p_c(1,0)/p_c(2,0)+cy_ ); &#125; Vector3d Camera::pixel2camera(const Vector2d&amp; p_p, double depth)&#123; return Vector3d( (p_p(0,0)-cx_)*depth/fx_, (p_p(1,0)-cy_)*depth/fy_, depth ); &#125; Vector3d Camera::pixel2world (const Vector2d&amp; p_p, const SE3&amp; T_c_w, double depth)&#123; return camera2world(pixel2camera(p_p, depth), T_c_w); &#125; Vector2d Camera::world2pixel (const Vector3d&amp; p_w, const SE3&amp; T_c_w)&#123; return camera2pixel(world2camera(p_w,T_c_w)); &#125;&#125; 1.4 Frame 类在 Frame 中，我们定义了 ID、时间戳、位姿、相机、图像这几个量，这应该是一个帧当中含有的最重要的信息。在方法中，我们提取了几个重要的方法：创建 Frame、寻找给定点对应的深度、获取相机光心、判断某个点是否在视野内等等。 123456789101112131415161718192021222324252627282930313233343536// frame.h#ifndef FRAME_H#define FRAME_H#include &lt;myslam/common_include.h&gt;#include "myslam/camera.h"namespace myslam&#123;class MapPoint;class Frame&#123;public: typedef std::shared_ptr&lt;Frame&gt; Ptr; unsigned long id_; // frame 的 id double time_stamp_; // 什么时候呼叫,时间戳 SE3 T_c_w_; // 从相机坐标系到世界坐标系的变换 Camera::Ptr camera_; // RGBD相机模式 Mat color_, depth_; //彩色和深度图像 bool is_key_frame_; // whether a key-frame 是否是关键帧public: Frame(); Frame( long id, double time_stamp=0, SE3 T_c_w=SE3(), Camera::Ptr camera=nullptr, Mat color=Mat(), Mat depth=Mat()); ~Frame(); // 工厂函数 static Frame::Ptr createFrame(); // 从深度图中获取指定点的深度信息 double findDepth(const cv::KeyPoint&amp; kp); // 获取相机光心 Vector3d getCamCenter() const ; void setPose( const SE3&amp; T_c_w ); // 检查point是否在该frame中，在视野内 bool isInFrame(const Vector3d&amp; pt_world);&#125;;&#125;#endif //FRAME_H 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283// frame.cpp#include "myslam/frame.h"namespace myslam&#123; Frame::Frame() : id_(-1), time_stamp_(-1), camera_(nullptr), is_key_frame_(false) &#123; &#125; Frame::Frame ( long id, double time_stamp, SE3 T_c_w, Camera::Ptr camera, Mat color, Mat depth ) : id_(id), time_stamp_(time_stamp), T_c_w_(T_c_w), camera_(camera), color_(color), depth_(depth), is_key_frame_(false) &#123; &#125; Frame::~Frame() &#123; &#125; // 由factory_id++一个数去构造Frame对象时，调用的是默认构造函数， // 由于默认构造函数全都有默认值，所以就是按坑填，先填第一个id_， // 所以也就是相当于创建了一个只有ID号的空白帧。 Frame::Ptr Frame::createFrame() &#123; static long factory_id = 0; return Frame::Ptr( new Frame(factory_id++) ); &#125; double Frame::findDepth ( const cv::KeyPoint&amp; kp ) &#123; int x = cvRound(kp.pt.x); int y = cvRound(kp.pt.y); ushort d = depth_.ptr&lt;ushort&gt;(y)[x];//这个是.ptr模板函数定位像素值的方法，记住用法 if ( d!=0 ) &#123; return double(d)/camera_-&gt;depth_scale_;//除以比例尺 &#125; else &#123; // check the nearby points int dx[4] = &#123;-1,0,1,0&#125;; int dy[4] = &#123;0,-1,0,1&#125;; for ( int i=0; i&lt;4; i++ ) &#123; d = depth_.ptr&lt;ushort&gt;( y+dy[i] )[x+dx[i]]; if ( d!=0 ) &#123; return double(d)/camera_-&gt;depth_scale_; &#125; &#125; &#125; return -1.0; &#125; void Frame::setPose ( const SE3&amp; T_c_w ) &#123; T_c_w_ = T_c_w; &#125; //获取相机光心。 // 这里瞪大眼看！.translation()是取平移部分！不是取转置！ // T_c_w_.inverse()求出来的平移部分就是R^(-1)*(-t), // 也就是相机坐标系下的(0,0,0)在世界坐标系下的坐标， // 也就是相机光心的世界坐标！ Vector3d Frame::getCamCenter() const &#123; return T_c_w_.inverse().translation(); &#125; bool Frame::isInFrame ( const Vector3d&amp; pt_world ) &#123; Vector3d p_cam = camera_-&gt;world2camera( pt_world, T_c_w_ ); if ( p_cam(2,0)&lt;0 ) //z值 return false; Vector2d pixel = camera_-&gt;world2pixel( pt_world, T_c_w_ ); return pixel(0,0)&gt;0 &amp;&amp; pixel(1,0)&gt;0 //xy值都大于0并且小于color图的行列 &amp;&amp; pixel(0,0)&lt;color_.cols &amp;&amp; pixel(1,0)&lt;color_.rows; &#125;&#125; 1.5 MapPoint 类MapPoint 表示路标点。我们将估计它的世界坐标，并且我们会拿当前帧提取到的特征点与地图中的路标点匹配，来估计相机的运动，因此还需要存储它对应的描述子。此外，我们会记录一个点被观测到的次数和被匹配到的次数，作为评价它的好坏程度的指标。 1234567891011121314151617181920212223// mappoint.h#ifndef MAPPOINT_H#define MAPPOINT_H#include &lt;myslam/common_include.h&gt;namespace myslam&#123; class Frame; class MapPoint&#123; public: typedef shared_ptr&lt;MapPoint&gt; Ptr; unsigned long id_; Vector3d pos_; // 世界坐标系中的位置 Vector3d norm_; // 视线方向法线 Mat descriptor_; // 匹配的描述子 int observed_times_; // 被特征匹配观测的次数 int correct_times_; // 位姿估计匹配次数，匹配正确 MapPoint(); MapPoint(long id, Vector3d position, Vector3d norm); static MapPoint::Ptr createMapPoint(); &#125;;&#125;#endif //MAPPOINT_H 1234567891011121314151617181920212223242526272829// mappoint.cpp#include "myslam/common_include.h"#include "myslam/mappoint.h"namespace myslam&#123; MapPoint::MapPoint() : id_(-1), pos_(Vector3d(0,0,0)), norm_(Vector3d(0,0,0)), observed_times_(0), correct_times_(0) &#123; &#125; MapPoint::MapPoint ( long id, Vector3d position, Vector3d norm ) : id_(id), pos_(position), norm_(norm), observed_times_(0), correct_times_(0) &#123; &#125;//这里的方法函数，还是跟默认构造函数类似，创建了一个零点 MapPoint::Ptr MapPoint::createMapPoint() &#123; static long factory_id = 0; return MapPoint::Ptr( new MapPoint( factory_id++, Vector3d(0,0,0), Vector3d(0,0,0) ) ); &#125;&#125; 1.6 Map 类Map 类管理着所有的路标点，并负责添加新路标、删除不好的路标等工作。VO 的匹配过程只需要和 Map 打交道即可。 12345678910111213141516171819202122// map.h#ifndef MAP_H#define MAP_H#include "myslam/common_include.h"#include "myslam/frame.h"#include "myslam/mappoint.h"namespace myslam&#123; class Map&#123; public: typedef shared_ptr&lt;Map&gt; Ptr; unordered_map&lt;unsigned long, MapPoint::Ptr&gt; map_points_; //所有路标 unordered_map&lt;unsigned long, Frame::Ptr&gt; keyframes_; //所有关键帧 Map()&#123;&#125; void insertKeyFrame(Frame::Ptr frame); void insertMapPoint(MapPoint::Ptr map_point); &#125;;&#125;#endif //MAP_H 12345678910111213141516171819202122232425262728293031// map.cpp#include "myslam/map.h"namespace myslam&#123; void Map::insertKeyFrame ( Frame::Ptr frame ) &#123; cout&lt;&lt;"Key frame size = "&lt;&lt;keyframes_.size()&lt;&lt;endl; if ( keyframes_.find(frame-&gt;id_) == keyframes_.end() ) &#123; keyframes_.insert( make_pair(frame-&gt;id_, frame) ); &#125; else &#123; keyframes_[ frame-&gt;id_ ] = frame; &#125; &#125; void Map::insertMapPoint ( MapPoint::Ptr map_point ) &#123; if ( map_points_.find(map_point-&gt;id_) == map_points_.end() ) &#123; map_points_.insert( make_pair(map_point-&gt;id_, map_point) ); &#125; else &#123; map_points_[map_point-&gt;id_] = map_point; &#125; &#125;&#125; 1.7 Config 类Config 类负责参数文件的读取，并在程序任意地方都可随时提供参数的值。 1234567891011121314151617181920212223242526// config.h#ifndef CONFIG_H#define CONFIG_H#include "myslam/common_include.h"namespace myslam&#123; class Config&#123; private: static std::shared_ptr&lt;Config&gt; config_; cv::FileStorage file_; Config()&#123;&#125; //私有的构造函数 public: ~Config(); // 设置新的config文件 static void setParameterFile(const std::string&amp; filename); // 访问参数值 template &lt; typename T &gt; static T get(const std::string&amp; key) &#123; return T(Config::config_-&gt;file_[key]); &#125; &#125;;&#125;#endif //CONFIG_H 123456789101112131415161718192021// config.cpp#include "myslam/config.h"namespace myslam&#123; void Config::setParameterFile(const std::string&amp; filename)&#123; if (config_ == nullptr) config_ = shared_ptr&lt;Config&gt;(new Config); config_-&gt;file_=cv::FileStorage(filename.c_str(), cv::FileStorage::READ); if (config_-&gt;file_.isOpened() == false) &#123; std::cerr&lt;&lt;"parameter file "&lt;&lt;filename&lt;&lt;"does not exist."&lt;&lt;std::endl; config_-&gt;file_.release(); return; &#125; &#125; Config::~Config() &#123; if (file_.isOpened()) file_.release(); &#125; shared_ptr&lt;Config&gt; Config::config_ = nullptr;&#125; 读取数据： 12myslam::Config::setParameterFile("parameter.yaml");double fx = myslam::Config::get&lt;double&gt;("Camera.fx"); 1.8 编译整个project中的CMakeLists.txt 123456789101112131415161718192021222324252627282930313233cmake_minimum_required( VERSION 2.8 )project ( myslam )set( CMAKE_CXX_COMPILER "g++" )set( CMAKE_BUILD_TYPE "Release" )set( CMAKE_CXX_FLAGS "-std=c++11 -march=native -O3" )list( APPEND CMAKE_MODULE_PATH $&#123;PROJECT_SOURCE_DIR&#125;/cmake_modules )set( EXECUTABLE_OUTPUT_PATH $&#123;PROJECT_SOURCE_DIR&#125;/bin )set( LIBRARY_OUTPUT_PATH $&#123;PROJECT_SOURCE_DIR&#125;/lib )############### dependencies ####################### Eigeninclude_directories( "/usr/include/eigen3" )# OpenCVfind_package( OpenCV REQUIRED )include_directories( $&#123;OpenCV_INCLUDE_DIRS&#125; )# Sophus find_package( Sophus REQUIRED )include_directories( $&#123;Sophus_INCLUDE_DIRS&#125; )# G2Ofind_package( G2O REQUIRED )include_directories( $&#123;G2O_INCLUDE_DIRS&#125; )set( THIRD_PARTY_LIBS $&#123;OpenCV_LIBS&#125; $&#123;Sophus_LIBRARIES&#125; g2o_core g2o_stuff g2o_types_sba)############### source and test ######################include_directories( $&#123;PROJECT_SOURCE_DIR&#125;/include )add_subdirectory( src )add_subdirectory( test ) src包中的CMakeLists.txt 1234567891011add_library( myslam SHARED frame.cpp mappoint.cpp map.cpp camera.cpp config.cpp)target_link_libraries( myslam $&#123;THIRD_PARTY_LIBS&#125;) test包中的新建空的CMakeLists.txt（后续添加内容） 12 编译G2O时遇到了找不到G2O的问题，需要到G2O安装编译的文件夹下的cmake_modules文件夹中复制FindG2O.cmake文件到project中的cmake_modules文件夹中。 到这里，最基本的VO框架就搭建好了。下一部分将增加执行的程序。 2 基本的 VO：特征提取和匹配2.1 流程 对新来的当前帧，提取关键点和描述子。 如果系统未初始化，以该帧为参考帧，根据深度图计算关键点的 3D 位置，返回1。 估计参考帧与当前帧间的运动。 判断上述估计是否成功。 若成功，把当前帧作为新的参考帧，回 1。 若失败，计连续丢失帧数。当连续丢失超过一定帧数，置 VO 状态为丢失，算法结束。若未超过，返回 1。 2.2 实现代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374// visual_odometry.h#ifndef VISUAL_ODOMETRY_H#define VISUAL_ODOMETRY_H#include "myslam/common_include.h"#include "myslam/map.h"#include &lt;opencv2/features2d/features2d.hpp&gt;namespace myslam&#123; class VisualOdometry&#123; public: typedef shared_ptr&lt;VisualOdometry&gt; Ptr; //定义枚举来表征VO状态，分别为：初始化、正常、丢失 enum VOState&#123; INITIALIZING=-1, OK=0, LOST &#125;; //这里为两两帧VO所用到的参考帧和当前帧。还有VO状态和整个地图。 VOState state_; // 当前VO状态 Map::Ptr map_; // 有所有frame和map point的map Frame::Ptr ref_; // reference frame 参考帧 Frame::Ptr curr_; // current frame 当前帧 //这里是两帧匹配需要的：keypoints，descriptors，matches cv::Ptr&lt;cv::ORB&gt; orb_; // orb 检测与计算 vector&lt;cv::Point3f&gt; pts_3d_ref_; // 参考帧的3d点 vector&lt;cv::KeyPoint&gt; keypoints_curr_; // 当前帧的keypoint Mat descriptors_curr_; // 当前帧的描述子 Mat descriptors_ref_; // 参考帧的描述子 vector&lt;cv::DMatch&gt; feature_matches_; // 特征匹配 //这里为匹配结果T，还有表征结果好坏的内点数和丢失数 SE3 T_c_r_estimated_; // 当前帧估计位姿 int num_inliers_; // 好的特征数量 int num_lost_; // 丢失数 // 参数 int num_of_features_; // 特征数量 double scale_factor_; // 图像金字塔的尺度因子 int level_pyramid_; // 图像金字塔的层数 float match_ratio_; // 选择好的匹配的系数 int max_num_lost_; // 持续丢失时间的最大值 int min_inliers_; // 最小 inliers //用于判定是否为关键帧的标准，就是规定一定幅度的旋转和平移，大于这个幅度就归为关键帧 double key_frame_min_rot; // 两个关键帧的最小旋转 double key_frame_min_trans; // 两个关键帧的最小平移 public:// 公式 VisualOdometry(); ~VisualOdometry(); //这个函数为核心处理函数，将帧添加进来，然后处理。 bool addFrame(Frame::Ptr frame); // 增加新的帧 protected: //一些内部处理函数，这块主要是特征匹配的 void extractKeyPoints(); void computeDescriptors(); void featureMatching(); void poseEstimationPnP(); void setRef3DPoints(); //这里是关键帧的一些功能函数 void addKeyFrame(); bool checkEstimatedPose(); bool checkKeyFrame(); &#125;;&#125;#endif //VISUAL_ODOMETRY_H 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254// visual_odometry.cpp#include &lt;opencv2/highgui/highgui.hpp&gt;#include &lt;opencv2/imgproc/imgproc.hpp&gt;#include &lt;opencv2/calib3d/calib3d.hpp&gt;#include &lt;algorithm&gt;#include &lt;boost/timer.hpp&gt;#include "myslam/config.h"#include "myslam/visual_odometry.h"namespace myslam&#123; //默认构造函数，提供默认值、读取配置参数 VisualOdometry::VisualOdometry(): state_ ( INITIALIZING ), ref_ ( nullptr ), curr_ ( nullptr ), map_ ( new Map ), num_lost_ ( 0 ), num_inliers_ ( 0 ) &#123; num_of_features_ = Config::get&lt;int&gt; ( "number_of_features" ); scale_factor_ = Config::get&lt;double&gt; ( "scale_factor" ); level_pyramid_ = Config::get&lt;int&gt; ( "level_pyramid" ); match_ratio_ = Config::get&lt;float&gt; ( "match_ratio" ); max_num_lost_ = Config::get&lt;float&gt; ( "max_num_lost" ); min_inliers_ = Config::get&lt;int&gt; ( "min_inliers" ); key_frame_min_rot = Config::get&lt;double&gt; ( "keyframe_rotation" ); key_frame_min_trans = Config::get&lt;double&gt; ( "keyframe_translation" ); //这个create()，之前用的时候，都是用的默认值，所以没有任何参数，这里传入了一些参数，可参见函数定义 orb_ = cv::ORB::create ( num_of_features_, scale_factor_, level_pyramid_ ); &#125; VisualOdometry::~VisualOdometry() &#123; &#125; //最核心的添加帧，参数即为新的一帧，根据VO当前状态选择是进行初始化还是计算T bool VisualOdometry::addFrame ( Frame::Ptr frame ) &#123; //根据VO状态来进行不同处理。 switch ( state_ ) &#123; //第一帧，则进行初始化处理 case INITIALIZING: &#123; //更改状态为OK state_ = OK; //因为是初始化，所以当前帧和参考帧都为此第一帧 curr_ = ref_ = frame; //并将此帧插入到地图中 map_-&gt;insertKeyFrame ( frame ); // extract features from first frame //匹配的操作，提取keypoint和计算描述子 extractKeyPoints(); computeDescriptors(); // compute the 3d position of features in ref frame //这里提取出的keypoint要形成3d坐标，所以调用setRef3DPoints()去补齐keypoint的depth数据。 setRef3DPoints(); break; &#125; //如果为正常，则匹配并调用poseEstimationPnP()函数计算T。 case OK: &#123; curr_ = frame; extractKeyPoints(); computeDescriptors(); featureMatching(); //进行位姿估计 poseEstimationPnP(); //根据位姿估计的结果进行分别处理 if ( checkEstimatedPose() == true ) // a good estimation &#123; //好的估计，计算当前位姿 curr_-&gt;T_c_w_ = T_c_r_estimated_ * ref_-&gt;T_c_w_; // T_c_w = T_c_r*T_r_w //把当前帧赋值为参考帧 ref_ = curr_; //补全参考帧的depth数据 setRef3DPoints(); num_lost_ = 0; //检验一下是否为关键帧，是的话加入关键帧 if ( checkKeyFrame() == true ) // is a key-frame &#123; addKeyFrame(); &#125; &#125; else // bad estimation due to various reasons &#123; //坏的估计将丢失计数+1，并判断是否大于最大丢失数，如果是，将VO状态切换为lost。 num_lost_++; if ( num_lost_ &gt; max_num_lost_ ) &#123; state_ = LOST; &#125; return false; &#125; break; &#125; case LOST: &#123; cout&lt;&lt;"vo has lost."&lt;&lt;endl; break; &#125; &#125; return true; &#125; //提取keypoint void VisualOdometry::extractKeyPoints() &#123; orb_-&gt;detect ( curr_-&gt;color_, keypoints_curr_ ); &#125; //计算描述子 void VisualOdometry::computeDescriptors() &#123; orb_-&gt;compute ( curr_-&gt;color_, keypoints_curr_, descriptors_curr_ ); &#125; //特征匹配 void VisualOdometry::featureMatching() &#123; // match desp_ref and desp_curr, use OpenCV's brute force match vector&lt;cv::DMatch&gt; matches; cv::BFMatcher matcher ( cv::NORM_HAMMING ); matcher.match ( descriptors_ref_, descriptors_curr_, matches ); // select the best matches //寻找最小距离，这里用到了STL中的std::min_element和lambda表达式 //这的作用是找到matches数组中最小的距离，然后赋值给min_dis float min_dis = std::min_element (matches.begin(), matches.end(),[] ( const cv::DMatch&amp; m1, const cv::DMatch&amp; m2 ) &#123; return m1.distance &lt; m2.distance; &#125; )-&gt;distance; //根据最小距离，对matches数组进行刷选，只有小于最小距离一定倍率或者小于30的才能push_back进数组。 //最终得到筛选过的，距离控制在一定范围内的可靠匹配 feature_matches_.clear(); for ( cv::DMatch&amp; m : matches ) &#123; if ( m.distance &lt; max&lt;float&gt;( min_dis*match_ratio_, 30.0 ) ) &#123; feature_matches_.push_back(m); &#125; &#125; cout&lt;&lt;"good matches: "&lt;&lt;feature_matches_.size()&lt;&lt;endl; &#125; //新的帧来的时候，是一个2D数据，因为PNP需要的是参考帧的3D，当前帧的2D。 //所以在当前帧迭代为参考帧时，有个工作就是加上depth数据。也就是设置参考帧的3D点。 void VisualOdometry::setRef3DPoints() &#123; // select the features with depth measurements //3D点数组先清空，后面重新装入 pts_3d_ref_.clear(); //参考帧的描述子也是构建个空Mat。 descriptors_ref_ = Mat(); //对当前keypoints数组进行遍历 for ( size_t i=0; i&lt;keypoints_curr_.size(); i++ ) &#123; //找到对应的depth数据赋值给d double d = ref_-&gt;findDepth(keypoints_curr_[i]); //如果&gt;0说明depth数据正确，进行构造 if ( d &gt; 0) &#123; //由像素坐标求得相机下3D坐标 Vector3d p_cam = ref_-&gt;camera_-&gt;pixel2camera(Vector2d(keypoints_curr_[i].pt.x, keypoints_curr_[i].pt.y), d); //由于列向量，所以按行构造Point3f，push_back进参考帧的3D点。 pts_3d_ref_.push_back( cv::Point3f( p_cam(0,0), p_cam(1,0), p_cam(2,0) )); //参考帧描述子这里就按照当前帧描述子按行push_back。这里也可以发现，算出来的Mat类型的描述子，是按行存储为一列，读取时需要遍历行。 descriptors_ref_.push_back(descriptors_curr_.row(i)); &#125; &#125; &#125; //核心功能函数，用PnP估计位姿 void VisualOdometry::poseEstimationPnP() &#123; // construct the 3d 2d observations vector&lt;cv::Point3f&gt; pts3d; vector&lt;cv::Point2f&gt; pts2d; //从这里就可以看出，参考帧用的是3D，当前帧用的2D。 for ( cv::DMatch m:feature_matches_ ) &#123; //这里不一样的，pts_3d_ref_本来就是3dpoint数组，所以直接定位索引就是3d点了 pts3d.push_back( pts_3d_ref_[m.queryIdx] ); //而这里keypoints_curr_是keypoint数组，所以定位索引后类型是keypoint，还需一步.pt获取关键点像素坐标。 pts2d.push_back( keypoints_curr_[m.trainIdx].pt ); &#125; //构造相机内参矩阵K Mat K = ( cv::Mat_&lt;double&gt;(3,3)&lt;&lt; ref_-&gt;camera_-&gt;fx_, 0, ref_-&gt;camera_-&gt;cx_, 0, ref_-&gt;camera_-&gt;fy_, ref_-&gt;camera_-&gt;cy_, 0,0,1 ); //旋转向量，平移向量，内点数组 Mat rvec, tvec, inliers; //整个核心就是用这个cv::solvePnPRansac()去求解两帧之间的位姿变化 cv::solvePnPRansac( pts3d, pts2d, K, Mat(), rvec, tvec, false, 100, 4.0, 0.99, inliers ); //内点数量为内点行数，所以为列存储。 num_inliers_ = inliers.rows; cout&lt;&lt;"pnp inliers: "&lt;&lt;num_inliers_&lt;&lt;endl; //根据旋转和平移构造出当前帧相对于参考帧的T，这样一个T计算完成了。循环计算就能得到轨迹。 T_c_r_estimated_ = SE3( SO3(rvec.at&lt;double&gt;(0,0), rvec.at&lt;double&gt;(1,0), rvec.at&lt;double&gt;(2,0)), Vector3d( tvec.at&lt;double&gt;(0,0), tvec.at&lt;double&gt;(1,0), tvec.at&lt;double&gt;(2,0)) ); &#125; //简单的位姿检验函数，整体思路就是匹配点不能过少，运动不能过大。 bool VisualOdometry::checkEstimatedPose() &#123; // check if the estimated pose is good //这里简单的做一下位姿估计判断，主要有两个，一就是匹配点太少的话，直接false，或者变换向量模长太大的话，也直接false if ( num_inliers_ &lt; min_inliers_ ) &#123; cout&lt;&lt;"reject because inlier is too small: "&lt;&lt;num_inliers_&lt;&lt;endl; return false; &#125; // if the motion is too large, it is probably wrong //将变换矩阵取log操作得到变换向量。 Sophus::Vector6d d = T_c_r_estimated_.log(); //根据变换向量的模长来判断运动的大小。过大的话返回false if ( d.norm() &gt; 5.0 ) &#123; cout&lt;&lt;"reject because motion is too large: "&lt;&lt;d.norm()&lt;&lt;endl; return false; &#125; //如果让面两项都没return，说明内点量不少，运动也没过大，return true return true; &#125; bool VisualOdometry::checkKeyFrame() &#123; //说一下这个是否为关键帧的判断，也很简单， //关键帧并不是之前理解的轨迹比较长了，隔一段选取一个，而还是每一帧的T都判断一下，比较大就说明为关键帧，说明在这一帧中，要么平移比较大，要么拐弯导致旋转比较大，所以添加，如果在运动上一直就是小运动，运动多久都不会添加为关键帧。 //另外上方的判断T计算错误也是运动过大，但是量级不一样，判断计算错误是要大于5，而关键帧，在配置文件中看只需要0.1就定义为关键帧了，所以0.1到5的差距，导致这两个函数并不冲突 Sophus::Vector6d d = T_c_r_estimated_.log(); Vector3d trans = d.head&lt;3&gt;(); Vector3d rot = d.tail&lt;3&gt;(); if ( rot.norm() &gt;key_frame_min_rot || trans.norm() &gt;key_frame_min_trans ) return true; return false; &#125;//关键帧添加，直接调用insertKeyFrame()将当前帧插入就好了。 void VisualOdometry::addKeyFrame() &#123; cout&lt;&lt;"adding a key-frame"&lt;&lt;endl; map_-&gt;insertKeyFrame ( curr_ ); &#125;&#125; 修改src包中的CMakeLists.txt 123456789101112add_library( myslam SHARED frame.cpp mappoint.cpp map.cpp camera.cpp config.cpp visual_odometry.cpp)target_link_libraries( myslam $&#123;THIRD_PARTY_LIBS&#125;) 在test包中增加run_vo.cpp 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149// run_vo.cpp// -------------- test the visual odometry -------------#include &lt;fstream&gt;#include &lt;boost/timer.hpp&gt;#include &lt;opencv2/imgcodecs.hpp&gt;#include &lt;opencv2/highgui/highgui.hpp&gt;#include &lt;opencv2/viz.hpp&gt;#include "myslam/config.h"#include "myslam/visual_odometry.h"int main ( int argc, char** argv )&#123; //terminal运行时需要添加参数文件命令行，这里加一步判断 if ( argc != 2 ) &#123; cout&lt;&lt;"usage: run_vo parameter_file"&lt;&lt;endl; return 1; &#125; //链接参数文件 myslam::Config::setParameterFile ( argv[1] ); //构造VO，类型就是在VisualOdometry类中定义的指向自身类型的指针，然后用New开辟内存 myslam::VisualOdometry::Ptr vo ( new myslam::VisualOdometry ); //读取数据文件夹地址 string dataset_dir = myslam::Config::get&lt;string&gt; ( "dataset_dir" ); cout&lt;&lt;"dataset: "&lt;&lt;dataset_dir&lt;&lt;endl; //读取数据文件夹中的associate.txt文件 ifstream fin ( dataset_dir+"/associate.txt" ); //防呆，没读取成功的话输出错误 if ( !fin ) &#123; cout&lt;&lt;"please generate the associate file called associate.txt!"&lt;&lt;endl; return 1; &#125; //定义图片名数组和时间戳数组，用于存放associate.txt文件中所示的时间戳对其的RGB图像和depth图像 vector&lt;string&gt; rgb_files, depth_files; vector&lt;double&gt; rgb_times, depth_times; //循环读取直到文件末尾 while ( !fin.eof() ) &#123; //associate.txt文件中的数据肯定都是string类型的，定义按顺序从fin中输入。 string rgb_time, rgb_file, depth_time, depth_file; fin&gt;&gt;rgb_time&gt;&gt;rgb_file&gt;&gt;depth_time&gt;&gt;depth_file; //push_back进各个数组。 //double atof (const char* str); //将一个单字节字符串转化成一个浮点数。 rgb_times.push_back ( atof ( rgb_time.c_str() ) ); depth_times.push_back ( atof ( depth_time.c_str() ) ); rgb_files.push_back ( dataset_dir+"/"+rgb_file ); depth_files.push_back ( dataset_dir+"/"+depth_file ); //.good()返回是否读取到文件末尾，文件末尾处此函数会返回false。所以跳出 if ( fin.good() == false ) break; &#125; //创建相机 myslam::Camera::Ptr camera ( new myslam::Camera ); // visualization //可视化内容，用到OpenCV中的viz模块 //第一步、创造一个可视化窗口，构造参数为窗口名称 cv::viz::Viz3d vis("Visual Odometry"); //第二步、创建坐标系部件，这里坐标系是以Widget部件类型存在的， // 构造参数是坐标系长度，也就是可视窗里的锥形小坐标系的长度，下面对坐标系部件进行设置 cv::viz::WCoordinateSystem world_coor(1.0), camera_coor(0.5); //这里设置坐标系部件属性，然后添加到视图窗口上去 //首先利用setRenderingProperty()函数设置渲染属性， // 第一个参数是个枚举，对应要渲染的属性这里是线宽，后面是属性值 world_coor.setRenderingProperty(cv::viz::LINE_WIDTH, 2.0); camera_coor.setRenderingProperty(cv::viz::LINE_WIDTH, 1.0); //用showWidget()函数将部件添加到窗口内 vis.showWidget( "World", world_coor ); vis.showWidget( "Camera", camera_coor ); //至此，窗口中已经显示了全部需要显示的东西，就是两个坐标系：世界坐标系，相机坐标系。 //世界坐标系就是写死不动的了，所以后面也没有再提起过世界坐标系。需要做的就是计算出各个帧的相机坐标系位置 //后续的核心就是下面的for循环，在循环中，不断的给相机坐标系设置新的pose，然后达到动画的效果。 //第三步、设置视角。这步是非必要步骤，进行设置有利于观察， //不设置也会有默认视角，就是可能比较别扭。而且开始后拖动鼠标，也可以改变观察视角。 //构建三个3D点,这里其实就是构造makeCameraPose()函数需要的三个向量： //相机位置坐标、相机焦点坐标、相机y轴朝向 //蓝色-Z，红色-X，绿色-Y cv::Point3d cam_pos( 0, -1, -1 ), cam_focal_point(0,0,0), cam_y_dir(0,1,0); //由这三个参数，用makeCameraPose()函数构造Affine3d类型的相机位姿，这里其实是视角位姿，也就是程序开始时你处在什么视角看 cv::Affine3d cam_pose = cv::viz::makeCameraPose( cam_pos, cam_focal_point, cam_y_dir ); //用setViewerPose()设置观看视角 vis.setViewerPose( cam_pose ); //输出RGB图像信息，共读到文件数 cout&lt;&lt;"read total "&lt;&lt;rgb_files.size() &lt;&lt;" entries"&lt;&lt;endl; //整个画面的快速刷新呈现动态，由此for循环控制。 for ( int i=0; i&lt;rgb_files.size(); i++ ) &#123; //读取图像，创建帧操作 Mat color = cv::imread ( rgb_files[i] ); Mat depth = cv::imread ( depth_files[i], -1 ); if ( color.data==nullptr || depth.data==nullptr ) break; myslam::Frame::Ptr pFrame = myslam::Frame::createFrame(); pFrame-&gt;camera_ = camera; pFrame-&gt;color_ = color; pFrame-&gt;depth_ = depth; pFrame-&gt;time_stamp_ = rgb_times[i]; //这里加个每帧的运算时间，看看实时性 boost::timer timer; //这里将帧添加进去，进行位姿变换计算 vo-&gt;addFrame ( pFrame ); cout&lt;&lt;"VO costs time: "&lt;&lt;timer.elapsed()&lt;&lt;endl; //VO状态为LOST时，跳出循环。 if ( vo-&gt;state_ == myslam::VisualOdometry::LOST ) break; //这里要说一下，可视化窗口中动的是相机坐标系，所以本质上是求取相机坐标系下的点在世界坐标系下的坐标， //Pw=Twc*Pc; SE3 Twc = pFrame-&gt;T_c_w_.inverse(); //SE3 Twc = pFrame-&gt;T_c_w_; //show the map and the camera pose //用Twc构造Affine3d类型的pose所需要的旋转矩阵和平移矩阵 cv::Affine3d::Mat3 rmat( Twc.rotation_matrix()(0,0), Twc.rotation_matrix()(0,1), Twc.rotation_matrix()(0,2), Twc.rotation_matrix()(1,0), Twc.rotation_matrix()(1,1), Twc.rotation_matrix()(1,2), Twc.rotation_matrix()(2,0), Twc.rotation_matrix()(2,1), Twc.rotation_matrix()(2,2) ); cv::Affine3d::Vec3 tvec(Twc.translation()(0,0), Twc.translation()(1,0), Twc.translation()(2,0)); //构造位姿 cv::Affine3d pose(rmat,tvec); //两窗口同时显示，一个是图像 cv::imshow("image", color ); cv::waitKey(1); //另外一个就是viz可视化窗口 vis.setWidgetPose( "Camera", pose); vis.spinOnce(1, false); &#125; return 0;&#125; test包中的CMakeLists.txt添加 12add_executable( run_vo run_vo.cpp )target_link_libraries( run_vo myslam ) 2.3 添加数据集data文件夹中是TUM数据集“freburg1_desk”中的图像。其中： rgb.txt 和 depth.txt 记录了各文件的采集时间和对应的文件名。 rgb/ 和 depth/目录存放着采集到的 png 格式图像文件。彩色图像为八位三通道，深度图为 16 位单通道图像。文件名即采集时间。 groundtruth.txt 为外部运动捕捉系统采集到的相机位姿，格式为(time, t x , t y , t z , q x , q y , q z , q w )，我们可以把它看成标准轨迹。 请注意彩色图、深度图和标准轨迹的采集都是独立的，轨迹的采集频率比图像高很多。在使用数据之前，需要根据采集时间，对数据进行一次时间上的对齐，以便对彩色图和深度图进行配对。原则上，我们可以把采集时间相近于一个阈值的数据，看成是一对图像。并把相近时间的位姿，看作是该图像的真实采集位置。TUM 提供了一个 python 脚本“associate.py”帮我们完成这件事。请把此文件放到数据集目录下，运行: 1python associate.py rgb.txt depth.txt &gt; associate.txt 这段脚本会根据输入两个文件中的采集时间进行配对，最后输出到一个文件 associate.txt。输出文件含有被配对的两个图像的时间、文件名信息，可以作为后续处理的来源。 2.4 参数文件在config文件夹中新建default.yaml 1234567891011121314151617181920212223%YAML:1.0# data# the tum dataset directory, change it to yours! dataset_dir: /home/ldy/CLionProjects/vo_project/data/desk1# camera intrinsics# fr1camera.fx: 517.3camera.fy: 516.5camera.cx: 325.1camera.cy: 249.7camera.depth_scale: 5000# VO parasnumber_of_features: 500scale_factor: 1.2level_pyramid: 8match_ratio: 2.0max_num_lost: 10min_inliers: 10keyframe_rotation: 0.1keyframe_translation: 0.1 其中dataset_dir为数据集地址。 这部分在1的基础上增加了简单的VO实现文件，下部分将对PNP进行优化。 3 改进:优化 PnP 的结果通过最小化重投影误差来构建优化问题，并使用G2O实现求解。 3.1 实现代码12345678910111213141516171819202122232425262728293031323334353637383940//g2o_types.h#ifndef G2O_TYPES_H#define G2O_TYPES_H#include "myslam/common_include.h"#include "camera.h"#include &lt;g2o/core/base_vertex.h&gt;#include &lt;g2o/core/base_unary_edge.h&gt;#include &lt;g2o/core/block_solver.h&gt;#include &lt;g2o/core/optimization_algorithm_levenberg.h&gt;#include &lt;g2o/types/sba/types_six_dof_expmap.h&gt;#include &lt;g2o/solvers/dense/linear_solver_dense.h&gt;#include &lt;g2o/core/robust_kernel.h&gt;#include &lt;g2o/core/robust_kernel_impl.h&gt;namespace myslam&#123; class EdgeProjectXYZ2UVPoseOnly: public g2o::BaseUnaryEdge&lt;2, Eigen::Vector2d, g2o::VertexSE3Expmap &gt; &#123; public: EIGEN_MAKE_ALIGNED_OPERATOR_NEW //还是边类型定义中最核心的两部分： //误差计算函数，实现误差计算方法 virtual void computeError(); //线性增量函数，也就是雅克比矩阵J的计算方法 virtual void linearizeOplus(); //读写功能函数，这里没用到，所以只是定义了，并没有在源文件中实现。 virtual bool read( std::istream&amp; in )&#123;&#125; virtual bool write(std::ostream&amp; os) const &#123;&#125;; //把三维点和相机模型写成员变量，方便误差计算和J计算，因为都需要这两项数据 Vector3d point_; Camera* camera_; &#125;;&#125;#endif //G2O_TYPES_H 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152//g2o_types.cpp#include "myslam/g2o_types.h"namespace myslam&#123;//前两种这里没有用//第三种，重投影误差 void EdgeProjectXYZ2UVPoseOnly::computeError() &#123; //顶点数组中取出顶点，转换成位姿指针类型，其实左边的pose类型可以写为auto const g2o::VertexSE3Expmap* pose = static_cast&lt;const g2o::VertexSE3Expmap*&gt; ( _vertices[0] ); //误差计算，测量值减去估计值，也就是重投影误差 //估计值计算方法是T*p,得到相机坐标系下坐标，然后在利用camera2pixel()函数得到像素坐标。 _error = _measurement - camera_-&gt;camera2pixel(pose-&gt;estimate().map(point_) ); &#125; void EdgeProjectXYZ2UVPoseOnly::linearizeOplus() &#123; /** * 这里说一下整体思路： * 重投影误差的雅克比矩阵在书中P164页式7.45已经呈现，所以这里就是直接构造， * 构造时发现需要变换后的空间点坐标，所以需要先求出。 */ //首先还是从顶点取出位姿 g2o::VertexSE3Expmap* pose = static_cast&lt;g2o::VertexSE3Expmap*&gt; ( _vertices[0] ); //这由位姿构造一个四元数形式T g2o::SE3Quat T ( pose-&gt;estimate() ); //用T求得变换后的3D点坐标。T*p Vector3d xyz_trans = T.map ( point_ ); //到这步，变换后的3D点xyz坐标就分别求出来了，后面的z平方，纯粹是为了后面构造J时方便定义的，因为需要多处用到 double x = xyz_trans[0]; double y = xyz_trans[1]; double z = xyz_trans[2]; double z_2 = z*z; //直接各个元素构造J就好了，对照式7.45是一模一样的，2*6的矩阵。 _jacobianOplusXi ( 0,0 ) = x*y/z_2 *camera_-&gt;fx_; _jacobianOplusXi ( 0,1 ) = - ( 1+ ( x*x/z_2 ) ) *camera_-&gt;fx_; _jacobianOplusXi ( 0,2 ) = y/z * camera_-&gt;fx_; _jacobianOplusXi ( 0,3 ) = -1./z * camera_-&gt;fx_; _jacobianOplusXi ( 0,4 ) = 0; _jacobianOplusXi ( 0,5 ) = x/z_2 * camera_-&gt;fx_; _jacobianOplusXi ( 1,0 ) = ( 1+y*y/z_2 ) *camera_-&gt;fy_; _jacobianOplusXi ( 1,1 ) = -x*y/z_2 *camera_-&gt;fy_; _jacobianOplusXi ( 1,2 ) = -x/z *camera_-&gt;fy_; _jacobianOplusXi ( 1,3 ) = 0; _jacobianOplusXi ( 1,4 ) = -1./z *camera_-&gt;fy_; _jacobianOplusXi ( 1,5 ) = y/z_2 *camera_-&gt;fy_; &#125;&#125; 在原有visual_odometry.cpp文件中的VisualOdometry::poseEstimationPnP()函数里增加 12345678910111213141516171819202122232425262728293031323334353637383940414243444546#include "myslam/g2o_types.h" // 加在最开头// using bundle adjustment to optimize the pose //初始化，注意由于更新所需要的unique指针问题。 typedef g2o::BlockSolver&lt;g2o::BlockSolverTraits&lt;6,2&gt;&gt; Block; Block::LinearSolverType* linearSolver = new g2o::LinearSolverDense&lt;Block::PoseMatrixType&gt;(); Block* solver_ptr = new Block( linearSolver ); g2o::OptimizationAlgorithmLevenberg* solver = new g2o::OptimizationAlgorithmLevenberg ( solver_ptr ); g2o::SparseOptimizer optimizer; optimizer.setAlgorithm ( solver ); //添加顶点，一帧只有一个位姿，也就是只有一个顶点 g2o::VertexSE3Expmap* pose = new g2o::VertexSE3Expmap(); pose-&gt;setId ( 0 ); pose-&gt;setEstimate ( g2o::SE3Quat ( T_c_r_estimated_.rotation_matrix(), T_c_r_estimated_.translation() ) ); optimizer.addVertex ( pose ); // edges边有许多，每个特征点都对应一个重投影误差，也就有一个边。 for ( int i=0; i&lt;inliers.rows; i++ ) &#123; int index = inliers.at&lt;int&gt;(i,0); // 3D -&gt; 2D projection EdgeProjectXYZ2UVPoseOnly* edge = new EdgeProjectXYZ2UVPoseOnly(); edge-&gt;setId(i); edge-&gt;setVertex(0, pose); edge-&gt;camera_ = curr_-&gt;camera_.get(); edge-&gt;point_ = Vector3d( pts3d[index].x, pts3d[index].y, pts3d[index].z ); edge-&gt;setMeasurement( Vector2d(pts2d[index].x, pts2d[index].y) ); edge-&gt;setInformation( Eigen::Matrix2d::Identity() ); optimizer.addEdge( edge ); &#125; //开始优化 optimizer.initializeOptimization(); //设置迭代次数 optimizer.optimize(10); //这步就是将优化后的结果，赋值给T_c_r_estimated_ T_c_r_estimated_ = SE3 ( pose-&gt;estimate().rotation(), pose-&gt;estimate().translation() ); 3.2 编译在src/CMakeLists.txt 中增加 g2o_types.cpp 修改根目录的CMakeLists.txt文件，改为 1set( CMAKE_CXX_FLAGS &quot;-std=c++11 -O3&quot; ) 否则编译会出错。 依旧运行run_vo，实现位姿估计。 4 改进:局部地图在第3部分中VO 仍受两两帧间匹配的局限性影响。一旦视频序列当中某个帧丢失，就会导致后续的帧也无法和上一帧匹配。在这部分，将 VO 匹配到的特征点放到地图中，并将当前帧与地图点进行匹配，计算位姿。这种做法与之前的差异可见下图 4.1 实现代码首先是修改MapPoint类。这个类之前一直没有使用。 1234567891011121314151617181920212223242526272829303132333435363738394041// mappoint.h#ifndef MAPPOINT_H#define MAPPOINT_H#include &lt;myslam/common_include.h&gt;namespace myslam&#123; class Frame; class MapPoint&#123; public: typedef shared_ptr&lt;MapPoint&gt; Ptr; unsigned long id_; static unsigned long factory_id_; // factory id bool good_; // 好的点 Vector3d pos_; // 世界坐标系中的位置 Vector3d norm_; // 视线方向法线 Mat descriptor_; // 匹配的描述子 list&lt;Frame*&gt; observed_frames_; // key-frames that can observe this point int matched_times_; // being an inliner in pose estimation int visible_times_; // being visible in current frame MapPoint(); MapPoint( unsigned long id, const Vector3d&amp; position, const Vector3d&amp; norm, Frame* frame=nullptr, const Mat&amp; descriptor=Mat()); inline cv::Point3f getPositionCV() const &#123; return cv::Point3f( pos_(0,0), pos_(1,0), pos_(2,0) ); &#125; static MapPoint::Ptr createMapPoint(); static MapPoint::Ptr createMapPoint( const Vector3d&amp; pos_world, const Vector3d&amp; norm_, const Mat&amp; descriptor, Frame* frame ); &#125;;&#125;#endif //MAPPOINT_H 头文件很简单，地图点本质上就是空间中3D点，成员函数为地图点的基本信息。有一个Frame*类型的 list，用于记录观察到此地图点的帧。还有个取得地图点3维坐标的功能函数，比较简单，直接写成内联了。还有个地图点生成函数，传入成员变量所需要的信息就好了。 123456789101112131415161718192021222324252627282930313233343536373839404142// mappoint.cpp#include "myslam/common_include.h"#include "myslam/mappoint.h"namespace myslam&#123; //默认构造函数，设定各种默认值 MapPoint::MapPoint() : id_(-1), pos_(Vector3d(0,0,0)), norm_(Vector3d(0,0,0)), good_(true), visible_times_(0), matched_times_(0) &#123; &#125; //构造函数，将观察帧push_back进去 MapPoint::MapPoint ( long unsigned int id, const Vector3d&amp; position, const Vector3d&amp; norm, Frame* frame, const Mat&amp; descriptor ) : id_(id), pos_(position), norm_(norm), good_(true), visible_times_(1), matched_times_(1), descriptor_(descriptor) &#123; observed_frames_.push_back(frame); &#125; // 初始化factory id unsigned long MapPoint::factory_id_ = 0; //创建地图点时，直接在累加上ID然后构造一个就好了。返回定义的MapPoint类型指针Ptr MapPoint::Ptr MapPoint::createMapPoint() &#123; return MapPoint::Ptr(new MapPoint(factory_id_++, Vector3d(0,0,0), Vector3d(0,0,0))); &#125; MapPoint::Ptr MapPoint::createMapPoint ( const Vector3d&amp; pos_world, const Vector3d&amp; norm, const Mat&amp; descriptor, Frame* frame ) &#123; return MapPoint::Ptr( new MapPoint( factory_id_++, pos_world, norm, frame, descriptor ) ); &#125;&#125; 由于工作流程的改变，所以主要的修改在VisualOdometry类。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889// visual_odometry.h#ifndef VISUAL_ODOMETRY_H#define VISUAL_ODOMETRY_H#include "myslam/common_include.h"#include "myslam/map.h"#include &lt;opencv2/features2d/features2d.hpp&gt;namespace myslam&#123; class VisualOdometry&#123; public: typedef shared_ptr&lt;VisualOdometry&gt; Ptr; //定义枚举来表征VO状态，分别为：初始化、正常、丢失 enum VOState&#123; INITIALIZING=-1, OK=0, LOST &#125;; //这里为两两帧VO所用到的参考帧和当前帧。还有VO状态和整个地图。 VOState state_; // 当前VO状态 Map::Ptr map_; // 有所有frame和map point的map Frame::Ptr ref_; // reference frame 参考帧 Frame::Ptr curr_; // current frame 当前帧 //这里是两帧匹配需要的：keypoints，descriptors，matches //在ORB部分去掉了关于参考帧的东西，3D点，描述子等 cv::Ptr&lt;cv::ORB&gt; orb_; // orb 检测与计算 // vector&lt;cv::Point3f&gt; pts_3d_ref_; // 参考帧的3d点 vector&lt;cv::KeyPoint&gt; keypoints_curr_; // 当前帧的keypoint Mat descriptors_curr_; // 当前帧的描述子 // Mat descriptors_ref_; // 参考帧的描述子 // vector&lt;cv::DMatch&gt; feature_matches_; // 特征匹配 //在匹配器中，所需要的匹配变成了地图点和帧中的关键点。 cv::FlannBasedMatcher matcher_flann_; // flann matcher vector&lt;MapPoint::Ptr&gt; match_3dpts_; // matched 3d points vector&lt;int&gt; match_2dkp_index_; // matched 2d pixels (index of kp_curr) //这里为匹配结果T，还有表征结果好坏的内点数和丢失数 //这里的T也变成了直接的cw，而不是之前的当前帧和参考帧的cr SE3 T_c_w_estimated_; // 当前帧估计位姿 int num_inliers_; // 好的特征数量 int num_lost_; // 丢失数 // 参数 int num_of_features_; // 特征数量 double scale_factor_; // 图像金字塔的尺度因子 int level_pyramid_; // 图像金字塔的层数 float match_ratio_; // 选择好的匹配的系数 int max_num_lost_; // 持续丢失时间的最大值 int min_inliers_; // 最小 inliers //用于判定是否为关键帧的标准，就是规定一定幅度的旋转和平移，大于这个幅度就归为关键帧 double key_frame_min_rot; // 两个关键帧的最小旋转 double key_frame_min_trans; // 两个关键帧的最小平移 double map_point_erase_ratio_; // remove map point ratio public:// 公式 VisualOdometry(); ~VisualOdometry(); //这个函数为核心处理函数，将帧添加进来，然后处理。 bool addFrame(Frame::Ptr frame); // 增加新的帧 protected: //一些内部处理函数，这块主要是特征匹配的 void extractKeyPoints(); void computeDescriptors(); void featureMatching(); void poseEstimationPnP(); // void setRef3DPoints(); //增加的优化地图的函数，这个函数可能实现的就是对整个后端地图的优化 void optimizeMap(); //这里是关键帧的一些功能函数 void addKeyFrame(); //增加地图点函数 void addMapPoints(); bool checkEstimatedPose(); bool checkKeyFrame(); double getViewAngle( Frame::Ptr frame, MapPoint::Ptr point ); &#125;;&#125;#endif //VISUAL_ODOMETRY_H 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476// visual_odometry.cpp#include &lt;opencv2/highgui/highgui.hpp&gt;#include &lt;opencv2/imgproc/imgproc.hpp&gt;#include &lt;opencv2/calib3d/calib3d.hpp&gt;#include &lt;algorithm&gt;#include &lt;boost/timer.hpp&gt;#include "myslam/config.h"#include "myslam/visual_odometry.h"#include "myslam/g2o_types.h"namespace myslam&#123; //默认构造函数，提供默认值、读取配置参数 VisualOdometry::VisualOdometry(): state_ ( INITIALIZING ), ref_ ( nullptr ), curr_ ( nullptr ), map_ ( new Map ), num_lost_ ( 0 ), num_inliers_ ( 0 ), matcher_flann_ ( new cv::flann::LshIndexParams ( 5,10,2 ) ) &#123; num_of_features_ = Config::get&lt;int&gt; ( "number_of_features" ); scale_factor_ = Config::get&lt;double&gt; ( "scale_factor" ); level_pyramid_ = Config::get&lt;int&gt; ( "level_pyramid" ); match_ratio_ = Config::get&lt;float&gt; ( "match_ratio" ); max_num_lost_ = Config::get&lt;float&gt; ( "max_num_lost" ); min_inliers_ = Config::get&lt;int&gt; ( "min_inliers" ); key_frame_min_rot = Config::get&lt;double&gt; ( "keyframe_rotation" ); key_frame_min_trans = Config::get&lt;double&gt; ( "keyframe_translation" ); map_point_erase_ratio_ = Config::get&lt;double&gt; ( "map_point_erase_ratio" ); //这个create()，之前用的时候，都是用的默认值，所以没有任何参数，这里传入了一些参数，可参见函数定义 orb_ = cv::ORB::create ( num_of_features_, scale_factor_, level_pyramid_ ); &#125; VisualOdometry::~VisualOdometry() &#123; &#125; //最核心的添加帧，参数即为新的一帧，根据VO当前状态选择是进行初始化还是计算T bool VisualOdometry::addFrame ( Frame::Ptr frame ) &#123; //根据VO状态来进行不同处理。 switch ( state_ ) &#123; //第一帧，则进行初始化处理 case INITIALIZING: &#123; //更改状态为OK state_ = OK; //因为是初始化，所以当前帧和参考帧都为此第一帧 curr_ = ref_ = frame; //并将此帧插入到地图中 // map_-&gt;insertKeyFrame ( frame ); // extract features from first frame //匹配的操作，提取keypoint和计算描述子 extractKeyPoints(); computeDescriptors(); // compute the 3d position of features in ref frame //这里提取出的keypoint要形成3d坐标，所以调用setRef3DPoints()去补齐keypoint的depth数据。 //setRef3DPoints(); //之前增加关键帧需调用map类中的insertKeyFrame()函数， // 这里第一帧的话，就直接调用自身的addKeyFrame()函数添加进地图 addKeyFrame(); // the first frame is a key-frame break; &#125; //如果为正常，则匹配并调用poseEstimationPnP()函数计算T。 case OK: &#123; //整个流程的改变就是只需要不断进行每一帧的位姿迭代， //而不需要用到参考帧的3D点进行匹配得T了 curr_ = frame; //新的帧来了，先将其位姿赋值为参考帧的位姿， //因为考虑到匹配失败的情况下，这一帧就定义为丢失了，所以位姿就用参考帧的了。 //如果一切正常，求得了当前帧的位姿，就进行赋值覆盖掉就好了。 curr_-&gt;T_c_w_ = ref_-&gt;T_c_w_; extractKeyPoints(); computeDescriptors(); featureMatching(); //进行位姿估计 poseEstimationPnP(); //根据位姿估计的结果进行分别处理 if ( checkEstimatedPose() == true ) // a good estimation &#123; //正常求得位姿T，对当前帧位姿进行更新 curr_-&gt;T_c_w_ = T_c_w_estimated_; optimizeMap(); num_lost_ = 0; //检验一下是否为关键帧，是的话加入关键帧 if ( checkKeyFrame() == true ) // is a key-frame &#123; addKeyFrame(); &#125; &#125; else // bad estimation due to various reasons &#123; //坏的估计将丢失计数+1，并判断是否大于最大丢失数，如果是，将VO状态切换为lost。 num_lost_++; if ( num_lost_ &gt; max_num_lost_ ) &#123; state_ = LOST; &#125; return false; &#125; break; &#125; case LOST: &#123; cout&lt;&lt;"vo has lost."&lt;&lt;endl; break; &#125; &#125; return true; &#125; //提取keypoint void VisualOdometry::extractKeyPoints() &#123; boost::timer timer; orb_-&gt;detect ( curr_-&gt;color_, keypoints_curr_ ); cout&lt;&lt;"extract keypoints cost time: "&lt;&lt;timer.elapsed() &lt;&lt;endl; &#125; //计算描述子 void VisualOdometry::computeDescriptors() &#123; boost::timer timer; orb_-&gt;compute ( curr_-&gt;color_, keypoints_curr_, descriptors_curr_ ); cout&lt;&lt;"descriptor computation cost time: "&lt;&lt;timer.elapsed() &lt;&lt;endl; &#125; //特征匹配 void VisualOdometry::featureMatching() &#123; boost::timer timer; vector&lt;cv::DMatch&gt; matches; // select the candidates in map //建立一个目标图，承接匹配需要地图点的描述子，因为匹配是需要的参数是描述子 Mat desp_map; //建立一个候选地图点数组，承接匹配需要的地图点 vector&lt;MapPoint::Ptr&gt; candidate; //检查地图点是否为匹配需要的，逻辑就是遍历维护的局部地图中所有地图点， //然后利用isInFrame()函数检查有哪些地图点在当前观察帧中， //如果在则把地图点push进candidate中，描述子push进desp_map中 for ( auto&amp; allpoints: map_-&gt;map_points_ ) &#123; //这里还是STL用法，allpoints为map类中定义的双模板类型类成员，此表示第二个模板类型 //总之功能上就是把地图点取出，赋值给p MapPoint::Ptr&amp; p = allpoints.second; // check if p in curr frame image //利用是否在匹配帧中来判断是否添加进去 if ( curr_-&gt;isInFrame(p-&gt;pos_) ) &#123; // add to candidate //观察次数增加一次 p-&gt;visible_times_++; //点push进candidate candidate.push_back( p ); //描述子push进desp_map desp_map.push_back( p-&gt;descriptor_ ); &#125; &#125; //这步匹配中，由原来的参考帧，变成了上面定义的desp_map地图，进行匹配。 matcher_flann_.match ( desp_map, descriptors_curr_, matches ); // select the best matches //寻找最小距离，这里用到了STL中的std::min_element和lambda表达式 //这的作用是找到matches数组中最小的距离，然后赋值给min_dis float min_dis = std::min_element ( matches.begin(), matches.end(), [] ( const cv::DMatch&amp; m1, const cv::DMatch&amp; m2 ) &#123; return m1.distance &lt; m2.distance; &#125; )-&gt;distance; match_3dpts_.clear(); match_2dkp_index_.clear(); for ( cv::DMatch&amp; m : matches ) &#123; //根据最小距离，对matches数组进行刷选，只有小于最小距离一定倍率或者小于30的才能push_back进数组。 //最终得到筛选过的，距离控制在一定范围内的可靠匹配 if ( m.distance &lt; max&lt;float&gt; ( min_dis*match_ratio_, 30.0 ) ) &#123; //这里变化是不像之前直接将好的m push进feature_matches_就完了。 //这里感觉像做一个记录，candidate中存的是观察到的地图点 // 进一步，在candidate中选出好的匹配的点，push进match_3dpts_， //这个match_3dpts_代表当前这一帧计算T时所利用到的所有好的地图点，放入其中。 //由此可见，candidate只是个中间存储，新的一帧过来会被刷新。 //同样match_2dkp_index_也是同样的道理，只不过是记录的当前帧detect出来的keypoint数组中的点的索引。 match_3dpts_.push_back( candidate[m.queryIdx] ); match_2dkp_index_.push_back( m.trainIdx ); &#125; &#125; cout&lt;&lt;"good matches: "&lt;&lt;match_3dpts_.size() &lt;&lt;endl; cout&lt;&lt;"match cost time: "&lt;&lt;timer.elapsed() &lt;&lt;endl; &#125;// //新的帧来的时候，是一个2D数据，因为PNP需要的是参考帧的3D，当前帧的2D。// //所以在当前帧迭代为参考帧时，有个工作就是加上depth数据。也就是设置参考帧的3D点。// void VisualOdometry::setRef3DPoints()// &#123;// // select the features with depth measurements// //3D点数组先清空，后面重新装入// pts_3d_ref_.clear();// //参考帧的描述子也是构建个空Mat。// descriptors_ref_ = Mat();// //对当前keypoints数组进行遍历// for ( size_t i=0; i&lt;keypoints_curr_.size(); i++ )// &#123;// //找到对应的depth数据赋值给d// double d = ref_-&gt;findDepth(keypoints_curr_[i]);// //如果&gt;0说明depth数据正确，进行构造// if ( d &gt; 0)// &#123;// //由像素坐标求得相机下3D坐标// Vector3d p_cam = ref_-&gt;camera_-&gt;pixel2camera(Vector2d(keypoints_curr_[i].pt.x, keypoints_curr_[i].pt.y), d);// //由于列向量，所以按行构造Point3f，push_back进参考帧的3D点。// pts_3d_ref_.push_back( cv::Point3f( p_cam(0,0), p_cam(1,0), p_cam(2,0) ));// //参考帧描述子这里就按照当前帧描述子按行push_back。这里也可以发现，算出来的Mat类型的描述子，是按行存储为一列，读取时需要遍历行。// descriptors_ref_.push_back(descriptors_curr_.row(i));// &#125;// &#125;// &#125; //核心功能函数，用PnP估计位姿 void VisualOdometry::poseEstimationPnP() &#123; // construct the 3d 2d observations vector&lt;cv::Point3f&gt; pts3d; vector&lt;cv::Point2f&gt; pts2d; //从这里就可以看出，地图点用的是3D，当前帧用的2D。 for ( int index:match_2dkp_index_ ) &#123; pts2d.push_back ( keypoints_curr_[index].pt ); &#125; for ( MapPoint::Ptr pt:match_3dpts_ ) &#123; pts3d.push_back( pt-&gt;getPositionCV() ); &#125; //构造相机内参矩阵K Mat K = ( cv::Mat_&lt;double&gt;(3,3)&lt;&lt; ref_-&gt;camera_-&gt;fx_, 0, ref_-&gt;camera_-&gt;cx_, 0, ref_-&gt;camera_-&gt;fy_, ref_-&gt;camera_-&gt;cy_, 0,0,1 ); //旋转向量，平移向量，内点数组 Mat rvec, tvec, inliers; //整个核心就是用这个cv::solvePnPRansac()去求解两帧之间的位姿变化 cv::solvePnPRansac( pts3d, pts2d, K, Mat(), rvec, tvec, false, 100, 4.0, 0.99, inliers ); //内点数量为内点行数，所以为列存储。 num_inliers_ = inliers.rows; cout&lt;&lt;"pnp inliers: "&lt;&lt;num_inliers_&lt;&lt;endl; //根据旋转和平移构造出当前帧相对于参考帧的T，这样一个T计算完成了。循环计算就能得到轨迹。 T_c_w_estimated_ = SE3( SO3(rvec.at&lt;double&gt;(0,0), rvec.at&lt;double&gt;(1,0), rvec.at&lt;double&gt;(2,0)), Vector3d( tvec.at&lt;double&gt;(0,0), tvec.at&lt;double&gt;(1,0), tvec.at&lt;double&gt;(2,0)) ); // using bundle adjustment to optimize the pose //初始化，注意由于更新所需要的unique指针问题。 typedef g2o::BlockSolver&lt;g2o::BlockSolverTraits&lt;6,2&gt;&gt; Block; Block::LinearSolverType* linearSolver = new g2o::LinearSolverDense&lt;Block::PoseMatrixType&gt;(); Block* solver_ptr = new Block( linearSolver ); g2o::OptimizationAlgorithmLevenberg* solver = new g2o::OptimizationAlgorithmLevenberg ( solver_ptr ); g2o::SparseOptimizer optimizer; optimizer.setAlgorithm ( solver ); //添加顶点，一帧只有一个位姿，也就是只有一个顶点 g2o::VertexSE3Expmap* pose = new g2o::VertexSE3Expmap(); pose-&gt;setId ( 0 ); pose-&gt;setEstimate ( g2o::SE3Quat ( T_c_w_estimated_.rotation_matrix(), T_c_w_estimated_.translation() ) ); optimizer.addVertex ( pose ); // edges边有许多，每个特征点都对应一个重投影误差，也就有一个边。 for ( int i=0; i&lt;inliers.rows; i++ ) &#123; int index = inliers.at&lt;int&gt;(i,0); // 3D -&gt; 2D projection EdgeProjectXYZ2UVPoseOnly* edge = new EdgeProjectXYZ2UVPoseOnly(); edge-&gt;setId(i); edge-&gt;setVertex(0, pose); edge-&gt;camera_ = curr_-&gt;camera_.get(); edge-&gt;point_ = Vector3d( pts3d[index].x, pts3d[index].y, pts3d[index].z ); edge-&gt;setMeasurement( Vector2d(pts2d[index].x, pts2d[index].y) ); edge-&gt;setInformation( Eigen::Matrix2d::Identity() ); optimizer.addEdge( edge ); // set the inlier map points match_3dpts_[index]-&gt;matched_times_++; &#125; //开始优化 optimizer.initializeOptimization(); //设置迭代次数 optimizer.optimize(10); //这步就是将优化后的结果，赋值给T_c_r_estimated_ T_c_w_estimated_ = SE3 ( pose-&gt;estimate().rotation(), pose-&gt;estimate().translation() ); cout&lt;&lt;"T_c_w_estimated_: "&lt;&lt;endl&lt;&lt;T_c_w_estimated_.matrix()&lt;&lt;endl; &#125; //简单的位姿检验函数，整体思路就是匹配点不能过少，运动不能过大。 bool VisualOdometry::checkEstimatedPose() &#123; // check if the estimated pose is good //这里简单的做一下位姿估计判断，主要有两个，一就是匹配点太少的话，直接false，或者变换向量模长太大的话，也直接false if ( num_inliers_ &lt; min_inliers_ ) &#123; cout&lt;&lt;"reject because inlier is too small: "&lt;&lt;num_inliers_&lt;&lt;endl; return false; &#125; // if the motion is too large, it is probably wrong //将变换矩阵取log操作得到变换向量。 SE3 T_r_c = ref_-&gt;T_c_w_ * T_c_w_estimated_.inverse(); Sophus::Vector6d d = T_r_c.log(); //根据变换向量的模长来判断运动的大小。过大的话返回false if ( d.norm() &gt; 5.0 ) &#123; cout&lt;&lt;"reject because motion is too large: "&lt;&lt;d.norm()&lt;&lt;endl; return false; &#125; //如果让面两项都没return，说明内点量不少，运动也没过大，return true return true; &#125; bool VisualOdometry::checkKeyFrame() &#123; //说一下这个是否为关键帧的判断，也很简单， //关键帧并不是之前理解的轨迹比较长了，隔一段选取一个，而还是每一帧的T都判断一下，比较大就说明为关键帧，说明在这一帧中，要么平移比较大，要么拐弯导致旋转比较大，所以添加，如果在运动上一直就是小运动，运动多久都不会添加为关键帧。 //另外上方的判断T计算错误也是运动过大，但是量级不一样，判断计算错误是要大于5，而关键帧，在配置文件中看只需要0.1就定义为关键帧了，所以0.1到5的差距，导致这两个函数并不冲突 SE3 T_r_c = ref_-&gt;T_c_w_ * T_c_w_estimated_.inverse(); Sophus::Vector6d d = T_r_c.log(); Vector3d trans = d.head&lt;3&gt;(); Vector3d rot = d.tail&lt;3&gt;(); if ( rot.norm() &gt;key_frame_min_rot || trans.norm() &gt;key_frame_min_trans ) return true; return false; &#125;//增加关键帧函数多了一步在第一帧时，将其对应的地图点全部添加进地图中。 void VisualOdometry::addKeyFrame() &#123; if ( map_-&gt;keyframes_.empty() ) &#123; // first key-frame, add all 3d points into map for ( size_t i=0; i&lt;keypoints_curr_.size(); i++ ) &#123; double d = curr_-&gt;findDepth ( keypoints_curr_[i] ); if ( d &lt; 0 ) continue; Vector3d p_world = ref_-&gt;camera_-&gt;pixel2world ( Vector2d ( keypoints_curr_[i].pt.x, keypoints_curr_[i].pt.y ), curr_-&gt;T_c_w_, d ); Vector3d n = p_world - ref_-&gt;getCamCenter(); n.normalize(); //上方求出构造地图点所需所有参数，3D点、模长、描述子、帧，然后构造一个地图点 MapPoint::Ptr map_point = MapPoint::createMapPoint( p_world, n, descriptors_curr_.row(i).clone(), curr_.get() ); //添加进地图 map_-&gt;insertMapPoint( map_point ); &#125; &#125; //一样的，第一帧添加进关键帧 map_-&gt;insertKeyFrame ( curr_ ); ref_ = curr_; &#125; //新增函数，增加地图中的点。随时的增删地图中的点，来跟随运动 void VisualOdometry::addMapPoints() &#123; // add the new map points into map //创建一个bool型的数组matched，大小为当前keypoints数组大小，值全为false vector&lt;bool&gt; matched(keypoints_curr_.size(), false); //首先这个match_2dkp_index_是新来的当前帧跟地图匹配时，好的匹配到的关键点在keypoins数组中的索引 //在这里将已经匹配的keypoint索引置为true for ( int index:match_2dkp_index_ ) matched[index] = true; //遍历当前keypoints数组 for ( int i=0; i&lt;keypoints_curr_.size(); i++ ) &#123; //如果为true，说明在地图中找到了匹配，也就意味着地图中已经有这个点了。直接continue if ( matched[i] == true ) continue; //如果没有continue的话，说明这个点在地图中没有找到匹配，认定为新的点， //下一步就是找到depth数据，构造3D点，然后构造地图点，添加进地图即可。 double d = ref_-&gt;findDepth ( keypoints_curr_[i] ); if ( d&lt;0 ) continue; Vector3d p_world = ref_-&gt;camera_-&gt;pixel2world ( Vector2d ( keypoints_curr_[i].pt.x, keypoints_curr_[i].pt.y ), curr_-&gt;T_c_w_, d ); Vector3d n = p_world - ref_-&gt;getCamCenter(); n.normalize(); MapPoint::Ptr map_point = MapPoint::createMapPoint( p_world, n, descriptors_curr_.row(i).clone(), curr_.get() ); map_-&gt;insertMapPoint( map_point ); &#125; &#125;//新增函数：优化地图。主要是为了维护地图的规模。删除一些地图点，在点过少时增加地图点等操作。 void VisualOdometry::optimizeMap() &#123; // remove the hardly seen and no visible points //删除地图点，遍历地图中的地图点。并分几种情况进行删除。 for ( auto iter = map_-&gt;map_points_.begin(); iter != map_-&gt;map_points_.end(); ) &#123; //如果点在当前帧都不可见了，说明跑的比较远，删掉 if ( !curr_-&gt;isInFrame(iter-&gt;second-&gt;pos_) ) &#123; iter = map_-&gt;map_points_.erase(iter); continue; &#125; //定义匹配率，用匹配次数/可见次数，匹配率过低说明经常见但是没有几次匹配。应该是一些比较难识别的点，也就是出来的描述子比较奇葩。所以删掉 float match_ratio = float(iter-&gt;second-&gt;matched_times_)/iter-&gt;second-&gt;visible_times_; if ( match_ratio &lt; map_point_erase_ratio_ ) &#123; iter = map_-&gt;map_points_.erase(iter); continue; &#125; double angle = getViewAngle( curr_, iter-&gt;second ); if ( angle &gt; M_PI/6. ) &#123; iter = map_-&gt;map_points_.erase(iter); continue; &#125; //继续，可以根据一些其他条件自己添加要删除点的情况 if ( iter-&gt;second-&gt;good_ == false ) &#123; // TODO try triangulate this map point &#125; iter++; &#125; //下面说一些增加点的情况，首先当前帧去地图中匹配时，点少于100个了， // 一般情况是运动幅度过大了，跟之前的帧没多少交集了，所以增加一下。 if ( match_2dkp_index_.size()&lt;100 ) addMapPoints(); //如果点过多了，多于1000个，适当增加释放率，让地图处于释放点的趋势。 if ( map_-&gt;map_points_.size() &gt; 1000 ) &#123; // TODO map is too large, remove some one map_point_erase_ratio_ += 0.05; &#125; //如果没有多于1000个，保持释放率在0.1，维持地图的体量为平衡态 else map_point_erase_ratio_ = 0.1; cout&lt;&lt;"map points: "&lt;&lt;map_-&gt;map_points_.size()&lt;&lt;endl; &#125;//取得一个空间点在一个帧下的视角角度。返回值是double类型的角度值。 double VisualOdometry::getViewAngle ( Frame::Ptr frame, MapPoint::Ptr point ) &#123; //构造发方法是空间点坐标减去相机中心坐标。得到从相机中心指向指向空间点的向量。 Vector3d n = point-&gt;pos_ - frame-&gt;getCamCenter(); //单位化 n.normalize(); //返回一个角度，acos()为反余弦， //向量*乘为：a*b=|a||b|cos&lt;a,b&gt; //所以单位向量的*乘得到的是两个向量的余弦值，再用acos()即得到角度，返回 //物理意义就是得到世界坐标系下看空间点和从相机坐标系下看空间点，视角差的角度。 return acos( n.transpose()*point-&gt;norm_ ); &#125;&#125; 在default.yaml中增加 1map_point_erase_ratio: 0.1 4.2 编译主要遇到的是代码出错（写错字符之类），已经改好了，没有其他需要改动的问题。运行run_vo即可。]]></content>
      <categories>
        <category>slam</category>
      </categories>
      <tags>
        <tag>vo实现</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv 添加 viz 模块]]></title>
    <url>%2F2019%2F09%2F26%2F20190926-opencv-%E6%B7%BB%E5%8A%A0-viz-%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[我在手撕视觉里程计的时候遇到了 #include &lt;opencv2/viz.hpp&gt; 而不存在viz.hpp的问题，百度一下，看起来很好解决，然后搞了一天半才搞定。。。。。说说遇到的坑，免得大家又进去了。 首先，我默认你安装了opencv3。 这里强调，是opencv3！！！！！ 这里是我遇到的最大的一个坑，我一直以为自己安装的是3，因为是从官网的3.4.7的github上直接clone源码编译的。然后安装viz死活不行，一看版本，4.1.。。。卸载再装，还是4.1.。。。 没法，我就去下载压缩包安装了。这个教程很多，安装很容易。 安装步骤：这里默认安装好了opencv3 1. 安装viz的依赖项1sudo apt-get install libvtk6-dev 2. 编译进入opencv3的源码文件夹 进入build或者release文件夹 1cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local -D WITH_TBB=ON -D BUILD_NEW_PYTHON_SUPPORT=ON -D WITH_V4L=ON -D WITH_QT=ON -D WITH_OPENGL=ON -D WITH_VTK=ON .. 主要是最后一个，VTK=on，保证打开VTK。 1sudo make -j4 这里特别慢。。。真的很慢。。。 1sudo make install 安装完成了。 3. 验证如果成功安装了viz，那么在 /usr/local/include/opencv2/ 中应该有viz的文件夹和viz.hpp。]]></content>
      <categories>
        <category>问题解决</category>
      </categories>
      <tags>
        <tag>opencv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++学习（5）--数组、指针与字符串]]></title>
    <url>%2F2019%2F09%2F25%2FC%2B%2B%E5%AD%A6%E4%B9%A0%EF%BC%885%EF%BC%89%2F</url>
    <content type="text"><![CDATA[1 数组数组是具有一定顺序关系的若干对象的集合体，组成数组的对象称为该数组的元素。 每个元素有 n 个下标的数组称为 n 维数组。 1.1 数组的声明与使用123456789101112131415#include&lt;iostream&gt;using namespace std;int main()&#123; int a[10],b[10]; for(int i=0;i&lt;10;i++)&#123; a[i]=i*2-1; b[10-i-1]=a[i]; &#125; for(int i=0;i&lt;10;i++)&#123; cout&lt;&lt;"a["&lt;&lt;i&lt;&lt;"]="&lt;&lt;a[i]&lt;&lt;" "; cout&lt;&lt;"b["&lt;&lt;i&lt;&lt;"]="&lt;&lt;b[i]&lt;&lt;endl; &#125; return 0;&#125; 1.2 数组的存储与初始化数组元素在内存中是顺序、连续存储的。 数组的初始化就是在声明数组时给部分或全部元素赋初值。 1.3 数组作为函数参数使用数组名传递数据时，传递的是地址。 123456789101112131415161718192021222324//使用数组名作为函数参数#include&lt;iostream&gt;using namespace std;void rowSum(int a[][4],int nRow)&#123; //计算二维数组a每行元素的值的和，nRow是行数 for(int i=0;i&lt;nRow;i++)&#123; for(int j=1;j&lt;4;j++) a[i][0]+=a[i][j]; &#125;&#125;int main()&#123; //主函数 int table[3][4]=&#123;&#123;1,2,3,4&#125;,&#123;2,3,4,5&#125;,&#123;3,4,5,6&#125;&#125;; //声明并初始化数组 for(int i=0;i&lt;3;i++)&#123; //输出数组元素 for(int j=0;j&lt;4;j++) cout&lt;&lt;table[i][j]&lt;&lt;" "; cout&lt;&lt;endl; &#125; rowSum(table,3); //调用子函数，计算各行和 for(int i=0;i&lt;3;i++) //输出计算结果 cout&lt;&lt;"Sum of row "&lt;&lt;i&lt;&lt;" is "&lt;&lt;table[i][0]&lt;&lt;endl; return 0;&#125; 1.4 对象数组声明一个一维对象数组的语句形式是:类名 数组名[常量表达式]; 每个数组元素都是一个对象，通过这个对象，便可以访问到它的公有成员，一般形式是:数组名[下标表达式].成员名 举例： 123456789101112131415161718//Point.h#ifndef LEARN_POINT_H#define LEARN_POINT_Hclass Point&#123; //类的定义public: //外部接口 Point(); Point(int x,int y); ~Point(); void move(int newX,int newY); int getX() const &#123;return x;&#125; int getY() const &#123;return y;&#125; static void showCount(); //静态函数成员private: //私有数据成员 int x,y; static int count;&#125;;#endif //LEARN_POINT_H 123456789101112131415161718192021222324252627282930//Point.cpp#include&lt;iostream&gt;#include "Point.h"using namespace std;Point::Point()&#123; x=y=0; cout&lt;&lt;"Default Constructor called."&lt;&lt;endl; count++;&#125;Point::Point(int x,int y):x(x),y(y)&#123; cout&lt;&lt;"Constructor called."&lt;&lt;endl;&#125;Point::~Point()&#123; cout&lt;&lt;"Destructor called."&lt;&lt;endl; count--;&#125;void Point::move(int newX,int newY)&#123; cout&lt;&lt;"Moving the point to ("&lt;&lt;newX&lt;&lt;","&lt;&lt;newY&lt;&lt;")"&lt;&lt;endl; x=newX; y=newY;&#125;void Point::showCount()&#123;cout &lt;&lt; "the number of points = "&lt;&lt;count&lt;&lt; endl;&#125; 12345678910111213141516//对象数组应用举例#include "Point.h"#include&lt;iostream&gt;using namespace std;int Point::count=0;int main()&#123; cout&lt;&lt;"Entering main..."&lt;&lt;endl; Point a[2]; Point::showCount(); for(int i=0;i&lt;2;i++) a[i].move(i+10,i+20); cout&lt;&lt;"Exiting main..."&lt;&lt;endl; return 0;&#125; CMakeLists.txt 1234567cmake_minimum_required(VERSION 3.14)project(learn)set(CMAKE_CXX_STANDARD 14)add_library(point Point.cpp)add_executable(learn main.cpp)target_link_libraries( learn point ) 2 指针123456789101112//指针基础#include&lt;iostream&gt;using namespace std;int main()&#123; int i; //定义int型数i int *ptr=&amp;i; //声明指针ptr,并取i的地址赋给ptr 相当于int *ptr; ptr=&amp;i; i=10; //int型数赋初值 cout&lt;&lt;"i="&lt;&lt;i&lt;&lt;endl; //输出int型数的值 cout&lt;&lt;"*ptr="&lt;&lt;*ptr&lt;&lt;endl; //输出int型指针所指地址的内容 return 0;&#125; 输出 12i=10*ptr=10 1234567891011// 指针处理数组#include&lt;iostream&gt;using namespace std;int main() &#123; int a[10]=&#123;1,2,3,4,5,6,7,8,9,0&#125;; for(int *p=a;p&lt;(a+10);p++) cout&lt;&lt;*p&lt;&lt;" "; cout&lt;&lt;endl; return 0;&#125; 1234567891011121314//指针数组 二维数组举例#include&lt;iostream&gt;using namespace std;int main()&#123; int array2[3][3]=&#123;&#123;11,12,13&#125;,&#123;21,22,23&#125;,&#123;31,32,33&#125;&#125;; for(int i=0;i&lt;3;i++)&#123; for(int j=0;j&lt;3;j++) cout&lt;&lt;*(*(array2+i)+j)&lt;&lt;" "; //逐个输出二维数组第i行元素值 cout&lt;&lt;endl; &#125; return 0;&#125; 1234567891011121314151617181920212223//指针作为函数参数//读入3个浮点数，将整数部分和小数部分分别输出#include&lt;iostream&gt;using namespace std;//将实数x分成整数部分和小数部分，形参intPart、fracPart是指针void splitFloat(float x,int *intPart,float *fracPart)&#123; *intPart=static_cast&lt;int&gt;(x); //取x的整数部分 *fracPart=x-*intPart; //取x的小数部分&#125;int main()&#123; cout&lt;&lt;"Enter 3 float point numbers:"&lt;&lt;endl; for(int i=0;i&lt;3;i++)&#123; float x,f; int n; cin&gt;&gt;x; splitFloat(x,&amp;n,&amp;f); //变量地址作为实参 cout&lt;&lt;"Integer Part="&lt;&lt;n&lt;&lt;" Fraction Part="&lt;&lt;f&lt;&lt;endl; &#125; return 0;&#125; 123456789101112131415161718192021222324252627282930313233//指向函数的指针实例#include&lt;iostream&gt;using namespace std;void printStuff(float)&#123; cout&lt;&lt;"This is the print stuff function."&lt;&lt;endl;&#125;void printMessage(float data)&#123; cout&lt;&lt;"The data to be listed is "&lt;&lt;data&lt;&lt;endl;&#125;void printFloat(float data)&#123; cout&lt;&lt;"The data to be printed is "&lt;&lt;data&lt;&lt;endl;&#125;const float PI=3.14159f;const float TWO_PI=PI*2.0f;int main()&#123; //主函数 void (*functionPointer)(float); //函数指针 printStuff(PI); functionPointer=printStuff; //函数指针指向printStuff functionPointer(PI); //函数指针调用 functionPointer=printMessage; //函数指针指向printMessage functionPointer(TWO_PI); //函数指针调用 functionPointer(13.0); //函数指针调用 functionPointer=printFloat; //函数指针指向printFloat functionPointer(PI); //函数指针调用 printFloat(PI); return 0;&#125; 1234567891011121314151617181920212223242526//访问对象的公有成员函数的不同方式#include&lt;iostream&gt;using namespace std;class Point&#123; //类的定义public: //外部接口 Point(int x=0,int y=0):x(x),y(y)&#123;&#125; //构造函数 int getX() const &#123;return x;&#125; //返回x int getY() const &#123;return y;&#125; //返回yprivate: //私有数据 int x,y;&#125;;int main()&#123; //主函数 Point a(4,5); //定义对象a Point *p1=&amp;a; //定义对象指针并初始化 int (Point::*funcPtr)() const=&amp;Point::getX; //定义成员函数指针并初始化 cout&lt;&lt;(a.*funcPtr)()&lt;&lt;endl; //（1）使用成员函数指针和对象名访问成员函数 cout&lt;&lt;(p1-&gt;*funcPtr)()&lt;&lt;endl; //（2）使用成员函数指针和对象指针访问成员函数 cout&lt;&lt;a.getX()&lt;&lt;endl; //（3）使用对象名访问成员函数 cout&lt;&lt;p1-&gt;getX()&lt;&lt;endl; //（4）使用对象指针访问成员函数 return 0;&#125;]]></content>
      <categories>
        <category>语言学习</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ORB_SLAM2 程序解读（3）-- ORB特征提取 ORBextractor.cc]]></title>
    <url>%2F2019%2F09%2F16%2FORB_SLAM2%20%E7%A8%8B%E5%BA%8F%E8%A7%A3%E8%AF%BB%EF%BC%883%EF%BC%89%2F</url>
    <content type="text"><![CDATA[1 ORB 特征点特征点由关键点（Key-point）和描述子（Descriptor）两部分组成。ORB特征点（Oriented FAST and Rotated BRIEF）是由Oriented FAST角点和 BRIEF （Binary Robust Independent Elementary Features）描述子构成，其计算速度是sift特征点的100倍，是surf特征点的10倍。 FAST和BRIEF在SLAM十四讲里都有介绍，这里主要说一下尺度不变性和旋转特性。 1.1 尺度不变性*尺度不变性 *是由构建图像金字塔，并在金字塔的每一层上检测角点来实现。 （1）对图像做不同尺度的高斯模糊 为了让尺度体现其连续性，高斯金字塔在简单降采样的基础上加上了高斯滤波。将图像金字塔每层的一张图像使用不同参数做高斯模糊，使得金字塔的每层含有多张高斯模糊图像，将金字塔每层多张图像合称为一组(Octave)，金字塔每层只有一组图像，组数和金字塔层数相等，使用下列公式计算，每组含有多张(也叫层Interval)图像。另外，降采样时，高斯金字塔上一组图像的初始图像(底层图像)是由前一组图像的倒数第三张图像隔点采样得到的。$$n=\log _{2}{\min (M, N)}-t, t \in\left[0, \log _{2}{\min (M, N)}\right]$$其中M、N为原图像的大小，t为塔顶图像的最小维数的对数值。如，对于大小为512x512的图像，金字塔上各层图像的大小如图1所示，当塔顶图像为4x4时，n=7，当塔顶图像为2x2时，n=8。 （2）对图像做降采样(隔点采样) 总结： 设置一个比例因子scaleFactor（opencv默认为1.2）和金字塔的层数nlevels（opencv默认为8）。将原图像按比例因子缩小成nlevels幅图像。缩放后的图像为：I’= I/scaleFactork(k=1,2,…, nlevels)。nlevels幅不同比例的图像提取特征点总和作为这幅图像的FAST特征点。 1.2 旋转特性特征的旋转 是由灰度质心法（Intensity Centroid）实现的。 灰度质心法在十四讲里有详细的讲解。这里放更加直观的公式。$$x_{0}=\frac{\sum_{i=1}^{m} \sum_{j=1}^{n} x_{i} f_{i j}}{\sum_{i=1}^{m} \sum_{j=1}^{n} f_{i j}}$$ $$y_{0}=\frac{\sum_{i=1}^{m} \sum_{j=1}^{n} y_{j} f_{i j}}{\sum_{i=1}^{m} \sum_{j=1}^{n} f_{i j}}$$ $$f_{ij}=\begin{cases} 0，像素灰度值&lt;T\ f_{ij}，像素灰度值\geq T\end{cases}$$ 其中，图片大小为$m\times n $，$(x_0,y_0)$表示质心，$x_i$表示第$i$行的坐标，$y_j$表示第$j$列的坐标，$f_{ij}$表示第$i$行第$j$列的像素值。 2 代码实现ORB-SLAM2中提取ORB特征是由ORBextractor.cc实现的。 该方法流程：1.对于输入的图片，计算其图像金字塔；2.提取图像金字塔中各层图像的关键点；3.计算提取出的关键点对应的描述子； 具体代码注释参见 https://github.com/LiDaiyang1205/SLAM_ldy]]></content>
      <categories>
        <category>slam</category>
      </categories>
      <tags>
        <tag>slam实现</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ORB_SLAM2 程序解读（2）-- System.cc]]></title>
    <url>%2F2019%2F09%2F11%2FORB_SLAM2%20%E7%A8%8B%E5%BA%8F%E8%A7%A3%E8%AF%BB%EF%BC%882%EF%BC%89%2F</url>
    <content type="text"><![CDATA[1 System.ccSystem.cc是ORB-SLAM2的主程序，也是ORB-SLAM2系统的入口。 System函数中的关键代码及标注 12345678910111213mpVocabulary = new ORBVocabulary();//读取ORB词袋mpKeyFrameDatabase = new KeyFrameDatabase(*mpVocabulary);//创建关键帧数据库mpMap = new Map();//创建地图对象mpFrameDrawer = new FrameDrawer(mpMap);//创建显示关键帧的窗口mpMapDrawer = new MapDrawer(mpMap, strSettingsFile);//创建显示地图的窗口mpTracker = new Tracking(this, mpVocabulary, mpFrameDrawer, mpMapDrawer,mpMap, mpKeyFrameDatabase, strSettingsFile, mSensor);//初始化Tracking线程，该线程在主循环中mpLocalMapper = new LocalMapping(mpMap, mSensor==MONOCULAR);//初始化Local Mapping线程mptLocalMapping = new thread(&amp;ORB_SLAM2::LocalMapping::Run,mpLocalMapper);//启动线程mpLoopCloser = new LoopClosing(mpMap, mpKeyFrameDatabase, mpVocabulary, mSensor!=MONOCULAR);//初始化LoopClosing线程mptLoopClosing = new thread(&amp;ORB_SLAM2::LoopClosing::Run, mpLoopCloser);//启动线程mpViewer = new Viewer(this, mpFrameDrawer,mpMapDrawer,mpTracker,strSettingsFile);//初始化窗口mptViewer = new thread(&amp;Viewer::Run, mpViewer);//启动，前面会有一个是否可视化的判断 其他函数接口： 123456789cv::Mat System::TrackStereo(const cv::Mat &amp;imLeft, const cv::Mat &amp;imRight, const double &amp;timestamp)//追踪双目数据，返回mpTracker-&gt;GrabImageStereo(imLeft,imRight,timestamp)cv::Mat System::TrackRGBD(const cv::Mat &amp;im, const cv::Mat &amp;depthmap, const double &amp;timestamp)//追踪深度相机数据，返回mpTracker-&gt;GrabImageRGBD(im,depthmap,timestamp)cv::Mat System::TrackMonocular(const cv::Mat &amp;im, const double &amp;timestamp)//追踪单目相机数据返回mpTracker-&gt;GrabImageMonocular(im,timestamp)void System::ActivateLocalizationMode()//mbActivateLocalizationMode = true，激活 mbActivateLocalizationModevoid System::DeactivateLocalizationMode()// mbDeactivateLocalizationMode = true，激活 mbDeactivateLocalizationModevoid System::Shutdown()//判断运行是否结束，结束则关闭系统void System::SaveTrajectoryTUM(const string &amp;filename)//求数据集中每一帧的位姿void System::SaveKeyFrameTrajectoryTUM(const string &amp;filename)//求数据集中每一关键帧的位姿void System::SaveTrajectoryKITTI(const string &amp;filename)//求数据集中每一帧的位姿 具体代码注释参见 https://github.com/LiDaiyang1205/SLAM_ldy]]></content>
      <categories>
        <category>slam</category>
      </categories>
      <tags>
        <tag>slam实现</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ORB_SLAM2 程序解读（1）-- 概述与ros_rgbd.cc]]></title>
    <url>%2F2019%2F09%2F10%2FORB_SLAM2%20%E7%A8%8B%E5%BA%8F%E8%A7%A3%E8%AF%BB%EF%BC%881%EF%BC%89%2F</url>
    <content type="text"><![CDATA[0 概述ORB-SLAM是由Raul Mur-Artal，J. M. M. Montiel和Juan D. Tardos于2015年发表在IEEE Transactions on Robotics。代码主页网址为：https://github.com/raulmur/ORB_SLAM2 。 我之所以选择ORB_SLAM2，是因为之前在研究回环检测，回环检测涉及到特征点的采集和处理，ORB就是特征点最常用的采集方法。为了实现ORB_SLAM2 的使用，我从安装依赖项到运行测试都写了完整的文档。实现ORBSLAM2的运行后，我发现这个包建图没问题，但是没有保存地图和加载地图的功能，于是又通过看博客改源码，实现了地图保存和地图加载。之前都是看着别人的博客改，自己对这个包一知半解，所以这里系统学习一下这个库。 ORB-SLAM是一个基于特征点的实时单目SLAM系统，在大规模的、小规模的、室内室外的环境都可以运行。该系统对剧烈运动也很鲁棒，支持宽基线的闭环检测和重定位，包括全自动初始化。该系统包含了所有SLAM系统共有的模块：跟踪（Tracking）、建图（Mapping）、重定位（Relocalization）、回环检测（Loop closing）。由于ORB-SLAM系统是基于特征点的SLAM系统，故其能够实时计算出相机的轨线，并生成场景的稀疏三维重建结果。 贡献： 1 系统架构ORB-SLAM其中的关键点如下图所示： ORB-SLAM主要分为三个线程进行，分别是Tracking、LocalMapping和LoopClosing。ORB-SLAM2的工程非常清晰漂亮，三个线程分别存放在对应的三个文件中，分别是Tracking.cpp、LocalMapping.cpp和LoopClosing.cpp文件中，很容易找到。 （1）跟踪（Tracking） 这一部分主要工作是从图像中提取ORB特征，根据上一帧进行姿态估计，或者进行通过全局重定位初始化位姿，然后跟踪已经重建的局部地图，优化位姿，再根据一些规则确定新的关键帧。 （2）建图（LocalMapping） 这一部分主要完成局部地图构建。包括对关键帧的插入，验证最近生成的地图点并进行筛选，然后生成新的地图点，使用局部光束平差调整（Local BA），最后再对插入的关键帧进行筛选，去除多余的关键帧。 （3）闭环检测（LoopClosing） 这一部分主要分为两个过程，分别是闭环探测和闭环校正。闭环检测先使用WOB进行探测，然后通过Sim3算法计算相似变换。闭环校正，主要是闭环融合和Essential Graph的图优化。 2 代码结构ORB-SLAM2代码写的很整洁。打开ORB-SLAM2的文件夹，你会看到以下文件夹： Examples文件夹：存放的分别是基于单目、双目、RGBD的实例程序 include文件夹：存放的是头文件，ORB-SLAM2可以被当作一个库来使用，很多函数都可以直接调用 src文件夹：存放的是和include对应的源文件，要讲的代码在该文件夹下 Thirdparty文件夹：存放的是用到的第三方库，优化库g2o在该文件夹下 Vocabulary文件夹：存放的是回环检测中BoW用到的视觉词典名为ORBvoc.txt build.sh:配置文件 其中变量命名规则： “p”表示指针数据类型；”n”表示int类型；“b”表示bool类型；”s”表示set类型； ”v”表示vevtor数据类型；“l”表示list数据类型；”m”表示类成员变量 3 ros_rgbd.ccORB_SLAM的代码非常整齐，简洁，便于阅读。由于我们用的是Kinect 2，有自己的slam小车，用的是ROS实现，所以使用的是/Examples/ROS/ORB_SLAM2/src/ros_rgbd.cc这个主程序。 1 定义了图像采集的类 123456789class ImageGrabber //图像采集&#123;public: ImageGrabber(ORB_SLAM2::System* pSLAM):mpSLAM(pSLAM)&#123;&#125; void GrabRGBD(const sensor_msgs::ImageConstPtr&amp; msgRGB,const sensor_msgs::ImageConstPtr&amp; msgD); ORB_SLAM2::System* mpSLAM;&#125;; 2 定义 ImageGrabber::GrabRGBD 函数 123456789101112131415161718192021222324252627void ImageGrabber::GrabRGBD(const sensor_msgs::ImageConstPtr&amp; msgRGB,const sensor_msgs::ImageConstPtr&amp; msgD)&#123; // Copy the ros image message to cv::Mat. cv_bridge::CvImageConstPtr cv_ptrRGB; // RGB信息 try &#123; cv_ptrRGB = cv_bridge::toCvShare(msgRGB); &#125; catch (cv_bridge::Exception&amp; e) &#123; ROS_ERROR("cv_bridge exception: %s", e.what()); return; &#125; cv_bridge::CvImageConstPtr cv_ptrD; // 深度信息 try &#123; cv_ptrD = cv_bridge::toCvShare(msgD); &#125; catch (cv_bridge::Exception&amp; e) &#123; ROS_ERROR("cv_bridge exception: %s", e.what()); return; &#125; mpSLAM-&gt;TrackRGBD(cv_ptrRGB-&gt;image,cv_ptrD-&gt;image,cv_ptrRGB-&gt;header.stamp.toSec());&#125; 2 主函数 123//初始化启动rosros::init(argc, argv, "RGBD");ros::start(); 12//创建ORB_SLAM2系统对象ORB_SLAM2::System SLAM(argv[1],argv[2],ORB_SLAM2::System::RGBD,true); 12//创建ImagGrabb 对象 igbImageGrabber igb(&amp;SLAM); 123456//读取rgb和深度信息message_filters::Subscriber&lt;sensor_msgs::Image&gt; rgb_sub(nh,"/kinect2/qhd/image_color",1);message_filters::Subscriber&lt;sensor_msgs::Image&gt; depth_sub(nh,"/kinect2/qhd/image_depth_rect",1);typedef message_filters::sync_policies::ApproximateTime&lt;sensor_msgs::Image, sensor_msgs::Image&gt; sync_pol;message_filters::Synchronizer&lt;sync_pol&gt; sync(sync_pol(10),rgb_sub,depth_sub);sync.registerCallback(boost::bind(&amp;ImageGrabber::GrabRGBD,&amp;igb,_1,_2)); 12345678//slam系统操作，加载、保存、关闭 //SLAM.LoadMap("/home/zj224/ORB_SLAM2/Examples/ROS/ORB_SLAM2/map.bin");//load the mapros::spin();// Stop all threadsSLAM.Shutdown();SLAM.SaveMap("map.bin"); //save the map// Save camera trajectorySLAM.SaveKeyFrameTrajectoryTUM("KeyFrameTrajectory.txt");]]></content>
      <categories>
        <category>slam</category>
      </categories>
      <tags>
        <tag>slam实现</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++学习（4）--数据的共享与保护]]></title>
    <url>%2F2019%2F09%2F10%2FC%2B%2B%E5%AD%A6%E4%B9%A0%EF%BC%884%EF%BC%89%2F</url>
    <content type="text"><![CDATA[1 标识符的作用域与可见性1.1 作用域作用域是一个标识符在程序正文中有效的区域。 (1) 函数原型作用域 在函数原型声明时形式参数的作用范围就是函数原型作用域。 (2) 局部作用域 函数形参列表中形参的作用域，从形参列表中的声明处开始，到整个函数体结束之处为止。 函数体内声明的变量，其作用域从声明处开始，一直到声明所在的块结束的大括号为止。 具有局部作用域的变堡也称为局部变量。 (3) 类作用域 类可以被看成是一组有名成员的集合，类$ X $的成员 $m$ 具有类作用域，对 $m $的访问方式有如下 3 种。(1) 如果在 $X $的成员函数中没有声明同名的局部作用域标识符，那么在该函数内可以直接访问成员 $m $。也就是说$m $在这样的函数中都起作用。(2) 通过表达式 $x.m $或者$ x::m$ 。这正是程序中访问对象成员的最基本方法。(3) 通过 ptr-&gt;m 这样的表达式，其中 ptr 为指向 X 类的一个对象的指针。 (4) 命名空间作用域 具有命名空间作用域的变量也称为全局变量。 123456789101112131415161718192021222324252627//作用域实例#include &lt;iostream&gt;using namespace std;int i; //在全局命名空间中的全局变量namespace Ns&#123; int j; //在 NS 命名空间中的全局变量&#125;// 主函数int main()&#123; i = 5; //为全局变量 i 赋值 Ns::j=6; //为全局变星]赋值 &#123; using namespace Ns; //使得在当前块中可以直接引用 Ns 命名空间的标识符 int i; //局部变量,局部作用域 i = 7; cout&lt;&lt;"i="&lt;&lt;i&lt;&lt;endl; //输出 7 cout&lt;&lt;"j="&lt;&lt;j&lt;&lt;endl; //输出 6 &#125; cout&lt;&lt;"i="&lt;&lt;i&lt;&lt;endl;//输出 5 return 0;&#125; 1.2 可见性从标识符引用的角度，来看标识符的有效范围，即标识符的可见性。程序运行到某一点，能够引用到的标识符，就是该处可见的标识符。 作用域可见性的一般规则如下： 标识符要声明在前，引用在后。 在同一作用域中，不能声明同名的标识符。 在没有互相包含关系的不同的作用域中声明的同名标识符，互不影响。 如果在两个或多个具有包含关系的作用域中声明了同名标识符，则外层标识符在内层不可见。 2 对象的生存期对象(包括简单变量)都有诞生和消失的时刻。对象从诞生到结束的这段时间就是它的生存期。 2.1 静态生存期如果对象的生存期与程序的运行期相同，则称它具有静态生存期。在命名空间作用域中声明的对象都是具有静态生存期的。如果要在函数内部的局部作用域中声明具有静态生存期的对象，则要使用关键字 $static$ 。 2.2 动态生存期在局部作用域中声明的具有动态生存期的对象，习惯上也称为局部生存期对象。局部生存期对象诞生于声明点，结束于声明所在的块执行完毕之时。 12345678910111213141516171819202122232425262728293031323334353637//变量的生存期与可见性#include&lt;iostream&gt;using namespace std;int i=1; //i为全局变量，具有静态生存期void other()&#123; //a,b为静态局部变量，具有全局寿命，局部可见，只第一次进入函数时被初始化 static int a=2; static int b; //c为局部变量，具有动态生存期，每次进入函数时都初始化 int c=10; a+=2; i+=32; c+=5; cout&lt;&lt;"---OTHER---"&lt;&lt;endl; cout&lt;&lt;" i: "&lt;&lt;i&lt;&lt;" a: "&lt;&lt;a&lt;&lt;" b: "&lt;&lt;b&lt;&lt;" c: "&lt;&lt;c&lt;&lt;endl; b=a;&#125;int main()&#123; //a为静态局部变量，具有全局寿命，局部可见 static int a; //b,c为局部变量，具有动态生存期 int b=-10; int c=0; cout&lt;&lt;"---MAIN---"&lt;&lt;endl; cout&lt;&lt;" i: "&lt;&lt;i&lt;&lt;" a: "&lt;&lt;a&lt;&lt;" b: "&lt;&lt;b&lt;&lt;" c: "&lt;&lt;c&lt;&lt;endl; c+=8; other(); cout&lt;&lt;"---MAIN---"&lt;&lt;endl; cout&lt;&lt;" i: "&lt;&lt;i&lt;&lt;" a: "&lt;&lt;a&lt;&lt;" b: "&lt;&lt;b&lt;&lt;" c: "&lt;&lt;c&lt;&lt;endl; i+=10; other(); return 0;&#125; 输出 12345678---MAIN--- i: 1 a: 0 b: -10 c: 0---OTHER--- i: 33 a: 4 b: 0 c: 15---MAIN--- i: 33 a: 0 b: -10 c: 8---OTHER--- i: 75 a: 6 b: 4 c: 15 1234567891011121314151617181920212223242526272829303132333435363738394041424344//具有静态和动态生存期对象的时钟程序#include&lt;iostream&gt;using namespace std;class Clock&#123; //时钟类定义 public: //外部接口 Clock(); void setTime(int newH,int newM,int newS); //3个形参均具有函数原型作用域 void showTime(); private: //私有数据成员 int hour,minute,second;&#125;; //时钟类成员函数的实现Clock::Clock():hour(0),minute(0),second(0)&#123;&#125; //构造函数 void Clock::setTime(int newH,int newM,int newS)&#123; //3个形参均具有局部作用域 hour=newH; minute=newM; second=newS;&#125; void Clock::showTime()&#123; cout&lt;&lt;hour&lt;&lt;":"&lt;&lt;minute&lt;&lt;":"&lt;&lt;second&lt;&lt;endl;&#125;Clock globClock; //声明对象globClock，具有静态生存期，命名空间作用域 //由默认构造函数初始化为0：0：0int main()&#123; //主函数 cout&lt;&lt;"First time output:"&lt;&lt;endl; //引用具有命名空间作用域的对象globClock globClock.showTime(); //对象的成员函数具有类作用域 //显示0：0：0 globClock.setTime(8,30,30); //将时间设置为8：30：30 Clock myClock(globClock); //声明具有块作用域的对象myClock //调用复制构造函数，以globClock为初始值 cout&lt;&lt;"Second time output:"&lt;&lt;endl; myClock.showTime(); //引用具有块作用域的对象myClock //输出8:30:30 return 0; &#125; 输出 1234First time output:0:0:0Second time output:8:30:30 3 类的静态成员3.1 静态数据成员如果某个属性为整个类所共有，不属于任何一个具体对象，则采用 static 关键字来声明为静态成员。 类属性是描述类的所有对象共同特征的一个数据项，对于任何对象实例，它的属性值是相同的。 静态数据成员具有静态生存期。由于静态数据成员不属于任何一个对象，因此可以通过类名对它进行访问，一般的用法是”类名::标识符”。 3.2 静态函数成员所谓静态成员函数就是使用 static 关键字声明的函数成员。同静态数据成员一样，静态成员函数也属于整个类，由同一个类的所有对象共同拥有，为这些对象所共享。 静态成员函数可以直接访问该类的静态数据和函数成员。而访问非静态成员，必须通过对象名。 123456789101112131415161718192021222324252627282930313233343536373839404142//具有静态数据和函数成员的Point类#include&lt;iostream&gt;using namespace std;class Point&#123; //Point类定义public: //外部接口 Point(int x=0,int y=0):x(x),y(y)&#123; //构造函数 //在构造函数中对count累加，所有对象共同维护同一个count count++; &#125; Point(Point &amp;p)&#123; //复制构造函数 x=p.x; y=p.y; count++; &#125; ~Point()&#123;count--;&#125; int getX()&#123;return x;&#125; int getY()&#123;return y;&#125; static void showCount()&#123; //静态函数成员 cout&lt;&lt;" Object count="&lt;&lt;count&lt;&lt;endl; &#125;private: //私有数据成员 int x,y; static int count; //静态数据成员声明，用于记录点的个数&#125;;int Point::count=0; //静态数据成员定义和初始化，使用类名限定int main()&#123; //主函数 Point::showCount(); Point a(4,5); //定义对象a，其构造函数会使count增1 cout&lt;&lt;"Point A: "&lt;&lt;a.getX()&lt;&lt;","&lt;&lt;a.getY(); Point::showCount(); //输出对象个数 Point b(a); //定义对象b，其构造函数会使count增1 cout&lt;&lt;"Point B: "&lt;&lt;b.getX()&lt;&lt;","&lt;&lt;b.getY(); Point::showCount(); //输出对象个数 return 0;&#125;]]></content>
      <categories>
        <category>语言学习</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ORBSLAM2 论文]]></title>
    <url>%2F2019%2F09%2F09%2FORBSLAM2%E8%AE%BA%E6%96%87%2F</url>
    <content type="text"><![CDATA[论文下载：https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7946260 摘要ORB-SLAM2是基于单目、双目和RGB-D相机的一套完整的SLAM方案。它能够实现地图重用、回环检测和重新定位的功能。无论是在室内的小型手持设备，还是到工厂环境的无人机和城市里驾驶的汽车，ORB-SLAM2都能够在标准的CPU上进行实时工作。ORB-SLAM2在后端上采用的是基于单目和双目的光束法平差优化（BA）的方式，这个方法允许”米“级别的轨迹精确度评估。此外，ORB-SLAM2包含一个轻量级的定位模式，该模式能够在允许零点漂移的条件下，利用视觉里程计来追踪未建图的区域并且匹配特征点。 我们用29个广泛使用的公共数据测试的结果显示，在大多数情况下，本文方案比此前方案精度更高，此外，我们开源了ORB-SLAM2源代码，不仅仅是为了整个SLAM领域，同时也希望能够为其他领域研究者提供一套SLAM的解决方案。 I. 引言SLAM（同时定位与地图重建）在过去的20年中，一直是计算机视觉和机器人领域的热门话题，同时也吸引了很多高科技公司的关注。SLAM技术是在未知的环境当中建立一个地图并且能够在地图当中实时的定位。在不同类型的传感器当中，相机十分廉价，并且能够提供丰富的环境信息，受到研究者的青睐。相机提供的图像信息可以用作鲁棒的和精确的位置识别。位置识别是SLAM系统中回环检测的关键模块（例如，当传感器检测到一个已经建好图的位置的时候，可以进行修正在探索过程中的误差）以及，能够修正由于剧烈的震动或者在系统进行初始化的时候在相机跟踪失败后的重新定位。因此以相机为核心的视觉SLAM在过去的一年中得到快速的发展。 视觉SLAM仅仅通过一个单目相机就能够完成。单目相机也是最便宜也是最小巧的传感器设备。然而深度信息无法从单目相机中观测到，地图的尺度和预测轨迹是未知的。此外，由于不能从第一帧当中进行三角测量化，单目视觉SLAM系统的启动往往需要多个视角或者滤波技术才能产生一个初始化的地图。最后，单目SLAM可能会造成尺度漂移,以及在探索的过程中执行纯旋转的时候可能会失败。通过使用一个双目或者RGB-D相机将会解决这些问题，并且能够成为一种更加有效的视觉SLAM的解决方案。 在这篇文章当中，我们在单目ORB-SLAM[1]的基础上提出ORB-SLAM2，有以下贡献： 1、这是首个基于单目、双目和RGB-D相机的开源SLAM方案，这个方案包括，回环检测、地图重用和重定位。 2、我们的RGB-D结果说明，光速法平差优化（BA）比ICP或者光度和深度误差最小方法的更加精确。 3、通过匹配远处和近处的双目匹配的点和单目观测，我们的双目的结果比直接使用双目系统更加精确。 4、针对无法建图的情况，提出了一个轻量级的定位模式 ，能够更加有效的重用地图。 ​ (a)双目输入：带有多次回环检测的城市环境轨迹和稀疏重建 (b) RGB-D输入：房间关键帧和稠密点云已经一次回环检测图，这些点云通过对深度图的关键帧的位姿进行映射得到，不进行渲染(融合) 图1 是ORB-SLAM2处理双目和RGB-D输入评估相机的轨迹并建图。这个系统能够保证在高精度和鲁棒性的前提下，做到在标准CPU上进行实时的，回环检测，重定位以及地图重用。 图a中显示的是双目和RGB输入下的ORBSLAM2的输出。双目例子显示的是最后轨迹和稀疏重建的地图。这里的数据集来源于KITTI的Sequence00数据集。这个城市数据集是ORB-SLAM2多次成功提取特征，并且回环检测而来。 RGB-D例子是来源于TUM 的RGB-D 数据库中的fr1_room的数据集，并且进行关键帧的位姿评估而来。通过评估关键帧的位姿，映射深度图，最终形成一个稠密的点云图。指的注意的一点是，ORB-SLAM2虽不像Kinect Fusion一样进行数据融合，但是却能够很精确的估计关键帧的位姿。更多的例子在附件视频中展示。在余下的篇章当中，我们将会在第二部分讨论相关的工作。在第三部分谈论ORB-SLAM2系统框架。第四部分评价ORB-SLAM2，第五部分得出结论。 II. 相关工作在这个章节，我们将会讨论双目和RGB-D SLAM的相关工作。评估部分我们放在第四部分，本章我们主要讨论的是SLAM的方法。 2.1 双目SLAM最早研究双目SLAM方案的是Paz 等人[5]，基于条件独立分割和扩展SLAM，其显著特点是能够在大场景中运行。更重要的是，这是第一个使用近特征点和远特征点（例如，由于双目相机差异较小，导致点的深度不能准确的估计）的双目SLAM系统，使用一个逆深度参数进行估计。经验值表明如果深度小于40倍双目的基线，那么这个点就能被三角测量化。我们就是跟随的这样思想来处理远近不同的特征点，具体解释放在第三部分。 目前大多数双目系统都是基于特征匹配和局部BA优化的方式，来获得尺度。Strasdat等人[8]采用在一个输出窗口的关键帧[7]和位姿的BA联合优化算法。在全局不一致性的情况下，通过限制窗口的大小的方式，实现了约束了时间的复杂程度的目的。Mei等人[9]在限定时间复杂度的条件下，使用路标和位姿相关性的方式的实现了RSLAM解决方案，并且提出和实现了在活动的区域的BA优化算法。即使在全局不一致的条件下，RSLAM也能够进行闭环，同时会扩大回环两侧的活动区域。 Pire等人[10]把局部的BA运用到了邻近S-PTAM上面来，但是，这种方法缺少大量的回环检测。与此相似的是，我们对局部关键帧采用BA优化，因此，这个地图的大小和复杂程度的大小是独立的，进而，我们可以在一个大场景当中运行。然而，我们目标是建立一个全局不变的地图。因此，我们的系统首先在回环的两端执行。这与RSLAM很相似，以便于能够使用旧的地图进行定位，之后进行位姿估计，即将回环产生的累积漂移最小化。 Engel等人[11]提出邻近双目LSD-SLAM方案，采用的是一种直接的半稠密方法，最小化梯度的图像区域中的光度误差。这种方法希望能够在不依赖特征提取的条件下，能够在纹理不清或者模糊运动的过程中获得更高的鲁棒性。然而，直接法的性能会由于滚动（卷帘）快门，或者非朗伯反射的未建模的因素影响而下降。 2.2 RGB-D SLAM最早和最著名的RGB-DSLAM系统是有Newcombe等人[4]提出的KinectFusion，这种方法将深度数据进行融合，深度数据来源于传感器到深度模型，常常使用ICP算法来跟踪相机的位姿。由于使用体素表示和缺乏回环检测，这种算法只能工作在小的工作空间。Whelan 等人[12]提出的Kintinuous能够在大环境中运行。它通过使用一个滚动循环缓冲器和包括使用位置定位和位姿优化来达到回环检测的目的。 第一个开源的RGBD-SLAM方案是由Endres[13]提出的，这是一种基于特征点提取的系统，他的前端采用提取和匹配特征点和ICP来计算帧与帧之间的运动。 后端采用位姿图优化的方式，回环检测约束条件来源于一个启发式搜索。相似的是，Kerl 等人[14]提出的DVO-SLAM，是在关键帧与关键帧之间的优化位姿图，视觉里程计通过计算最小化光度和深度误差来计算约束条件。DVO-SLAM同时在以前的所有帧当中，搜索回环的候选者，而不依赖于位置识别。 Whelan等人[15]提出的邻近ElasticFusion算法，是建立在基于确定环境的地图。这是一种以地图为中心的方法。这种方法忽略了非刚性形变地图的位姿和回环检测的性能，也是不是一个标准的位姿图优化。这种方法在重建和定位的精度都是十分优秀的，但是目前的应用十分有限对于一个房间大小的地图，由于在地图当中面元的数量影响计算的复杂程度。 Strasdat等人[8]提出ORB-SLAM2这种方法，这个方法使用深度信息去合成一个三维坐标，能够精确的提取到一副图像的信息。ORB-SLAM2能够处理来自双目和RGB-D的图像，与上述方法不同的是，我们的后端是用的BA算法，来建立一个全局的稀疏的地图重建，因此我们的方法更加轻量级并且能够在标准的CPU上面运行。我们的目标是长时间并且全局精准定位，而不是建立一个有很多细节的稠密地图。然而，高精度的关键帧的位姿，能够融合深度图像以及在计算中得到精准的重建，或者能够处理所有的关键帧和深度图，以及所有的BA并且得到一个精准的3D模型。 III. ORBSLAM2针对双目相机和RGB-D相机的ORB-SLAM2建立在单目ORB-SLAM的基础上，它的核心组件，如图2所示。 图2 ORB-SLAM2由三个并行的线程组成，跟踪，局部建图和回环检测。在一次回环检测后，会执行第四个线程，去执行BA优化。跟踪的线程在双目或者RGB-D输入之前进行，因此剩下的系统模块能够跟传感器模块独立运行。单目的ORB-SLAM2工作图也是这幅图。 这个系统主要有3个并行的线程： 1、通过寻找局部地图的特征匹配，以及只运用BA算法来最小化重投影误差，进行跟踪和定位每帧的相机。 2、运用局部的BA算法完成局部建图并且优化。 3、回环检测检能够通过执行位姿图的优化来更正累计漂移误差。在位姿图优化之后，会启动第四个线程来执行全局BA算法，来计算整个系统最优结构和运动的结果。 这个系统有一个基于DBoW2[16]的嵌入式位置识别模型，来达到重定位，防止跟踪失败（如遮挡），和已知地图的场景重初始化，以及回环检测的目的。这个系统维持着连接任意两个关键帧的共同点的共视图[8]，以及连接所有关键帧的最小生长树。这些关键帧的图结构能够得到一个关键帧的局部窗口，以便于跟踪和局部建图，并且允许在大型的环境中工作，且在回环检测中作为一种图优化的结构。 这个系统使用相同的ORB特征进行跟踪、建图和位置识别的任务。这些特征在旋转不变和尺度不变性上有良好的鲁棒性，同时对相机的自动增益、曝光和光线的变化表现出良好的稳定性。并且能够迅速的提取特征和进行匹配，能够满足实时操作的需求，能够在基于词袋模型的位置识别过程中，显示出良好的精度[18]。 在本章的剩下的部分当中，我将会展示双目以及深度信息是如何利用，和系统中的哪些部分会受其影响。对每个系统块更详尽的描述，可参见论文[1] 3.1 单目、近处双目和远处双目特征点ORB-SLAM2作为一种基于特征提取的方法，预处理输入以提取显著关键点位置处的特征，如图2b所示。系统的所有运行都是基于输入图像的特征展开，而不依赖于双目或者RGB-D的相机。我们的系统处理单目或者双目的特征点，分成远处特征点和近处特征点两类。 双目特征点 通过三个坐标$x_s=(u_L,v_L,u_R)$定义。当中，$(u_L,v_L)$是左边图像的坐标，$u_R$是右边图像的水平坐标。对于双目相机而言，我们提取两幅图像当中的ORB特征，对于每个左边的ORB特征我们对其匹配到右边的图像中。这对于建设双目图像校正十分有效，因此极线是水平的。之后我们会在左边的图像产生双目的ORB特征点，和一条水平的线向右边的图像进行匹配，通过修补相关性来重新定义亚像素。对于RGB-D相机，正如Strasdat等人[8]所言，我们提取在图像通道$(u_L,v_L)$上提取ORB特征点，我们将其深度值$d$转换到一个虚拟的右坐标中，$$u_R=u_L-\frac{f_xb}{d}$$其中，$f_x$是水平焦距，$b$是结构光投影器和红外相机的基线，kinect和华硕 Xtion 我们近似为8cm。我们将深度值和已经处理的深度地图，和基线在结构光投影器和红外相机进行匹配，对每一帧的图像与右边图像的坐标系进行融合。这是kinect和华硕 Xtion 精度大约是8cm。深度传感器的不确定性由虚拟右坐标的不确定性表示。 通过这种方式，双目和RGB-D输入的信息由系统的其余部分同等处理。 近双目特征点的定义是：匹配的深度值小于40倍双目/RGB-D的基线，否则的话，是远特征点。当深度信息估计精准时，近的特征点能够从一帧的深度值实现三角测量化，并且能够提供尺度、平移和旋转的信息。另外一方面，远的特征点，能够提供精确的旋转信息，但是更差的尺度和平移信息。当提供多视图的时候，我们才能三角化那些远的点。 单目特征点由左图像上的两个坐标$x_m=(u_L,v_L)$定义，相当于双目中所有ORB无法找到匹配的情况或者是在RGB-D中具有无效深度值的情况。这些点仅能够从多视图中三角测量化并且不能够提供尺度信息，但是可以提供旋转和平移的估计信息。 3.2 系统引导使用双目和RGB-D相机的主要优势在于，我们可以直接从一帧图像中获得深度信息，我们不需要像单目情况中那样做一个特定的运动结构（SFM）初始化。在系统初始化的时候，我们就创造了一个关键帧（也就是第一帧），将他的位姿进行初始化，从所有的立体点中创造一个初始化地图。 3.3 使用单目或者双目光束平差法我们的系统采用光束优化法（BA），优化在跟踪过程（纯运动BA）中相机的位姿，优化本地窗口的关键帧和局部地图的特征点（局部BA），并且在回环检测之后优化所有的关键帧和特征点（全局BA）。我们在g2o当中使用Levenberg-Marquadt方法[19]。 纯运动BA，优化相机旋转矩阵$R\in SO(3)$和位置$t\in R^3$，最小化世界坐标系下匹配的3D点云$X^i\in R^3$和特征点$x^i_{(·)}$（单目$x^i_m\in R^2$的或双目的$x^i_s\in R^3$），其中$i\in \chi$的重投影误差： 在这个式子当中，$\rho$是鲁棒的Huber cost函数，$\Sigma$是协方差矩阵，与特征点的尺度相关。这个投影函数$\pi _{(·)}$，单目的时候使用$\pi _{(m)}$，修正双目的时候用$\pi _{(s)}$,他们的定义如下： 在这个式子当中$(f_x,f_y)$是焦距，$(c_x,c_y)$是主要点（象点），b是基线，所有的这些参数都是通过标定获得。 局部BA 优化一系列可用的关键帧$\kappa_L$和所有在这些关键帧中的可观点$P_L$，所有的其他关键帧$\kappa_F$，即不在$\kappa_L$，考虑$P_L$当中的特征点有助于代价函数，但是在优化中是固定的。定义$\chi_k$为$P_L$和关键帧$k$的一系列匹配特征点，这个优化问题如下： 全局BA是 局部光束法的一个特例，这个方法除了初始帧所有的关键帧和点在地图当中都会被优化.初始帧是固定的，用来消除随机化。 3.4 回环检测和全局BA回环检测有两步：首先，一个回环信息被确定检测到，然后利用这个回环纠正和优化位姿图。相比于单目的ORB-SLAM中可能出现尺度漂移的地方[20]，这个双目或者深度的信息将会使得尺度信息可观测。并且，几何校验和位姿图优化将不再需要处理尺度漂移，而且是基于刚体变换的，而不是基于相似性。 在ORB-SLAM2的位姿优化后，我们包含一个全局的BA优化。这个优化方案可能计算量非常巨大，我们必须采用一个独立的线程，允许系统能够持续的建图，并且检测回环信息。但是这将会再次触发全局BA优化与当前地图的合成。如果在优化运行时检测到新的循环，我们将中止优化并继续完成回环，这将再次启动完整的BA优化。当完整的BA结束时，我们需要将更新的关键帧子集和由完整BA优化的点与未更新的关键帧和在优化运行时插入的点合并。最后通过生成树将校正后的更新关键帧（例如，这个变换从未优化到已优化）传播到一个未更新关键帧中。根据校正参考帧来转换这些未更新的特征点。 3.5 关键帧的插入ORB-SLAM2遵循在单目ORB-SLAM中提的法则，即经常插入关键帧并且剔除上一帧的冗余。远近特征点的差异为我们插入一个新的关键帧提供了条件，这在大场景的条件下是至关重要的，如图3所示。 图3 KITTI01中的跟踪点。高速公路的跟踪点。绿色的特征点深度小于40倍双目的基线，蓝色特征点大于40倍双目的基线，在这种数据集当中，需要插入大量的关键帧，以便于有足够的近处特征点来完成精确的位移估计。远处的特征点来估计方向，但是不能够计算平移和尺度。 在这样的环境中，我们需要大量的近点用以精确估计平移，因而如果跟踪近点的数量小于$\tau_t$并且这一帧可以创造$\tau_c$个新邻近立体点，那么这个系统将会插入一个新的关键帧。经验值认为，当$\tau_t=100$和$\tau_c=70$的条件下效果最好。 3.6 定位模式ORB-SLAM2包括一个定位模式，该模式适用于轻量级以及在地图已知情况下长期运行，只要那个环境没有发生剧烈变化。在该模式中，局部建图和回环检测的线程中是停用的，并且这个相机始终都是在通过追踪进行重定位。在这个模式下，追踪模块使用视觉里程计进行匹配图像的点云。视觉里程计对当前帧的ORB算子和由双目或者深度相机收集的3D点云进行匹配。这些匹配使得在没有地图的区域也能够精确重新定位，但是漂移将会被累加。地图点云匹配确保了在一个已经存在的地图当中零漂移定位。这个模型在附带的video当中会显示。 IV. 评估我们已经在三个流行的数据集中评估了ORB-SLAM2，并与其他最先进的SLAM系统进行了比较，使用原始作者发布的结果和文献中的标准评估指标。我们在一台16G的RAM，Intel Core i7-4790的台式机运行。为了消除多线程系统的不确定性，我们运行每个序列五次，并显示估计轨迹精度的中值结果。我们的开源实现包括在所有这些数据集中运行系统的标定和指令。 4.1 KITTI数据集KITTI数据集包含从在城市和高速公路环境中运行的汽车那里采集的双目数据。双目传感器具有小于54厘米的基线，并且在分辨率为1240×376像素的校正之后以10Hz频率工作。其中序列00,02,05,06,07和09包含回环。我们的ORB-SLAM2能够检测出几乎所有回环并且能够在此之后实现地图重用，除了09序列以外，09序列的回环只发生在尾端少数的几帧当中。表1显示了11个训练数据的结果，这是一个公开的真实数据，对比于原先的LSD-SLAM算法，我们展示了的双目SLAM系统测试数据结果。我们使用两个不同的度量方式，绝对平移均方根误差$t_{abs}$（在论文[3]中提到），与平均相对平移$t_{rel}$和旋转误差$r_{rel}$（在论文[2]中提到）。我们的系统在大多数序列当中比双目的LSD-SLAM要优秀很多，并且能够获得低于1%的相对误差。这个序列01，如图3所示，是训练集中唯一个高速公路的序列，其平移误差表现稍差。平移在这个序列当中是难以评估的，因为由于很高的速度和较低的帧率导致只有很少几个近点能够被跟踪到。然而方向能够被精确的评估，获得的误差是每100米0.21度，因为很多远点能够被长期跟踪。如图4所示，显示了一些评估的例子。 表1两种SLAM在测试KITTI数据的精度对比 图4 在KITTI数据集01,05,07和08数据集，估计轨迹（黑色线）和以及实际运动（红色线） 与[1]中提出的单目结果相比，所提出的双目版本能够处理单目系统失败的序列01。 在这个高速公路序列中，参见图3，近点仅在几帧中可见。 双目版本有着仅从一个立体关键帧就可以创建点云的能力，而不是依靠单目的延迟初始化（包括在两个关键帧之间找到匹配），在这个序列中是至关重要的，不会丢失跟踪。 此外，双目系统用米尺估计地图和轨迹，并且不受尺度漂移的影响，如图5所示。 图5. KITTI 08中的估计轨迹（黑色）和地面实况（红色）。左：单目ORB-SLAM [1]，右：ORB-SLAM2（双目）。 单目ORBSLAM在此序列中有严重的尺度漂移，特别是在转弯时。 相比之下，所提出的双目版本能够在没有比例漂移的情况下估计轨迹和地图的真实比例。 4.2 EuRoC 数据集EuRoC 数据集包含了11个双目的序列，数据通过一个微型飞行器（MAV）在两个不同的房间和一个大型工业环境中采集。这个双目传感器有一个小于11cm的基线，并能够提供20HZ的高分辨率图像。这个序列分成，简单、中等和困难，这取决于MAV（微型飞行器）的速度，照明和场景的纹理。在所有的序列当中，MAV（微型飞行器）再次访问这个环境的时候，ORB-SLAM2能够重用地图，在必要时可以进行回环检测。这个表格2显示的是ORB-SLAM2在所有序列中的绝对平移的最小均方误差。并与文献11中采用双目LSD-SLAM得到的结果相比较。 ORB-SLAM2能够实现一个厘米级精准的定位，并且比双目的LSD-SLAM更加的精确。由于严重的运动模糊，跟踪部分可能会在V2_ 03_ difficul 序列当中部分丢失。在文献22中，这个序列可以通过IMU信息进行处理。图6显示的是一些估计轨迹与真实的情况相对比的例子。 图6在EuRoC V1_02_medium，V2_02_medium，MH_03_medium和MH_05_数据集测试结果，其中估计轨迹（黑色）实际运动轨迹（红色） 4.3 TUM RGB-D数据集TUM RGB-D数据集[3]包含来自RGB-D传感器的室内序列，这些传感器分组在几个类别中，以评估不同纹理，照明和结构条件下的物体重建和SLAM /视觉里程计的性能。和大多数RGB-DSLAM方法一样，我们将实验结果展示在一个序列子集当中。在表格3当中，我们将我们的精度与以下最先进的方法进行比较，例如ElasticFusion，Kintinuous，DVO-SLAM以及RGB-DSLAM。ORB-SLAM2是唯一一种基于光束流差法，并且在大多数序列中比其他的方法都更加优秀。我们已经注意到RGB-DSLAM在文献1中的结果，深度地图对于freiburg2序列有一个4%的尺度误差，误差可能来自错误的标定，我们已经在运行过程中，进行了一定程度的补偿，这能够部分解释我们取得好的结果的原因。图7显示出了四个序列中通过从计算出的关键帧姿势反向投影传感器深度图而得到的点云。书桌和海报的良好清晰度和直线轮廓证明了我们方法的高精度定位。 图6 TUM RGB-D数据的fr3office, fr1 room, fr2 desk and fr3 nst 序列的通过评估关键帧的位姿和深度图进行稠密的点云重建图 4.4 计时结果为了完成对所提出的系统的评估，我们在表IV中呈现了具有不同图像分辨率和传感器的三个序列的计时结果。 显示每个线程任务的平均值和两个标准偏差范围。 由于这些序列包含一个单回环，因此完整的BA和回环检测线程的一些任务仅执行一次，并且仅报告单个时间测量。 每帧的平均跟踪时间低于每个序列的相机帧速率的倒数，这意味着我们的系统能够实时工作。 由于立体图像中的ORB提取是并行化的，可以看出在V2_02的双目高分辨率图像中提取1000个ORB特征类似于在fr3_office的单个VGA图像通道中提取相同数量的特征。 回环中关键帧的数量显示为与回环检测相关的时间的参考。 虽然KITTI 07中的回环包含更多关键帧，但作为室内fr3_office构建的共视图更密集，因此，回环融合、姿势图优化和完整BA任务更加耗时。共视图的较高密度使得局部地图包含更多关键帧和点，因此，局部地图跟踪和局部BA也更耗时。 V. 结论我们为单目，双目和RGB-D传感器提供了一个完整的SLAM系统，能够在标准CPU上实时执行重定位、回环检测、重用其地图等功能。 我们专注于构建全局一致的地图，以便在各种环境中提长时间且可靠的定位，如实验中所示。 所提出的具有系统重定位能力的定位模式是对于已知环境而言非常稳健、零漂移和轻量级定位方法。 此模式对于某些应用程序非常有用，例如在良好映射的空间中跟踪虚拟现实中的用户视角。 与现有技术的比较表明，ORB-SLAM2在大多数情况下实现了最高的精度。 在KITTI视觉里程计基准测试中，ORB-SLAM2是目前最好的双目SLAM解决方案。 至关重要的是，与近年来蓬勃发展的立体视觉测距方法相比，ORB-SLAM2在已经建图区域实现了零漂移定位。 令人惊讶的是，我们的RGB-D结果表明，如果需要最精确的相机定位，则BA法比直接方法或ICP更好，另外的优点是计算成本较低，不需要GPU处理来实时操作。 我们已经发布了我们系统的源代码，其中包含示例和说明，以便其他研究人员可以轻松使用。ORB-SLAM2是我们所知的第一个开源视觉SLAM系统，可以使用单目、双目和RGB-D输入。此外，我们的源代码包含使用单目相机的增强现实应用2的示例，以显示我们的解决方案的潜力。 未来扩展可能包括，举一些例子，非重叠多视角摄像机，鱼眼或全向摄像机，大规模密集融合，以及联合建图或者增加运动模糊的鲁棒性。]]></content>
      <categories>
        <category>slam</category>
      </categories>
      <tags>
        <tag>slam理论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++学习（3）--类与对象]]></title>
    <url>%2F2019%2F09%2F05%2FC%2B%2B%E5%AD%A6%E4%B9%A0%EF%BC%883%EF%BC%89%2F</url>
    <content type="text"><![CDATA[1 面向对象程序设计的基本特点 抽象：解释类与对象之间关系的词。类与对象之间的关系就是抽象的关系。一句话来说明：类是对象的抽象，而对象则是类得特例，即类的具体表现形式。 封装：封装是面向对象编程的核心思想。将对象的属性和行为封装起来,其载体就是类,类通常会对客户隐藏其实现细节,这就是封装的思想。 继承：子类继承父类，可以继承父类原有的属性和方法，也可以增加其他的属性和方法，可以直接重写父类中的某些方法。 多态：多态性一般是指在父类中定义的方法被子类继承后，可以表现出不同的行为。这使得同一个方法在父类及其各个子类中具有不同的语义。 2 类和对象2.1 类的定义语法形式如下： 123456789class 类名称&#123;public: 外部接口private: 私有成员protected: 保护型成员&#125; 2.2类成员的访问控制public:公有类型，类的外部接口。 private:私有类型，只能被本类的成员函数访问，来自类外部的任何访问都是非法的。 protected:保护类型，和私有类型类似，差别在于继承过程中对产生的新类的影响不同。 2.3 对象类的对象是该类的某一特定实体，即类类型的变量。 声明形式： 12类名 对象；Clock myClock； 类中成员互访：直接使用成员名 类外访问：使用“对象名.成员名” 方式访问public属性成员 2.4 类的成员函数实现形式 1234返回值类型 类名：：函数成员名（参数表）&#123; 函数体&#125; 例子 12345678910111213141516171819202122232425262728293031323334353637//设置时钟#include &lt;iostream&gt;using namespace std;class Clock&#123; //时钟类的定义public: //外部接口 void setTime(int newH=0, int newM=0, int newS=0); void showTime();private: //私有数据成员 int hour,minute,second;&#125;;//时钟类函数的实现void Clock::setTime(int newH, int newM, int newS) &#123; hour = newH; minute = newM; second = newS;&#125;inline void Clock::showTime() &#123; cout&lt;&lt;hour&lt;&lt;":"&lt;&lt;minute&lt;&lt;":"&lt;&lt;second&lt;&lt;endl;&#125;//主函数int main()&#123; Clock myClock; //定义对象myClock cout&lt;&lt;"First time set and output:"&lt;&lt;endl; myClock.setTime(); myClock.showTime(); cout&lt;&lt;"Second time set and output:"&lt;&lt;endl; myClock.setTime(22,10,30); myClock.showTime(); return (0);&#125; 3 构造函数和析构函数3.1 构造函数构造函数的作用是在对象被创建时利用特定的值构造对象，将对象初始化为一个特定的状态。 123456789101112131415161718192021222324252627282930313233343536373839404142434445//设置时钟#include &lt;iostream&gt;using namespace std;class Clock&#123; //时钟类的定义public: //外部接口 Clock (int newH, int newM, int newS); //构造函数 void setTime(int newH=0, int newM=0, int newS=0); void showTime();private: //私有数据成员 int hour,minute,second;&#125;;//构造函数的实现Clock::Clock(int newH, int newM, int newS) &#123; hour = newH; minute = newM; second = newS;&#125;//时钟类函数的实现void Clock::setTime(int newH, int newM, int newS) &#123; hour = newH; minute = newM; second = newS;&#125;inline void Clock::showTime() &#123; cout&lt;&lt;hour&lt;&lt;":"&lt;&lt;minute&lt;&lt;":"&lt;&lt;second&lt;&lt;endl;&#125;//主函数int main()&#123; Clock myClock(1,2,3); //定义对象myClock cout&lt;&lt;"First time set and output:"&lt;&lt;endl; myClock.showTime(); cout&lt;&lt;"Second time set and output:"&lt;&lt;endl; myClock.setTime(22,10,30); myClock.showTime(); return (0);&#125; 3.2 复制构造函数复制构造函数是一种特殊的构造函数，其形参是本类的对象的引用。作用是使用一个已经存在的对象（由复制构造函数的参数指定），去初始化同类的一个新的对象。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748//设置时钟#include &lt;iostream&gt;using namespace std;class Point&#123; //Point 类的定义public: // 外部接口 Point(int xx=0, int yy=0)&#123; // 构造函数 x=xx; y=yy; &#125; Point(Point &amp;p); //复制构造函数 int getX()&#123; return x; &#125; int getY()&#123; return y; &#125;private: // 私有数据 int x, y;&#125;;// 成员函数的实现Point::Point(Point &amp;p) &#123; x=p.x; y=p.y; cout&lt;&lt;"呼叫复制构造函数"&lt;&lt;endl;&#125;// 形参为Point类对象的函数void fun1(Point p)&#123; cout&lt;&lt;p.getX()&lt;&lt;endl;&#125;//返回值为Point类对象的函数Point fun2()&#123; Point a(1,2); return a;&#125;// 主函数int main()&#123; Point a(4,5); //第一个对象a Point b=a; //用法1，用a初始化b cout&lt;&lt;b.getX()&lt;&lt;endl; fun1(b); // 用法2，对象b作为fun1的实参 b=fun2(); // 用法3，函数的返回值是类对象，函数返回时，调用复制构造函数 cout&lt;&lt;b.getX()&lt;&lt;endl; return (0);&#125; 3.3 析构函数作用与构造函数正好相反，用来完成对象被删除前的一些清理工作。在对象生存期即将结束的时刻被自动调用，不接受任何参数，也没有返回值。 4 类的组合类的组合描述的是一个类内嵌套其他类的对象作为成员的情况，是一种包含与被包含的关系。 当创建类的对象时，如果这个类具有内嵌对象成员，那么各个内嵌对象将首先被自动创建。在创建对象时，既要对本类的基本类型数据成员进行初始化，又要对内嵌对象成员进行初始化。 例子 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768//Line 类的例子#include &lt;iostream&gt;#include &lt;cmath&gt; //数学函数using namespace std;class Point&#123; //Point 类定义public: Point(int xx=0, int yy=0)&#123; // 构造函数 x=xx; y=yy; &#125; Point(Point &amp;p); //复制构造函数 int getX()&#123; return x; &#125; int getY()&#123; return y; &#125;private: // 私有数据 int x, y;&#125;;// 成员函数的实现Point::Point(Point &amp;p) &#123; x=p.x; y=p.y; cout&lt;&lt;"呼叫复制构造函数"&lt;&lt;endl;&#125;// 类的组合class Line&#123; // Line类的定义public: Line(Point xp1, Point xp2); Line(Line &amp;l); double getLen() &#123; return len;&#125;private: Point p1, p2; double len;&#125;;// 组合类的构造函数Line::Line(Point xp1, Point xp2) :p1(xp1),p2(xp2)&#123; cout&lt;&lt;"呼叫Line的构造函数:"&lt;&lt;endl; double x= static_cast&lt;double&gt;(p1.getX()-p2.getX()); double y= static_cast&lt;double&gt;(p1.getY()-p2.getY()); len = sqrt(x*x+y*y);&#125;// 组合类的复制构造函数Line::Line(Line &amp;l):p1(l.p1),p2(l.p2) &#123; cout&lt;&lt;"呼叫Line的复制构造函数:"&lt;&lt;endl; len=l.len;&#125;// 主函数int main()&#123; Point myp1(1,1), myp2(4,5); Line line(myp1,myp2); Line line2(line); cout&lt;&lt;"Line的长度为:"&lt;&lt;endl; cout&lt;&lt;line.getLen()&lt;&lt;endl; cout&lt;&lt;"Line2的长度为:"&lt;&lt;endl; cout&lt;&lt;line2.getLen()&lt;&lt;endl; return (0);&#125;]]></content>
      <categories>
        <category>语言学习</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++学习（2）--函数]]></title>
    <url>%2F2019%2F09%2F04%2FC%2B%2B%E5%AD%A6%E4%B9%A0%EF%BC%882%EF%BC%89%2F</url>
    <content type="text"><![CDATA[1 函数的定义与使用1.1 函数定义的语法形式1234567类型说明符 函数名（含类型说明的形参表）&#123;​ 语句序列&#125; 1.2 函数的调用调用前先声明： 1类型说明符 函数名（含类型说明的形参表）; 再调用： 1函数名（实参表） 一般调用 12345678910111213141516//求x的n次方#include &lt;iostream&gt;using namespace std;double power(double x, int n);int main()&#123; cout &lt;&lt; "5 to the power 2 is "&lt;&lt; power(5,2) &lt;&lt;endl; return 0;&#125;double power(double x, int n)&#123; double val=1.0; while (n--) val *= x; return val;&#125; 嵌套调用 12345678910111213141516171819//求两个数平方和#include &lt;iostream&gt;using namespace std;int fun2(int m)&#123; return m*m;&#125;int fun1(int x, int y)&#123; return fun2(x)+fun2(y);&#125;int main()&#123; int a, b; cout&lt;&lt;"请输入两个数："; cin&gt;&gt;a&gt;&gt;b; cout&lt;&lt;"它们的平方和为："&lt;&lt;fun1(a,b)&lt;&lt;endl; return 0;&#125; 递归调用 1234567891011121314151617181920//求n的阶乘#include &lt;iostream&gt;using namespace std;unsigned fac(unsigned n)&#123; unsigned f; if (n==0) f=1; else f=fac(n-1)*n; return f;&#125;int main()&#123; unsigned n; cout&lt;&lt;"输入一个正整数："; cin&gt;&gt;n; unsigned y=fac(n); cout&lt;&lt;n&lt;&lt;"!="&lt;&lt;y&lt;&lt;endl; return 0;&#125; 1.3 函数的参数传递值传递（单项传递） 12345678910111213141516//交换两个数#include &lt;iostream&gt;using namespace std;void swap (int a, int b)&#123; int t = a; a=b; b=t;&#125;int main()&#123; int x=5, y=10; cout &lt;&lt; "x="&lt;&lt;x&lt;&lt;" y="&lt;&lt;y&lt;&lt;endl; swap(x,y); cout &lt;&lt; "x="&lt;&lt;x&lt;&lt;" y="&lt;&lt;y&lt;&lt;endl; return 0;&#125; 输出： 12x=5 y=10x=5 y=10 引用传递（双向） 12345678910111213141516//交换两个数#include &lt;iostream&gt;using namespace std;void swap (int &amp;a, int &amp;b)&#123; int t = a; a=b; b=t;&#125;int main()&#123; int x=5, y=10; cout &lt;&lt; "x="&lt;&lt;x&lt;&lt;" y="&lt;&lt;y&lt;&lt;endl; swap(x,y); cout &lt;&lt; "x="&lt;&lt;x&lt;&lt;" y="&lt;&lt;y&lt;&lt;endl; return 0;&#125; 输出： 12x=5 y=10x=10 y=5 2 C++系统函数 C++系统提供很多自带的函数，如数学函数，平方根（sqrt)，求绝对值（abs）等等。 只需要用include指令嵌入相应的头文件，就可以使用系统函数。如数学函数，只需在开头写： 1#include &lt;cmath&gt;]]></content>
      <categories>
        <category>语言学习</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++学习（1）--基础]]></title>
    <url>%2F2019%2F08%2F29%2FC%2B%2B%E5%AD%A6%E4%B9%A0%EF%BC%881%EF%BC%89%2F</url>
    <content type="text"><![CDATA[1 命名空间避免命名冲突 std是C++标准库的命名空间（ namespace）名 using namespace std表示打开std命名空间 12345678910111213#include &lt;iostream&gt;using namespace std;int main() &#123; cout &lt;&lt; "Hello!" &lt;&lt; endl; cout &lt;&lt; "Welcome to c++!" &lt;&lt; endl; return 0;&#125; 运行结果： Hello! Welcome to c++！ 2 基本运算12345678910111213141516#include &lt;iostream&gt;using namespace std;int main() &#123; const int PRICE = 30; // 整数常量 int num, total; // 初始化 double v, r, h; num = 10; // 赋值 total = num * PRICE; cout &lt;&lt; total &lt;&lt; endl; r = 2.5; h = 3.2; v = 3.14159 * r * r * h; cout &lt;&lt; v &lt;&lt; endl; return 0;&#125; 运行结果： 30062.8318 注意：1/2为0，1/2.0为0.5 。 3 数据输入输出 “&lt;&lt;”是预定义的插入符，作用在流类对象cout上便可以实现项标准输出设备输出。 1cout &lt;&lt; 表达式 &lt;&lt; 表达式... 标准输入是将提取符作用在流类对象cin上。 1cin &gt;&gt; 表达式 &gt;&gt; 表达式... 提取符可以连续写多个，每个后面跟一个表达式，该表达式通常是用于存放输入值的变量。例如： 123int a, b;cin &gt;&gt; a &gt;&gt; b; 常用的I/O流类库操纵符 例： 1234567#include &lt;iostream&gt;#include &lt;iomanip&gt;using namespace std;int main() &#123; cout &lt;&lt; setw(5) &lt;&lt; setprecision(3) &lt;&lt; 3.1415; return 0;&#125; 输出 3.14 4 算法基本控制结构4.1 if 语句12345678910111213141516171819//判断是不是闰年#include &lt;iostream&gt;using namespace std;int main()&#123; int year; bool isLeapYear; cout &lt;&lt; "Enter the year: "; cin &gt;&gt; year; isLeapYear = ((year % 4 == 0 &amp;&amp; year % 100 != 0) || (year % 400 == 0)); if (isLeapYear) cout &lt;&lt; year &lt;&lt; " is a leap year" &lt;&lt; endl; else cout &lt;&lt; year &lt;&lt; " is not a leap year" &lt;&lt; endl; return 0;&#125; 注意：如果if else 语句里面还有if else语句，最好有{}来确定层次关系，否则在省略else情况下会出错。 4.2 switch 语句1234567891011121314151617181920212223242526//判断星期几#include &lt;iostream&gt;using namespace std;int main()&#123; int day; cin &gt;&gt; day; switch (day)&#123; case 0: cout &lt;&lt; "星期天" &lt;&lt; endl; break; case 1: cout &lt;&lt; "星期一" &lt;&lt; endl; break; case 2: cout &lt;&lt; "星期二" &lt;&lt; endl; break; case 3: cout &lt;&lt; "星期三" &lt;&lt; endl; break; case 4: cout &lt;&lt; "星期四" &lt;&lt; endl; break; case 5: cout &lt;&lt; "星期五" &lt;&lt; endl; break; case 6: cout &lt;&lt; "星期六" &lt;&lt; endl; break; &#125; return 0;&#125; 4.3 while 语句12345678910111213//计算1加到10#include &lt;iostream&gt;using namespace std;int main()&#123; int i = 1, sum = 0; while (i &lt;= 10)&#123; sum += i; i++; &#125; cout &lt;&lt; "sum = " &lt;&lt; sum &lt;&lt; endl; return 0;&#125; 4.4 do while 语句12345678910111213//计算1加到10#include &lt;iostream&gt;using namespace std;int main()&#123; int i = 1, sum = 0; do&#123; sum += i; i++; &#125;while (i &lt;= 10); cout &lt;&lt; "sum = " &lt;&lt; sum &lt;&lt; endl; return 0;&#125; 4.5 for 语句123456789101112131415//求一个整数所有因子#include &lt;iostream&gt;using namespace std;int main()&#123; int n; cout &lt;&lt; "Enter a positive integer: "; cin &gt;&gt; n; cout &lt;&lt; "Number " &lt;&lt; n &lt;&lt; " Factors "; for (int k=1; k&lt;=n; k++) if (n%k==0) cout &lt;&lt; k &lt;&lt; " "; cout &lt;&lt; endl; return 0;&#125;]]></content>
      <categories>
        <category>语言学习</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ORB_SLAM2地图保存与加载（2）]]></title>
    <url>%2F2019%2F08%2F27%2FORB_SLAM2%E5%9C%B0%E5%9B%BE%E4%BF%9D%E5%AD%98%E4%B8%8E%E5%8A%A0%E8%BD%BD%EF%BC%882%EF%BC%89%2F</url>
    <content type="text"><![CDATA[本文记录了ORB_SLAM2中地图加载的过程。参考博客：https://blog.csdn.net/qq_34254510/article/details/79969046http://www.cnblogs.com/mafuqiang/p/6972841.htmlhttps://blog.csdn.net/felaim/article/details/79667635https://blog.csdn.net/u014709760/article/details/86319090 2 地图加载2.1 源码修改地图加载部分需要修改的较多，所以按所需修改的文件来进行说明。 （1）Map相关文件修改在Map.h文件中声明地图加载函数、地图点加载函数和关键帧加载函数： 1234//加载地图信息 void Load(const string &amp;filename,SystemSetting* mySystemSetting); MapPoint* LoadMapPoint(ifstream &amp;f); KeyFrame* LoadKeyFrame(ifstream &amp;f,SystemSetting* mySystemSetting); Map.h中需要加入SystemSetting.h： 1234#include "SystemSetting.h"### 还要添加class SystemSetting; 在Map.cc文件中进行相应实现： 地图加载函数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293//地图加载函数void Map::Load ( const string &amp;filename, SystemSetting* mySystemSetting)&#123; cerr &lt;&lt; "Map.cc :: Map reading from:"&lt;&lt;filename&lt;&lt;endl; ifstream f; f.open( filename.c_str() ); // Same as the sequence that we save the file, we first read the number of MapPoints. unsigned long int nMapPoints; f.read((char*)&amp;nMapPoints, sizeof(nMapPoints)); // Then read MapPoints one after another, and add them into the map cerr&lt;&lt;"Map.cc :: The number of MapPoints:"&lt;&lt;nMapPoints&lt;&lt;endl; for ( unsigned int i = 0; i &lt; nMapPoints; i ++ ) &#123; MapPoint* mp = LoadMapPoint(f); AddMapPoint(mp); &#125; // Get all MapPoints std::vector&lt;MapPoint*&gt; vmp = GetAllMapPoints(); // Read the number of KeyFrames unsigned long int nKeyFrames; f.read((char*)&amp;nKeyFrames, sizeof(nKeyFrames)); cerr&lt;&lt;"Map.cc :: The number of KeyFrames:"&lt;&lt;nKeyFrames&lt;&lt;endl; // Then read KeyFrames one after another, and add them into the map vector&lt;KeyFrame*&gt;kf_by_order; for( unsigned int i = 0; i &lt; nKeyFrames; i ++ ) &#123; KeyFrame* kf = LoadKeyFrame(f, mySystemSetting); AddKeyFrame(kf); kf_by_order.push_back(kf); &#125; cerr&lt;&lt;"Map.cc :: Max KeyFrame ID is: " &lt;&lt; mnMaxKFid &lt;&lt; ", and I set mnId to this number" &lt;&lt;endl; cerr&lt;&lt;"Map.cc :: KeyFrame Load OVER!"&lt;&lt;endl; // Read Spanning Tree(open loop trajectory) map&lt;unsigned long int, KeyFrame*&gt; kf_by_id; for ( auto kf: mspKeyFrames ) kf_by_id[kf-&gt;mnId] = kf; cerr&lt;&lt;"Map.cc :: Start Load The Parent!"&lt;&lt;endl; for( auto kf: kf_by_order ) &#123; // Read parent_id of current KeyFrame. unsigned long int parent_id; f.read((char*)&amp;parent_id, sizeof(parent_id)); // Add parent KeyFrame to current KeyFrame. // cout&lt;&lt;"Map::Load : Add parent KeyFrame to current KeyFrame"&lt;&lt;endl; if ( parent_id != ULONG_MAX ) kf-&gt;ChangeParent(kf_by_id[parent_id]); // Read covisibility graphs. // Read the number of Connected KeyFrames of current KeyFrame. unsigned long int nb_con; f.read((char*)&amp;nb_con, sizeof(nb_con)); // Read id and weight of Connected KeyFrames of current KeyFrame, // and add Connected KeyFrames into covisibility graph. // cout&lt;&lt;"Map::Load : Read id and weight of Connected KeyFrames"&lt;&lt;endl; for ( unsigned long int i = 0; i &lt; nb_con; i ++ ) &#123; unsigned long int id; int weight; f.read((char*)&amp;id, sizeof(id)); f.read((char*)&amp;weight, sizeof(weight)); kf-&gt;AddConnection(kf_by_id[id],weight); &#125; &#125; cerr&lt;&lt;"Map.cc :: Parent Load OVER!"&lt;&lt;endl; for ( auto mp: vmp ) &#123; // cout &lt;&lt; "Now mp = "&lt;&lt; mp &lt;&lt; endl; if(mp) &#123; // cout &lt;&lt; "compute for mp = "&lt;&lt; mp &lt;&lt; endl; mp-&gt;ComputeDistinctiveDescriptors(); // cout &lt;&lt; "Computed Distinctive Descriptors." &lt;&lt; endl; mp-&gt;UpdateNormalAndDepth(); // cout &lt;&lt; "Updated Normal And Depth." &lt;&lt; endl; &#125; &#125; f.close(); cerr&lt;&lt;"Map.cc :: Load IS OVER!"&lt;&lt;endl; return;&#125; 其过程就是根据保存的顺序依次加载地图点的数目、地图点、关键帧的数目、关键帧、生长树和关联关系。 地图点加载函数： 123456789101112131415161718MapPoint* Map::LoadMapPoint( ifstream &amp;f )&#123; // Position and Orientation of the MapPoints. cv::Mat Position(3,1,CV_32F); long unsigned int id; f.read((char*)&amp;id, sizeof(id)); f.read((char*)&amp;Position.at&lt;float&gt;(0), sizeof(float)); f.read((char*)&amp;Position.at&lt;float&gt;(1), sizeof(float)); f.read((char*)&amp;Position.at&lt;float&gt;(2), sizeof(float)); // Initialize a MapPoint, and set its id and Position. MapPoint* mp = new MapPoint(Position, this ); mp-&gt;mnId = id; mp-&gt;SetWorldPos( Position ); return mp;&#125; 关键帧加载函数： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485KeyFrame* Map::LoadKeyFrame( ifstream &amp;f, SystemSetting* mySystemSetting )&#123; InitKeyFrame initkf(*mySystemSetting); // Read ID and TimeStamp of each KeyFrame. f.read((char*)&amp;initkf.nId, sizeof(initkf.nId)); f.read((char*)&amp;initkf.TimeStamp, sizeof(double)); // Read position and quaternion cv::Mat T = cv::Mat::zeros(4,4,CV_32F); std::vector&lt;float&gt; Quat(4); //Quat.reserve(4); for ( int i = 0; i &lt; 4; i ++ ) f.read((char*)&amp;Quat[i],sizeof(float)); cv::Mat R = Converter::toCvMat(Quat); for ( int i = 0; i &lt; 3; i ++ ) f.read((char*)&amp;T.at&lt;float&gt;(i,3),sizeof(float)); for ( int i = 0; i &lt; 3; i ++ ) for ( int j = 0; j &lt; 3; j ++ ) T.at&lt;float&gt;(i,j) = R.at&lt;float&gt;(i,j); T.at&lt;float&gt;(3,3) = 1; // Read feature point number of current Key Frame f.read((char*)&amp;initkf.N, sizeof(initkf.N)); initkf.vKps.reserve(initkf.N); initkf.Descriptors.create(initkf.N, 32, CV_8UC1); vector&lt;float&gt;KeypointDepth; std::vector&lt;MapPoint*&gt; vpMapPoints; vpMapPoints = vector&lt;MapPoint*&gt;(initkf.N,static_cast&lt;MapPoint*&gt;(NULL)); // Read Keypoints and descriptors of current KeyFrame std::vector&lt;MapPoint*&gt; vmp = GetAllMapPoints(); for(int i = 0; i &lt; initkf.N; i ++ ) &#123; cv::KeyPoint kp; f.read((char*)&amp;kp.pt.x, sizeof(kp.pt.x)); f.read((char*)&amp;kp.pt.y, sizeof(kp.pt.y)); f.read((char*)&amp;kp.size, sizeof(kp.size)); f.read((char*)&amp;kp.angle,sizeof(kp.angle)); f.read((char*)&amp;kp.response, sizeof(kp.response)); f.read((char*)&amp;kp.octave, sizeof(kp.octave)); initkf.vKps.push_back(kp); // Read descriptors of keypoints f.read((char*)&amp;initkf.Descriptors.cols, sizeof(initkf.Descriptors.cols)); // for ( int j = 0; j &lt; 32; j ++ ) // Since initkf.Descriptors.cols is always 32, for loop may also write like this. for ( int j = 0; j &lt; initkf.Descriptors.cols; j ++ ) f.read((char*)&amp;initkf.Descriptors.at&lt;unsigned char&gt;(i,j),sizeof(char)); // Read the mapping from keypoints to MapPoints. unsigned long int mpidx; f.read((char*)&amp;mpidx, sizeof(mpidx)); // Look up from vmp, which contains all MapPoints, MapPoint of current KeyFrame, and then insert in vpMapPoints. if( mpidx == ULONG_MAX ) vpMapPoints[i] = NULL; else vpMapPoints[i] = vmp[mpidx]; &#125; initkf.vRight = vector&lt;float&gt;(initkf.N,-1); initkf.vDepth = vector&lt;float&gt;(initkf.N,-1); //initkf.vDepth = KeypointDepth; initkf.UndistortKeyPoints(); initkf.AssignFeaturesToGrid(); // Use initkf to initialize a KeyFrame and set parameters KeyFrame* kf = new KeyFrame( initkf, this, NULL, vpMapPoints ); kf-&gt;mnId = initkf.nId; kf-&gt;SetPose(T); kf-&gt;ComputeBoW(); for ( int i = 0; i &lt; initkf.N; i ++ ) &#123; if ( vpMapPoints[i] ) &#123; vpMapPoints[i]-&gt;AddObservation(kf,i); if( !vpMapPoints[i]-&gt;GetReferenceKeyFrame()) vpMapPoints[i]-&gt;SetReferenceKeyFrame(kf); &#125; &#125; return kf;&#125; （2）MapPoint相关文件修改由于在加载地图时我们只有Position以及当前的Map，所以需要重新定义一种MapPoint类的构造函数以满足要求。 在MapPoint.h文件中添加如下构造函数： 123MapPoint(const cv::Mat &amp;Pos,Map* pMap);KeyFrame* SetReferenceKeyFrame(KeyFrame* RFKF); 在MapPoint.cc文件中实现该构造函数： 123456789101112MapPoint::MapPoint(const cv::Mat &amp;Pos, Map* pMap): mnFirstKFid(0), mnFirstFrame(0), nObs(0), mnTrackReferenceForFrame(0), mnLastFrameSeen(0), mnBALocalForKF(0), mnFuseCandidateForKF(0), mnLoopPointForKF(0), mnCorrectedByKF(0), mnCorrectedReference(0), mnBAGlobalForKF(0), mpRefKF(static_cast&lt;KeyFrame*&gt;(NULL)), mnVisible(1), mnFound(1), mbBad(false), mpReplaced(static_cast&lt;MapPoint*&gt;(NULL)), mfMinDistance(0), mfMaxDistance(0), mpMap(pMap) &#123; Pos.copyTo(mWorldPos); mNormalVector = cv::Mat::zeros(3,1,CV_32F); // MapPoints can be created from Tracking and Local Mapping. This mutex avoid conflicts with id. unique_lock&lt;mutex&gt; lock(mpMap-&gt;mMutexPointCreation); mnId=nNextId++; &#125; 此外，还需要添加如下函数： 1234KeyFrame* MapPoint::SetReferenceKeyFrame(KeyFrame* RFKF)&#123; return mpRefKF = RFKF;&#125; （3）KeyFrame相关文件修改与MapPoint文件相同，KeyFrame文件也要做相关修改。 在KeyFrame.h文件中添加如下构造函数： 1KeyFrame(InitKeyFrame &amp;initkf,Map* pMap,KeyFrameDatabase* pKFDB,vector&lt;MapPoint*&gt;&amp; vpMapPoints); 不要忘记在KeyFrame.h中添加相应头文件和命名空间中的类声明 12345678#include "MapPoint.h"#include "Thirdparty/DBoW2/DBoW2/BowVector.h"#include "Thirdparty/DBoW2/DBoW2/FeatureVector.h"#include "ORBVocabulary.h"#include "ORBextractor.h"#include "Frame.h"#include "KeyFrameDatabase.h"#include "InitKeyFrame.h" 123456789class Map;class MapPoint;class Frame;class KeyFrameDatabase;class InitKeyFrame;class KeyFrame&#123;public: 在KeyFrame.cc文件中实现该构造函数： 123456789101112131415161718KeyFrame::KeyFrame(InitKeyFrame &amp;initkf, Map *pMap, KeyFrameDatabase *pKFDB, vector&lt;MapPoint*&gt; &amp;vpMapPoints): mnFrameId(0), mTimeStamp(initkf.TimeStamp), mnGridCols(FRAME_GRID_COLS), mnGridRows(FRAME_GRID_ROWS), mfGridElementWidthInv(initkf.fGridElementWidthInv), mfGridElementHeightInv(initkf.fGridElementHeightInv), mnTrackReferenceForFrame(0), mnFuseTargetForKF(0), mnBALocalForKF(0), mnBAFixedForKF(0), mnLoopQuery(0), mnLoopWords(0), mnRelocQuery(0), mnRelocWords(0), mnBAGlobalForKF(0), fx(initkf.fx), fy(initkf.fy), cx(initkf.cx), cy(initkf.cy), invfx(initkf.invfx), invfy(initkf.invfy), mbf(initkf.bf), mb(initkf.b), mThDepth(initkf.ThDepth), N(initkf.N), mvKeys(initkf.vKps), mvKeysUn(initkf.vKpsUn), mvuRight(initkf.vRight), mvDepth(initkf.vDepth), mDescriptors(initkf.Descriptors.clone()), mBowVec(initkf.BowVec), mFeatVec(initkf.FeatVec), mnScaleLevels(initkf.nScaleLevels), mfScaleFactor(initkf.fScaleFactor), mfLogScaleFactor(initkf.fLogScaleFactor), mvScaleFactors(initkf.vScaleFactors), mvLevelSigma2(initkf.vLevelSigma2),mvInvLevelSigma2(initkf.vInvLevelSigma2), mnMinX(initkf.nMinX), mnMinY(initkf.nMinY), mnMaxX(initkf.nMaxX), mnMaxY(initkf.nMaxY), mK(initkf.K), mvpMapPoints(vpMapPoints), mpKeyFrameDB(pKFDB), mpORBvocabulary(initkf.pVocabulary), mbFirstConnection(true), mpParent(NULL), mbNotErase(false), mbToBeErased(false), mbBad(false), mHalfBaseline(initkf.b/2), mpMap(pMap) &#123; mnId = nNextId ++; &#125; （4）SystemSetting和InitKeyFrame相关文件在上面的函数中我们用到了SystemSetting类和InitKeyFrame类。其中SystemSetting类用于读取参数文件中的相关参数，InitKeyFrame类用于进行关键帧初始化。其实现过程如下： 创建 SystemSetting.h 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#ifndef SYSTEMSETTING_H#define SYSTEMSETTING_H#include&lt;string&gt;#include"ORBVocabulary.h"#include&lt;opencv2/opencv.hpp&gt;namespace ORB_SLAM2 &#123; class SystemSetting&#123; public: SystemSetting(ORBVocabulary* pVoc); bool LoadSystemSetting(const std::string strSettingPath); public: ORBVocabulary* pVocavulary; //相机参数 float width; float height; float fx; float fy; float cx; float cy; float invfx; float invfy; float bf; float b; float fps; cv::Mat K; cv::Mat DistCoef; bool initialized; //相机 RGB 参数 int nRGB; //ORB特征参数 int nFeatures; float fScaleFactor; int nLevels; float fIniThFAST; float fMinThFAST; //其他参数 float ThDepth = -1; float DepthMapFactor = -1; &#125;; &#125;//namespace ORB_SLAM2#endif //SystemSetting 创建 SystemSetting.cc的函数具体实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788#include&lt;iostream&gt;#include"SystemSetting.h"using namespace std;namespace ORB_SLAM2 &#123; SystemSetting::SystemSetting(ORBVocabulary* pVoc):pVocavulary(pVoc) &#123; &#125; bool SystemSetting::LoadSystemSetting(const std::string strSettingPath) &#123; cout&lt;&lt;endl&lt;&lt;"Loading System Parameters form:"&lt;&lt;strSettingPath&lt;&lt;endl; cv::FileStorage fSettings(strSettingPath, cv::FileStorage::READ); width = fSettings["Camera.width"]; height = fSettings["Camera.height"]; fx = fSettings["Camera.fx"]; fy = fSettings["Camera.fy"]; cx = fSettings["Camera.cx"]; cy = fSettings["Camera.cy"]; cv::Mat tmpK = cv::Mat::eye(3,3,CV_32F); tmpK.at&lt;float&gt;(0,0) = fx; tmpK.at&lt;float&gt;(1,1) = fy; tmpK.at&lt;float&gt;(0,2) = cx; tmpK.at&lt;float&gt;(1,2) = cy; tmpK.copyTo(K); cv::Mat tmpDistCoef(4,1,CV_32F); tmpDistCoef.at&lt;float&gt;(0) = fSettings["Camera.k1"]; tmpDistCoef.at&lt;float&gt;(1) = fSettings["Camera.k2"]; tmpDistCoef.at&lt;float&gt;(2) = fSettings["Camera.p1"]; tmpDistCoef.at&lt;float&gt;(3) = fSettings["Camera.p2"]; const float k3 = fSettings["Camera.k3"]; if( k3!=0 ) &#123; tmpDistCoef.resize(5); tmpDistCoef.at&lt;float&gt;(4) = k3; &#125; tmpDistCoef.copyTo( DistCoef ); bf = fSettings["Camera.bf"]; fps= fSettings["Camera.fps"]; invfx = 1.0f/fx; invfy = 1.0f/fy; b = bf /fx; initialized = true; cout&lt;&lt;"- size:"&lt;&lt;width&lt;&lt;"x"&lt;&lt;height&lt;&lt;endl; cout&lt;&lt;"- fx:" &lt;&lt;fx&lt;&lt;endl; cout &lt;&lt; "- fy: " &lt;&lt; fy &lt;&lt; endl; cout &lt;&lt; "- cx: " &lt;&lt; cx &lt;&lt; endl; cout &lt;&lt; "- cy: " &lt;&lt; cy &lt;&lt; endl; cout &lt;&lt; "- k1: " &lt;&lt; DistCoef.at&lt;float&gt;(0) &lt;&lt; endl; cout &lt;&lt; "- k2: " &lt;&lt; DistCoef.at&lt;float&gt;(1) &lt;&lt; endl; if(DistCoef.rows==5) cout &lt;&lt; "- k3: " &lt;&lt; DistCoef.at&lt;float&gt;(4) &lt;&lt; endl; cout &lt;&lt; "- p1: " &lt;&lt; DistCoef.at&lt;float&gt;(2) &lt;&lt; endl; cout &lt;&lt; "- p2: " &lt;&lt; DistCoef.at&lt;float&gt;(3) &lt;&lt; endl; cout &lt;&lt; "- bf: " &lt;&lt; bf &lt;&lt; endl; //Load RGB parameter nRGB = fSettings["Camera.RGB"]; //Load ORB feature parameters nFeatures = fSettings["ORBextractor.nFeatures"]; fScaleFactor = fSettings["ORBextractor.scaleFactor"]; nLevels = fSettings["ORBextractor.nLevels"]; fIniThFAST = fSettings["ORBextractor.iniThFAST"]; fMinThFAST = fSettings["ORBextractor.minThFAST"]; cout &lt;&lt; endl &lt;&lt; "ORB Extractor Parameters: " &lt;&lt; endl; cout &lt;&lt; "- Number of Features: " &lt;&lt; nFeatures &lt;&lt; endl; cout &lt;&lt; "- Scale Levels: " &lt;&lt; nLevels &lt;&lt; endl; cout &lt;&lt; "- Scale Factor: " &lt;&lt; fScaleFactor &lt;&lt; endl; cout &lt;&lt; "- Initial Fast Threshold: " &lt;&lt; fIniThFAST &lt;&lt; endl; cout &lt;&lt; "- Minimum Fast Threshold: " &lt;&lt; fMinThFAST &lt;&lt; endl; //Load others parameters, if the sensor is MONOCULAR, the parameters is zero; //ThDepth = fSettings["ThDepth"]; //DepthMapFactor = fSettings["DepthMapFactor"]; fSettings.release(); return true; &#125;&#125; 创建 InitKeyFrame.h文件 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182#ifndef INITKEYFRAME_H#define INITKEYFRAME_H#include "Thirdparty/DBoW2/DBoW2/BowVector.h"#include "Thirdparty/DBoW2/DBoW2/FeatureVector.h"#include "SystemSetting.h"#include &lt;opencv2/opencv.hpp&gt;#include "ORBVocabulary.h"#include "KeyFrameDatabase.h"//#include "MapPoints.h"namespace ORB_SLAM2&#123;#define FRAME_GRID_ROWS 48#define FRAME_GRID_COLS 64class SystemSetting;class KeyFrameDatabase;//class ORBVocabulary;class InitKeyFrame&#123;public: InitKeyFrame(SystemSetting &amp;SS); void UndistortKeyPoints(); bool PosInGrid(const cv::KeyPoint&amp; kp, int &amp;posX, int &amp;posY); void AssignFeaturesToGrid();public: ORBVocabulary* pVocabulary; //KeyFrameDatabase* pKeyFrameDatabase; long unsigned int nId; double TimeStamp; float fGridElementWidthInv; float fGridElementHeightInv; std::vector&lt;std::size_t&gt; vGrid[FRAME_GRID_COLS][FRAME_GRID_ROWS]; float fx; float fy; float cx; float cy; float invfx; float invfy; float bf; float b; float ThDepth; int N; std::vector&lt;cv::KeyPoint&gt; vKps; std::vector&lt;cv::KeyPoint&gt; vKpsUn; cv::Mat Descriptors; //it's zero for mono std::vector&lt;float&gt; vRight; std::vector&lt;float&gt; vDepth; DBoW2::BowVector BowVec; DBoW2::FeatureVector FeatVec; int nScaleLevels; float fScaleFactor; float fLogScaleFactor; std::vector&lt;float&gt; vScaleFactors; std::vector&lt;float&gt; vLevelSigma2; std::vector&lt;float&gt; vInvLevelSigma2; std::vector&lt;float&gt; vInvScaleFactors; int nMinX; int nMinY; int nMaxX; int nMaxY; cv::Mat K; cv::Mat DistCoef; &#125;;&#125; //namespace ORB_SLAM2#endif //INITKEYFRAME_H 创建 InitKeyFrame.cc的函数具体实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136#include "InitKeyFrame.h"#include &lt;opencv2/opencv.hpp&gt;#include "SystemSetting.h"namespace ORB_SLAM2&#123;InitKeyFrame::InitKeyFrame(SystemSetting &amp;SS):pVocabulary(SS.pVocavulary)//, pKeyFrameDatabase(SS.pKeyFrameDatabase)&#123; fx = SS.fx; fy = SS.fy; cx = SS.cx; cy = SS.cy; invfx = SS.invfx; invfy = SS.invfy; bf = SS.bf; b = SS.b; ThDepth = SS.ThDepth; nScaleLevels = SS.nLevels; fScaleFactor = SS.fScaleFactor; fLogScaleFactor = log(SS.fScaleFactor); vScaleFactors.resize(nScaleLevels); vLevelSigma2.resize(nScaleLevels); vScaleFactors[0] = 1.0f; vLevelSigma2[0] = 1.0f; for ( int i = 1; i &lt; nScaleLevels; i ++ ) &#123; vScaleFactors[i] = vScaleFactors[i-1]*fScaleFactor; vLevelSigma2[i] = vScaleFactors[i]*vScaleFactors[i]; &#125; vInvScaleFactors.resize(nScaleLevels); vInvLevelSigma2.resize(nScaleLevels); for ( int i = 0; i &lt; nScaleLevels; i ++ ) &#123; vInvScaleFactors[i] = 1.0f/vScaleFactors[i]; vInvLevelSigma2[i] = 1.0f/vLevelSigma2[i]; &#125; K = SS.K; DistCoef = SS.DistCoef; if( SS.DistCoef.at&lt;float&gt;(0)!=0.0) &#123; cv::Mat mat(4,2,CV_32F); mat.at&lt;float&gt;(0,0) = 0.0; mat.at&lt;float&gt;(0,1) = 0.0; mat.at&lt;float&gt;(1,0) = SS.width; mat.at&lt;float&gt;(1,1) = 0.0; mat.at&lt;float&gt;(2,0) = 0.0; mat.at&lt;float&gt;(2,1) = SS.height; mat.at&lt;float&gt;(3,0) = SS.width; mat.at&lt;float&gt;(3,1) = SS.height; mat = mat.reshape(2); cv::undistortPoints(mat, mat, SS.K, SS.DistCoef, cv::Mat(), SS.K); mat = mat.reshape(1); nMinX = min(mat.at&lt;float&gt;(0,0), mat.at&lt;float&gt;(2,0)); nMaxX = max(mat.at&lt;float&gt;(1,0), mat.at&lt;float&gt;(3,0)); nMinY = min(mat.at&lt;float&gt;(0,1), mat.at&lt;float&gt;(1,1)); nMaxY = max(mat.at&lt;float&gt;(2,1), mat.at&lt;float&gt;(3,1)); &#125; else &#123; nMinX = 0.0f; nMaxX = SS.width; nMinY = 0.0f; nMaxY = SS.height; &#125; fGridElementWidthInv=static_cast&lt;float&gt;(FRAME_GRID_COLS)/(nMaxX-nMinX); fGridElementHeightInv=static_cast&lt;float&gt;(FRAME_GRID_ROWS)/(nMaxY-nMinY); &#125;void InitKeyFrame::UndistortKeyPoints()&#123; if( DistCoef.at&lt;float&gt;(0) == 0.0) &#123; vKpsUn = vKps; return; &#125; cv::Mat mat(N,2,CV_32F); for ( int i = 0; i &lt; N; i ++ ) &#123; mat.at&lt;float&gt;(i,0) = vKps[i].pt.x; mat.at&lt;float&gt;(i,1) = vKps[i].pt.y; &#125; mat = mat.reshape(2); cv::undistortPoints(mat, mat, K, DistCoef, cv::Mat(), K ); mat = mat.reshape(1); vKpsUn.resize(N); for( int i = 0; i &lt; N; i ++ ) &#123; cv::KeyPoint kp = vKps[i]; kp.pt.x = mat.at&lt;float&gt;(i,0); kp.pt.y = mat.at&lt;float&gt;(i,1); vKpsUn[i] = kp; &#125;&#125;void InitKeyFrame::AssignFeaturesToGrid()&#123; int nReserve = 0.5f*N/(FRAME_GRID_COLS*FRAME_GRID_ROWS); for ( unsigned int i = 0; i &lt; FRAME_GRID_COLS; i ++ ) &#123; for ( unsigned int j = 0; j &lt; FRAME_GRID_ROWS; j ++) vGrid[i][j].reserve(nReserve); &#125; for ( int i = 0; i &lt; N; i ++ ) &#123; const cv::KeyPoint&amp; kp = vKpsUn[i]; int nGridPosX, nGridPosY; if( PosInGrid(kp, nGridPosX, nGridPosY)) vGrid[nGridPosX][nGridPosY].push_back(i); &#125;&#125;bool InitKeyFrame::PosInGrid(const cv::KeyPoint &amp;kp, int &amp;posX, int &amp;posY)&#123; posX = round((kp.pt.x-nMinX)*fGridElementWidthInv); posY = round((kp.pt.y-nMinY)*fGridElementHeightInv); if(posX&lt;0 || posX&gt;=FRAME_GRID_COLS ||posY&lt;0 || posY&gt;=FRAME_GRID_ROWS) return false; return true;&#125;&#125; （5）System相关文件的修改在System.h中添加函数定义： 1void LoadMap(const string &amp;filename); 添加声明： 1std::string mySettingFile; 在对应的System.cc中添加了定义 12345678//地图加载void System::LoadMap(const string &amp;filename)&#123; SystemSetting* mySystemSetting = new SystemSetting(mpVocabulary); mySystemSetting-&gt;LoadSystemSetting(mySettingFile); mpMap-&gt;Load(filename,mySystemSetting);&#125; 同时在构造函数函数中对mySettingFile成员变量赋值 1mySettingFile = strSettingsFile; //放在System::System里的check settings file 后面 2.2 测试修改CMakeLists.txt 文件 123456789101112131415161718192021222324#在add_library 中加入 src/InitkeyFrame.cc src/SystemSetting.ccadd_library($&#123;PROJECT_NAME&#125; SHAREDsrc/System.ccsrc/Tracking.ccsrc/LocalMapping.ccsrc/LoopClosing.ccsrc/ORBextractor.ccsrc/ORBmatcher.ccsrc/FrameDrawer.ccsrc/Converter.ccsrc/MapPoint.ccsrc/KeyFrame.ccsrc/Map.ccsrc/MapDrawer.ccsrc/Optimizer.ccsrc/PnPsolver.ccsrc/Frame.ccsrc/KeyFrameDatabase.ccsrc/Sim3Solver.ccsrc/Initializer.ccsrc/Viewer.ccsrc/InitKeyFrame.ccsrc/SystemSetting.cc) 一定要注意大小写！！！！网上写的是src/InitkeyFrame.cc，报错找了半天问题。 修改/ORB_SLAM2/Examples/ROS/ORB_SLAM2/src/ros_rgbd.cc文件中的main函数中加入如下语句： 1SLAM.LoadMap("/home/zj224/ORB_SLAM2/Examples/ROS/ORB_SLAM2/map.bin");//load the map 加在ros::spin()之前 重新编译ORB_SLAM2库 123456789cd ~/ORB_SLAM2chmod +x build.sh./build.sh source /opt/ros/kinetic/setup.shsource ~/.bashrcchmod +x build_ros.sh./build_ros.sh### 若出错，参考 ORB_slam实现 测试 12345678roscore### 新建终端roslaunch kinect2_bridge kinect2_bridge.launch### 新建终端cd ~/ORB_SLAM2/Examples/ROS/ORB_SLAM2rosrun ORB_SLAM2 RGBD /home/zj224/ORB_SLAM2/Vocabulary/ORBvoc.txt /home/zj224/ORB_SLAM2/Examples/RGB-D/kinect2.yaml### 就能看到加载的地图了 TODO加载功能也只是单纯地实现了地图加载，重定位、导航等问题有待进一步完善。这两天做地图保存和加载功能的实现，我发觉应该先学习ORB_SLAM2整个库的逻辑结构和代码解读，而不是直接按照博客的东西一顿忙改，功能实现了，但是不懂内部是不行的。接下来会先学习这个库，再去研究进一步的功能。]]></content>
      <categories>
        <category>slam</category>
      </categories>
      <tags>
        <tag>slam实现</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ORB_SLAM2地图保存与加载（1）]]></title>
    <url>%2F2019%2F08%2F26%2FORB_SLAM2%E5%9C%B0%E5%9B%BE%E4%BF%9D%E5%AD%98%E4%B8%8E%E5%8A%A0%E8%BD%BD%EF%BC%881%EF%BC%89%2F</url>
    <content type="text"><![CDATA[本文记录了ORB_SLAM2中地图保存的过程。参考博客：https://blog.csdn.net/qq_34254510/article/details/79969046http://www.cnblogs.com/mafuqiang/p/6972841.htmlhttps://blog.csdn.net/felaim/article/details/79667635https://blog.csdn.net/u014709760/article/details/86319090 1 地图保存1.1 地图元素分析所谓地图保存，就是保存地图“Map”中的各个元素，以及它们之间的关系，凡是跟踪过程中需要用到的东西自然也就是需要保存的对象。地图主要包含关键帧、3D地图点、BoW向量、共视图、生长树等，在跟踪过程中有三种跟踪模型和局部地图跟踪等过程，局部地图跟踪需要用到3D地图点、共视关系等元素，参考帧模型需要用到关键帧的BoW向量，重定位需要用到BoW向量、3D点等。所以基本上述元素都需要保存。 另一方面，关键帧也是一个抽象的概念（一个类），我们看看具体包含什么（其实都在关键帧类里面了），关键帧是从普通帧来的，所以得到视频帧后首先需要做的就是检测特征点，计算描述符，还有当前帧的相机位姿。成为关键帧之后需要有对应的ID编号，以及特征点进行三角化之后的3D地图点等。 关于3D地图点需要保存的就只有世界坐标了，至于其它的关联关系可以从关键帧获得。需要单独说的是在关键帧类中包含了特征点和描述符，所以BoW向量是不需要保存的（也没办法保存），只需要在加载了关键帧之后利用特征描述符重新计算即可。 所以现在需要保存的东西包括关键帧、3D地图点、共视图、生长树。 1.2 源码修改SLAM对地图维护的操作均在Map.cc这个函数类中，所以要保存地图，我们需要在这个文件中添加相应代码。 （1）修改Map.h头文件在/ORB_SLAM2/include/Map.h文件中的Map类中添加如下函数： 在Map.h的头文件中要添加Converter.h 123456789public: //保存地图信息 void Save(const string &amp;filename);protected: //保存地图点和关键帧 void SaveMapPoint(ofstream &amp;f,MapPoint* mp); void SaveKeyFrame(ofstream &amp;f,KeyFrame* kf); std::map&lt;MapPoint*, unsigned long int&gt; mmpnMapPointsIdx; void GetMapPointsIdx(); （2）修改Map.cc文件在/ORB_SLAM2/src/Map.cc文件中添加第一步中函数的实现。 Save()函数的实现过程： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162//保存地图信息void Map::Save ( const string&amp; filename )&#123; //Print the information of the saving map cerr&lt;&lt;"Map.cc :: Map Saving to "&lt;&lt;filename &lt;&lt;endl; ofstream f; f.open(filename.c_str(), ios_base::out|ios::binary); //Number of MapPoints unsigned long int nMapPoints = mspMapPoints.size(); f.write((char*)&amp;nMapPoints, sizeof(nMapPoints) ); //Save MapPoint sequentially for ( auto mp: mspMapPoints )&#123; //Save MapPoint SaveMapPoint( f, mp ); // cerr &lt;&lt; "Map.cc :: Saving map point number: " &lt;&lt; mp-&gt;mnId &lt;&lt; endl; &#125; //Print The number of MapPoints cerr &lt;&lt; "Map.cc :: The number of MapPoints is :"&lt;&lt;mspMapPoints.size()&lt;&lt;endl; //Grab the index of each MapPoint, count from 0, in which we initialized mmpnMapPointsIdx GetMapPointsIdx(); //Print the number of KeyFrames cerr &lt;&lt;"Map.cc :: The number of KeyFrames:"&lt;&lt;mspKeyFrames.size()&lt;&lt;endl; //Number of KeyFrames unsigned long int nKeyFrames = mspKeyFrames.size(); f.write((char*)&amp;nKeyFrames, sizeof(nKeyFrames)); //Save KeyFrames sequentially for ( auto kf: mspKeyFrames ) SaveKeyFrame( f, kf ); for (auto kf:mspKeyFrames ) &#123; //Get parent of current KeyFrame and save the ID of this parent KeyFrame* parent = kf-&gt;GetParent(); unsigned long int parent_id = ULONG_MAX; if ( parent ) parent_id = parent-&gt;mnId; f.write((char*)&amp;parent_id, sizeof(parent_id)); //Get the size of the Connected KeyFrames of the current KeyFrames //and then save the ID and weight of the Connected KeyFrames unsigned long int nb_con = kf-&gt;GetConnectedKeyFrames().size(); f.write((char*)&amp;nb_con, sizeof(nb_con)); for ( auto ckf: kf-&gt;GetConnectedKeyFrames()) &#123; int weight = kf-&gt;GetWeight(ckf); f.write((char*)&amp;ckf-&gt;mnId, sizeof(ckf-&gt;mnId)); f.write((char*)&amp;weight, sizeof(weight)); &#125; &#125; // Save last Frame ID // SaveFrameID(f); f.close(); cerr&lt;&lt;"Map.cc :: Map Saving Finished!"&lt;&lt;endl;&#125; 存储地图点函数——SaveMapPoint()函数的实现： 123456789void Map::SaveMapPoint( ofstream&amp; f, MapPoint* mp)&#123; //Save ID and the x,y,z coordinates of the current MapPoint f.write((char*)&amp;mp-&gt;mnId, sizeof(mp-&gt;mnId)); cv::Mat mpWorldPos = mp-&gt;GetWorldPos(); f.write((char*)&amp; mpWorldPos.at&lt;float&gt;(0),sizeof(float)); f.write((char*)&amp; mpWorldPos.at&lt;float&gt;(1),sizeof(float)); f.write((char*)&amp; mpWorldPos.at&lt;float&gt;(2),sizeof(float));&#125; 存储关键帧函数——SaveKeyFrame()函数的实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263void Map::SaveKeyFrame( ofstream &amp;f, KeyFrame* kf )&#123; //Save the ID and timesteps of current KeyFrame f.write((char*)&amp;kf-&gt;mnId, sizeof(kf-&gt;mnId)); // cout &lt;&lt; "saving kf-&gt;mnId = " &lt;&lt; kf-&gt;mnId &lt;&lt;endl; f.write((char*)&amp;kf-&gt;mTimeStamp, sizeof(kf-&gt;mTimeStamp)); //Save the Pose Matrix of current KeyFrame cv::Mat Tcw = kf-&gt;GetPose(); ////Save the rotation matrix // for ( int i = 0; i &lt; Tcw.rows; i ++ ) // &#123; // for ( int j = 0; j &lt; Tcw.cols; j ++ ) // &#123; // f.write((char*)&amp;Tcw.at&lt;float&gt;(i,j), sizeof(float)); // //cerr&lt;&lt;"Tcw.at&lt;float&gt;("&lt;&lt;i&lt;&lt;","&lt;&lt;j&lt;&lt;"):"&lt;&lt;Tcw.at&lt;float&gt;(i,j)&lt;&lt;endl; // &#125; // &#125; //Save the rotation matrix in Quaternion std::vector&lt;float&gt; Quat = Converter::toQuaternion(Tcw); for ( int i = 0; i &lt; 4; i ++ ) f.write((char*)&amp;Quat[i],sizeof(float)); //Save the translation matrix for ( int i = 0; i &lt; 3; i ++ ) f.write((char*)&amp;Tcw.at&lt;float&gt;(i,3),sizeof(float)); //Save the size of the ORB features current KeyFrame //cerr&lt;&lt;"kf-&gt;N:"&lt;&lt;kf-&gt;N&lt;&lt;endl; f.write((char*)&amp;kf-&gt;N, sizeof(kf-&gt;N)); //Save each ORB features for( int i = 0; i &lt; kf-&gt;N; i ++ ) &#123; cv::KeyPoint kp = kf-&gt;mvKeys[i]; f.write((char*)&amp;kp.pt.x, sizeof(kp.pt.x)); f.write((char*)&amp;kp.pt.y, sizeof(kp.pt.y)); f.write((char*)&amp;kp.size, sizeof(kp.size)); f.write((char*)&amp;kp.angle,sizeof(kp.angle)); f.write((char*)&amp;kp.response, sizeof(kp.response)); f.write((char*)&amp;kp.octave, sizeof(kp.octave)); //Save the Descriptors of current ORB features f.write((char*)&amp;kf-&gt;mDescriptors.cols, sizeof(kf-&gt;mDescriptors.cols)); //kf-&gt;mDescriptors.cols is always 32 here. for (int j = 0; j &lt; kf-&gt;mDescriptors.cols; j ++ ) f.write((char*)&amp;kf-&gt;mDescriptors.at&lt;unsigned char&gt;(i,j), sizeof(char)); //Save the index of MapPoints that corresponds to current ORB features unsigned long int mnIdx; MapPoint* mp = kf-&gt;GetMapPoint(i); if (mp == NULL ) mnIdx = ULONG_MAX; else mnIdx = mmpnMapPointsIdx[mp]; f.write((char*)&amp;mnIdx, sizeof(mnIdx)); &#125; // Save BoW for relocalization. // f.write((char*)&amp;kf-&gt;mBowVec, sizeof(kf-&gt;mBowVec));&#125; GetMapPointsIdx()函数的实现过程为： 12345678910void Map::GetMapPointsIdx()&#123; unique_lock&lt;mutex&gt; lock(mMutexMap); unsigned long int i = 0; for ( auto mp: mspMapPoints ) &#123; mmpnMapPointsIdx[mp] = i; i += 1; &#125;&#125; （3）修改Converter相关文件关于旋转矩阵的存储可以通过四元数或矩阵的形式存储，如果使用四元数需要自定义一个矩阵和四元数相互转换的函数，在Converter.cc类里面添加如下函数： 123456789101112cv::Mat Converter::toCvMat(const std::vector&lt;float&gt;&amp; v)&#123; Eigen::Quaterniond q; q.x() = v[0]; q.y() = v[1]; q.z() = v[2]; q.w() = v[3]; Eigen::Matrix&lt;double,3,3&gt; eigMat(q); cv::Mat M = toCvMat(eigMat); return M;&#125; 在Converter.h里面加上如下函数定义 1static cv::Mat toCvMat( const std::vector&lt;float&gt;&amp; v )； （4）System文件修改上述修改完成之后，还需要对system.h和system.cc文件进行修改，分别添加声明和定义。system.h文件： 1void SaveMap(const string &amp;filename); system.cc文件: 12345//地图保存void System::SaveMap(const string &amp;filename)&#123; mpMap-&gt;Save(filename);&#125; 1.3 测试做完这些修改之后，在Examples文件中对应的示例程序中加入地图存储代码即可实现地图存储功能。 修改/ORB_SLAM2/Examples/ROS/ORB_SLAM2/src/ros_rgbd.cc文件中的main函数中加入如下语句： 1SLAM.SaveMap("map.bin"); 重新编译ORB_SLAM2库 123456789cd ~/ORB_SLAM2chmod +x build.sh./build.sh source /opt/ros/kinetic/setup.shsource ~/.bashrcchmod +x build_ros.sh./build_ros.sh### 若出错，参考 ORB_slam实现 测试 12345678roscore### 新建终端roslaunch kinect2_bridge kinect2_bridge.launch### 新建终端cd ~/ORB_SLAM2/Examples/ROS/ORB_SLAM2rosrun ORB_SLAM2 RGBD /home/zj224/ORB_SLAM2/Vocabulary/ORBvoc.txt /home/zj224/ORB_SLAM2/Examples/RGB-D/kinect2.yaml### 建图完成后 ctrl+C 地图保存在/ORB_SLAM2/Examples/ROS/ORB_SLAM2]]></content>
      <categories>
        <category>slam</category>
      </categories>
      <tags>
        <tag>slam实现</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[yilia 主题一些问题解决]]></title>
    <url>%2F2019%2F08%2F25%2Fyilia%20%E4%B8%BB%E9%A2%98%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%2F</url>
    <content type="text"><![CDATA[1 公式无法显示yilia 目录下有一个_config.yml 编辑里面的 123#数学公式mathjax: false 将false 改为true即可 2 修改头像yilia 目录下有一个_config.yml 编辑里面的 12#你的头像urlavatar: /img/1.jpg 将头像图片放在yilia/source/img中即可 3 上传图片配置 hexo 的_config.yml 1post_asset_folder: true 安装上传本地图片插件 1npm install hexo-asset-image --save ### 先cd 到你的文件夹 新建博客 12hexo n &quot;xxx&quot;# 在/source/_posts路径下会生成一个xxx.md和xxx文件 在md文件中引入图片，将你的图片放入xxx文件夹中 1![你想输入的替代文字](xxxx/图片名.jpg)]]></content>
      <categories>
        <category>问题解决</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ORB_SLAM2 在ROS中使用KinectV2实现]]></title>
    <url>%2F2019%2F08%2F24%2FORB_SLAM2%20%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[安装依赖包12345678910111213141516171819202122ssh -T git@github.comsudo apt-get install libboost-all-dev libblas-dev liblapack-dev#### 安装 Pangolingit clone git@github.com:stevenlovegrove/Pangolin.gitcd Pangolinmkdir buildcd buildcmake ..cmake --build .##### 安装 eigenhttp://eigen.tuxfamily.org/index.php?title=Main_Page ### 下载eigen包 解压cd eigenmkdir buildcd buildcmake ..makesudo make installsudo apt-get install libopencv-dev libeigen3-dev libqt4-dev qt4-qmake libqglviewer-dev libsuitesparse-dev libcxsparse3.1.4 libcholmod3.0.6 安装ORB_SLAM21234567891011git clone git@github.com:raulmur/ORB_SLAM2.gitcd ORB_SLAM2chmod +x build.shgedit ./build.sh修改最后一行，改为make./build.sh### 若报错 error: ‘usleep’ was not declaredcd ORB_SLAM2/srcgedit System.cc ### 报错的其他文件一样，不再一一说明，很多，要有耐心 T_T添加头文件 #include &lt;unistd.h&gt; 配置KINECT 苏齐光已经整理了一个Kinect配置文件，这里不再赘述。 KINECT 标定制作标定板chess5x7x0.03.pdfchess7x9x0.025.pdfchess9x11x0.02.pdf 这里我选择的是第三个 ！！！这里一定要注意，9x11实际上方格数是10x12 ！！！这里我搞错了耽误了一天。。。 建立临时文件夹以免图片太多看起来很乱 12mkdir ~/kinect_cal_tempcd kinect_cal_temp 标定步骤12345678910111213141516171819202122rosrun kinect2_bridge kinect2_bridge _fps_limit:=2 ### 先运行roscore### 显示的 [ INFO] [Kinect2Bridge::initDevice] device serial: 019968265047 后面的数为设备串口号### 在我的/home/catkin_ws/src/iai_kinect2/kinect2_bridge/data的文件夹里建立一个文件夹，取名叫 019968265047### 标定彩色摄像头：rosrun kinect2_calibration kinect2_calibration chess9x11x0.02 record color ### 按空格保存图片，10+张，后面一样rosrun kinect2_calibration kinect2_calibration chess9x11x0.02 calibrate color生成calib_color.yaml 文件 ### 标定红外rosrun kinect2_calibration kinect2_calibration chess9x11x0.02 record irrosrun kinect2_calibration kinect2_calibration chess9x11x0.02 calibrate ir### 会生成calib_ir.yaml 文件### 帧同步标定rosrun kinect2_calibration kinect2_calibration chess9x11x0.02 record syncrosrun kinect2_calibration kinect2_calibration chess9x11x0.02 calibrate sync### 会生成calib_pose.yaml 文件 ### 深度标定rosrun kinect2_calibration kinect2_calibration chess9x11x0.02 calibrate depth### 会生成calib_depth.yaml 文件 标定后的文件calib_color.yaml calib_ir.yaml calib_pose.yaml calib_depth.yaml 需要放到/home/catkin_ws/src/iai_kinect2/kinect2_bridge/data/019968265047这个文件夹里 标定至此完成！ 修改 ORB-SLAM2文件改动 1修改/home/zj224/ORB_SLAM2/Examples/ROS/ORB_SLAM2/src 中的 ros_rgbd.cc 文件的 main 函数： 12message_filters::Subscriber&lt;sensor_msgs::Image&gt; rgb_sub(nh, &quot;/camera/rgb/image_raw&quot;, 1);message_filters::Subscriber&lt;sensor_msgs::Image&gt; depth_sub(nh,&quot;camera/depth_registered/image_raw&quot;, 1); 改为： 12message_filters::Subscriber&lt;sensor_msgs::Image&gt; rgb_sub(nh,&quot;/kinect2/qhd/image_color&quot;,1);message_filters::Subscriber&lt;sensor_msgs::Image&gt; depth_sub(nh,&quot;/kinect2/qhd/image_depth_rect&quot;,1); 改动 2设置标定相机参数，仿照/Examples/RGB-D/TUM1.yaml 根据之前得到的 calib_color.yaml 修改并另存为 kinect2.yaml 1234567891011121314# Camera calibration and distortion parameters (OpenCV) Camera.fx: 1.0679837281443886e+03Camera.fy: 1.0697937777504162e+03Camera.cx: 9.3735357113460532e+02Camera.cy: 5.5068347235162321e+02Camera.k1: 8.4529458178805153e-02Camera.k2: -1.3472803452135898e-01Camera.p1: 2.4226930973738920e-03Camera.p2: -3.1065128414445530e-03Camera.k3: 3.3625687689377249e-03Camera.width: 1920Camera.height: 1080 改动 3123sudo gedit ~/.bashrc添加 export ROS_PACKAGE_PATH=$&#123;ROS_PACKAGE_PATH&#125;:/home/zj224/ORB_SLAM2/Examples/ROSsource ~/.bashrc 再次编译 ORB_SLAM2 12345678cd ~/ORB_SLAM2chmod +x build.sh./build.sh source /opt/ros/kinetic/setup.shchmod +x build_ros.sh./build_ros.sh 报错 [rosbuild] rospack found package “ORB_SLAM2” at “”, but the current directory is “/home/zj224/ORB_SLAM2/Examples/ROS/ORB_SLAM2”. 运行： 12sudo ln -s /home/ORB_SLAM2/Examples/ROS/ORB_SLAM2 /opt/ros/kinetic/share/ORB_SLAM2source ~/.bashrc 报错 CMakeFiles/RGBD.dir/src/ros_rgbd.cc.o: undefined reference to symbol ‘_ZN5boost6system15system_categoryEv’ 将/usr/lib/x86_64-linux-gnu/libboost_system.so/usr/lib/x86_64-linux-gnu/libboost_filesystem.so两个文件复制到 ORB_SLAM2 / lib修改/home/zj224/ORB_SLAM2/Examples/ROS/ORB_SLAM2 中的 CMakeLists.txt修改前： 123456789set(LIBS $&#123;OpenCV_LIBS&#125; $&#123;EIGEN3_LIBS&#125;$&#123;Pangolin_LIBRARIES&#125;$&#123;PROJECT_SOURCE_DIR&#125;/../../../Thirdparty/DBoW2/lib/libDBoW2.so$&#123;PROJECT_SOURCE_DIR&#125;/../../../Thirdparty/g2o/lib/libg2o.so$&#123;PROJECT_SOURCE_DIR&#125;/../../../lib/libORB_SLAM2.so) 修改后： 12345678910set(LIBS $&#123;OpenCV_LIBS&#125; $&#123;EIGEN3_LIBS&#125;$&#123;Pangolin_LIBRARIES&#125;$&#123;PROJECT_SOURCE_DIR&#125;/../../../Thirdparty/DBoW2/lib/libDBoW2.so$&#123;PROJECT_SOURCE_DIR&#125;/../../../Thirdparty/g2o/lib/libg2o.so$&#123;PROJECT_SOURCE_DIR&#125;/../../../lib/libORB_SLAM2.so-lboost_system) 运行程序1roscore 新建一个终端 1roslaunch kinect2_bridge kinect2_bridge.launch 新建一个终端 12cd ~/ORB_SLAM2/Examples/ROS/ORB_SLAM2rosrun ORB_SLAM2 RGBD /home/zj224/ORB_SLAM2/Vocabulary/ORBvoc.txt /home/zj224/ORB_SLAM2/Examples/RGB-D/kinect2.yaml]]></content>
      <categories>
        <category>slam</category>
      </categories>
      <tags>
        <tag>slam实现</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[回环检测]]></title>
    <url>%2F2019%2F08%2F22%2F%E5%9B%9E%E7%8E%AF%E6%A3%80%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[1 什么是回环检测？ 在视觉slam问题中，位姿的估计往往是一个递推的过程，即由上一帧位姿解算当前帧位姿，因此其中的误差便这样一帧一帧的传递下去，也就是我们所说的累计误差。 我们的位姿约束都是与上一帧建立的，第五帧的位姿误差中便已经积累了前面四个约束中的误差。但如果我们发现第五帧位姿不一定要由第四帧推出来，还可以由第二帧推算出来，显然这样计算误差会小很多，因为只存在两个约束的误差了。像这样与之前的某一帧建立位姿约束关系就叫做回环。回环通过减少约束数，起到了减小累计误差的作用。 那么我们怎么知道可以由第二帧推算第五帧位姿呢？也许第一帧、第三帧也可以呢。确实，我们之所以用前一帧递推下一帧位姿，因为这两帧足够近，肯定可以建立两帧的约束，但是距离较远的两帧就不一定可以建立这样的约束关系了。找出可以建立这种位姿约束的历史帧，就是回环检测。 2 回环检测的意义 举例来说，假设我们在前端提取了特征，然后忽略掉特征点，在后端使用 Pose Graph优化整个轨迹，如图 12-1(a) 所示。由于前端给出的只是局部的位姿间约束，比方说，可能是$x_1 − x_2$， $x_ 2 − x _3 $等等。但是，由于 $x _1$ 的估计存在误差,而 $x_2 $是根据 $x _1$ 决定的,$x_3$是由$x_2 $决定的。以此类推，误差就会被累积起来，使得后端优化的结果如图 12-1 (b)所示，慢慢地趋向不准确。而回环检测则可以消除这种累积误差，如图12-1 (c)所示。 回环检测对于 SLAM 系统意义重大。它关系到我们估计的轨迹和地图在长时间下的正确性。另一方面，由于回环检测提供了当前数据与所有历史数据的关联，在跟踪算法丢失之后，我们还可以利用回环检测进行重定位。因此，回环检测对整个 SLAM 系统精度与鲁棒性的提升，是非常明显的。甚至在某些时候，我们把仅有前端和局部后端的系统称为VO，而把带有回环检测和全局后端的称为 SLAM。 3 回环检测的方法 回环的产生是因为相机经过了同一个地方，采集到了相似的数据。而回环检测的关键，就是如何有效地检测出相机经过同一个地方这件事。 回环检测的方法大致分为两种思路：基于里程计的几何关系(Odometry based)，或基于外观(Appearance based)。这里只讲最主流的基于外观的回环检测方法。 它和前端后端的估计都无关，仅根据两张图像的相似性确定回环检测关系。这种做法摆脱了累计误差，使回环检测模块成为 SLAM 系统中一个相对独立的模块（当然前端可以为它提供特征点）。 3.1 准确率和召回率(Precision &amp; Recall)$$Precision = TP/(TP+FP) , Recall = TP/(TP+FN)$$ 准确率描述的是，算法提取的所有回环中，确实是真实回环的概率。 召回率则是说，在所有真实回环中，被正确检测出来的概率。 在 SLAM 中，我们对准确率要求更高，而对召回率则相对宽容一些。 3.2 词袋模型 在基于外观的回环检测算法中，核心问题是如何计算图像间的相似性。 最为直观的想法就是直接对比两个图像的矩阵，将两个图像相减，但是由于灰度是一种不稳定的测量值，严重受环境光照和相机曝光的影响，此外如果相机视角发生少量变化，同样的物体，同样的光照，像素发生了位移就会导致灰度值产生巨大差异。 词袋，也就是 Bag-of-Words（BoW），目的是用”图像上有哪几种特征“来描述一个图像。例如，如果某个照片，我们说里面有一个人、一辆车；而另一张则有两个人、一只狗。根据这样的描述，可以度量这两个图像的相似性。 ”人、车、狗“就是单词（Word）；许多单词放在一起组成字典（dictionary）。 “人”、“车”、“狗”都是记录在字典中的单词，我们不妨记为 $w_1,w_2,w_3$ 。然后，对于任意图像 A，根据它们含有的单词，可记为: $A=1\times w_1+1\times w_2+0\times w_3$ （即，$[1,1,0]^T$）来表示图像A中有一个“人”，一辆“车”，没有“狗”。这种方式只考虑有没有，不考虑在哪儿，能保证相机发生少量运动时，描述向量不发生变化。 3.3 字典 按照前面的介绍，字典由很多单词组成，而每一个单词代表了一个概念。一个单词与一个单独的特征点不同，它不是从单个图像上提取出来的，而是某一类特征的组合。所以，字典生成问题类似于一个聚类(Clustering)问题。 聚类问题是无监督机器学习(Unsupervised ML)中一个特别常见的问题，而K-means 是一个非常简单有效的方法。当我们有 N 个数据，想要归成 k 个类，K-means的步骤为： 随机选取 k 个中心点：$c_1 , . . . , c_k $； 对每一个样本，计算与每个中心点之间的距离，取最小的作为它的归类； 重新计算每个类的中心点。 如果每个中心点都变化很小，则算法收敛，退出；否则返回 1。 考虑到字典的通用性 ，,我们通常会使用一个较大规模的字典。这就需要使用k叉树来表达字典了。假定我们有 N 个特征点，希望构建一个深度为 d，每次分叉为 k 的树，那么做法如下：（如图12-4） 在根节点，用 k-means 把所有样本聚成 k 类。这样得到了第一层。 对第一层的每个节点，把属于该节点的样本再聚成 k 类，得到下一层。 依此类推，最后得到叶子层。叶子层即为所谓的 Words。 3.4 相似度计算 当我们建立了字典，并对两个图片分析特征点得到他们的词袋后，如何计算它们的相似的便成为一个非常关键的问题。考虑到，不同的单词在区分性上的重要性并不相同。例如“的”、“是”这样的字可能在许许多多的句子中出现，我们无法根据它们判别句子的类型；但如果有“文档”、“足球”这样的单词，对判别句子的作用就更大一些，可以说它们提供了更多信息。所以概括的话，我们希望对单词的区分性或重要性加以评估，给它们不同的权值以起到更好的效果。 在文本检索中，常用的一种做法称为 TF-IDF（Term Frequency–Inverse Document Frequency）。TF 部分的思想是，某单词在一个图像中经常出现，它的区分度就高。另一方面，IDF 的思想是，某单词在字典中出现的频率越低，则分类图像时区分度越高。 我们统计某个叶子节点 $w_i$ 中的特征数量相对于所有特征数量的比例，作为 IDF 部分。假设所有特征数量为 $n$，$w_i $数量为$n_i$ ，那么该单词的 IDF 为：$IDF_i=log\frac{n}{n_i}$ 另一方面，TF 部分则是指某个特征在单个图像中出现的频率。假设图像 A 中，单词$w_i $出现了$n_i$ 次，而一共出现的单词次数为$n$，那么 TF 为：$TF_i=\frac{n_i}{n}$ 单词$w_i $的权重等于：$\eta_i=TF_i\times IDF_i$ 考虑权重以后，对于某个图像 A，它的特征点可对应到许多个单词，组成它的 Bag-of-Words: $$A=[(w_1,\eta_1),(w_2,\eta_2),\cdots,(w_N,\eta_N)]=v_A$$ 给定 $v_A$和 $v_B$计算差异：$s(v_A-v_B)=$$\sqrt{\sum_{i=1}^{N}{|v_{Ai}-v_{Bi}|^2}}$ 欧氏距离；余弦相似度。。。 相似性评分的处理 对于一些相识度本身就很高的场景，单纯的计算相识度、设置阈值是很难取得好的效果的。这时候需要相对相似度的评分处理。取一个先验相似度 $s(v_t,v_{t-\Delta t})$，它表示某时刻关键帧图像与上一时刻的关键帧的相似性。然后，其他的分值都参照这个值进行归一化:$s(v_t,v_{t_j})’=s(v_t,v_{t_j})/s(v_t,v_{t-\Delta t})$，再设置阈值。 4 回环检测的实现主要使用的库基础库：DBoW3 https://github.com/rmsalinas/DBow3 slam方法库：ORB_SLAM2 https://github.com/raulmur/ORB_SLAM2 改进参考： https://github.com/Ewenwan/ORB_SLAM2_SSD_Semantic 参考资料： https://www.cnblogs.com/slamtec/p/9837877.html 视觉slam十四讲_高翔]]></content>
      <categories>
        <category>slam</category>
      </categories>
      <tags>
        <tag>slam理论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu系统备份为ISO文件]]></title>
    <url>%2F2019%2F08%2F21%2Fubuntu%E7%B3%BB%E7%BB%9F%E5%A4%87%E4%BB%BD%E4%B8%BAISO%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[1 安装systemback12345sudo add-apt-repository ppa:nemh/systembacksudo apt-get updatesudo apt-get install systemback unionfs-fuse 2 制作当前系统的镜像该软件还可以设置还原点，进行系统还原，操作很简单，一键设置一键还原，这里不再介绍 打开systemback 输入密码 选择Live system create 勾选 include the user data files 点击create new 生成备份结束后，选择备份文件，点击Convert to ISO 到你保存的路径下就能看到整体打包的系统ISO文件。 3 问题处理我进行完成2.5后，无法转为ISO文件，原因为系统大小大于4GB，解决办法如下： 创建sblive文件夹并解压通过systemback生成的.sblive 文件至sblive文件夹: 12mkdir sblivetar -xf /home/systemback_live_2019-08-16.sblive -C sblive ## 中间为你创建的镜像 重命名 syslinux 至 isolinux: 12mv sblive/syslinux/syslinux.cfg sblive/syslinux/isolinux.cfgmv sblive/syslinux sblive/isolinux 安装 cdtools 123456789101112sudo apt-get install aria2 aria2c -s 10 https://nchc.dl.sourceforge.net/project/cdrtools/alpha/cdrtools-3.02a07.tar.gz tar -xzvf cdrtools-3.02a07.tar.gz cd /home/hadoop/cdrtools-3.02 make sudo make install 生成 ISO 文件: 123cd ~ /opt/schily/bin/mkisofs -iso-level 3 -r -V sblive -cache-inodes -J -l -b isolinux/isolinux.bin -no-emul-boot -boot-load-size 4 -boot-info-table -c isolinux/boot.cat -o sblive.iso sblive 等待执行完成，我们便可在主文件夹下看见生成的sblive.iso镜像文件了 该部分来自：本文链接：https://blog.csdn.net/qq_39940390/article/details/94980229 4 系统安装1 虚拟机 VM14虚拟机的安装包和ubuntu16.04的ISO文件在实验室三星U盘里 安装过程可以参考 https://blog.csdn.net/qq_28090573/article/details/82724910，非常详细 我尝试了在虚拟机中安装我自己生成的ISO文件，但是卡在登陆界面无法进入，在网上查了很久尝试了很多方法也没能解决，最后选择了安装原生ubuntu的ISO文件 2 双系统未尝试安装 总结 systemback 的系统还原功能亲测可用，但是还原点会占一定的存储空间，建议只在重大环境安装前后备份使用 systemback 的ISO文件刻录功能亲测可用，但是得到的ISO文件在安装过程中存在问题，目前没有证明得到的ISO文件可以完成安装（虽然网上很多成功的。。。。）]]></content>
      <categories>
        <category>问题解决</category>
      </categories>
      <tags>
        <tag>Ubuntu系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[修改默认打开方式]]></title>
    <url>%2F2019%2F08%2F21%2F%E4%BF%AE%E6%94%B9%E9%BB%98%E8%AE%A4%E6%89%93%E5%BC%80%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[刚装上Typaro时markdown文件没有默认用它打开，而且打开方式中也找不到，可以采用以下方法 个人的打开方式保存在~/.local/share/applications/mimeapps.list 1sudo gedit ~/.local/share/applications/mimeapps.list 修改mimeapps.list 文件，在文件末尾添加 1text/markdown=typora.desktop 完成。]]></content>
      <categories>
        <category>问题解决</category>
      </categories>
      <tags>
        <tag>Ubuntu系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[激光雷达安装步骤]]></title>
    <url>%2F2019%2F08%2F21%2F%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E5%AE%89%E8%A3%85%E6%AD%A5%E9%AA%A4%2F</url>
    <content type="text"><![CDATA[1. 安装SDK1.1 下载相关文件 网址：http://go.slamtec.com/rplidar/a3/download 我已将相关文件下载至思岚雷达文件夹 后续工作可以按照我的说明，不必去看SDK说明，里面包含Windows、macos等安装，很杂。 1.2 编译SDK 解压SDK压缩包至根目录，重命名为rplidar_sdk cd ~/rplidar_sdk/sdk make 1.3 交叉编译 我没有进行这一步骤，因为目前不知道是否需要 透过交叉编译特性,SDK 的编译系统支持编译产生其他平台/系统的二进制可执行文件。注意: 该功能仅针对使用 Makefile 的环境.交叉编译特性将通过调用 cross_compile.sh 脚本激活。该脚本的调用语法如下:CROSS_COMPILE_PREFIX= ./cross_compile.sh例如: CROSS_COMPILE_PREFIX=arm-linux-gnueabihf ./cross_compile.sh 2. 连接雷达和PC 直接接电脑即可 3. 运行DEMO3.1 ultra_simple 该示例程序演示 PC 通过串口与 RPLIDAR 进行连接，并不断的将 RPLIDAR 扫描数据输出的最简单过程。 ls /dev/ttyUSB* ##这个命令可以检测你的雷达USB编号，看你雷达连上没 cd ~/rplidar_sdk/sdk/output/Linux/Release ./ultra_simple /dev/ttyUSB0 能转起来就说明安装好了]]></content>
      <categories>
        <category>slam</category>
      </categories>
      <tags>
        <tag>slam实现</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[激光雷达和ROS结合]]></title>
    <url>%2F2019%2F08%2F21%2F%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E5%92%8CROS%E7%BB%93%E5%90%88%2F</url>
    <content type="text"><![CDATA[网址：https://github.com/slamtec/rplidar_ros 1 下载整个rplidar_ros包 cd catkin_ws/src ##到你的工作空间的src git clone https://github.com/Slamtec/rplidar_ros.git cd .. catkin_make source ./devel/setup.bash 2 运行rplidar_ros包###每次雷达重新连接电脑时，需要进行2.1前的步骤，如果USB口不是USB0，需要改动launch文件中的 param name=”serial_port” type=”string” value=”/dev/rplidar”/ ls -l /dev |grep ttyUSB #检查雷达的USB口 sudo chmod 666 /dev/ttyUSB0 #赋予权限 2.1在rviz中显示 roslaunch rplidar_ros view_rplidar_a3.launch 2.2在终端显示 roslaunch rplidar_ros rplidar_a3.launch #启动雷达 rosrun rplidar_ros rplidarNodeClient #在另外一个终端打开]]></content>
      <categories>
        <category>slam</category>
      </categories>
      <tags>
        <tag>slam实现</tag>
      </tags>
  </entry>
</search>
